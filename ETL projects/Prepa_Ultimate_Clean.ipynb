{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71d2565c-69ee-4f9e-adf0-84d84601d24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"D:\\Scripts1\\Code\\ActPEA\\CODE\\Actu_Colums.py\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Actu_Colums as Ac\n",
    "import win32com.client as win32\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from openpyxl.worksheet.table import Table, TableStyleInfo\n",
    "import datetime\n",
    "import os\n",
    "import glob\n",
    "from typing import Optional\n",
    "from babel.dates import format_date, format_datetime, Locale\n",
    "import re\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed13dd5-0f7a-4729-ad65-ab4962e5b491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73c4d6b5-0b0a-40dd-a282-e42b0332c9ab",
   "metadata": {},
   "source": [
    "### El flujo de actualizacion es el siguiente : \n",
    "    -01.Extraigo un fragmente del prepa_global_N solo de EI\n",
    "    -01,5. Concateno los Bloqueos de Factura\n",
    "    -02.Hereda informacion del Prepa_old\n",
    "    -03.Actualizo info del PAP\n",
    "    -03,5. Añado analistas\n",
    "    -04.Vuelvo a meter el fragmento en el prepa_global_N\n",
    "    -05.To Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b5f85d-3035-48b0-9aa4-9ae2dd8603de",
   "metadata": {},
   "source": [
    "## Time pointer and funcions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72f6e4c1-6111-4625-a259-6e0dce48bf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hoy es: 01.07\n"
     ]
    }
   ],
   "source": [
    "Today_str = datetime.date.today().strftime('%d-%m-%Y')\n",
    "Today_D_M = Today_str[0:5]\n",
    "Today_D_M = [Today_D_M[0:2],'.',Today_D_M[3:6]]\n",
    "Today_D_M = ''.join(Today_D_M)\n",
    "#Today_D_M = '06.02'\n",
    "print('Hoy es:',Today_D_M)\n",
    "def process_df_V1(df: pd.DataFrame,clases_unicas: list,index=['NOMPROVEEDOR','ESTADO']):\n",
    "    \"\"\" La funcion recibe un df, y las clases unicas\n",
    "        Y devuelve el df uniformizado para cada clase\n",
    "        Llena los valores faltantes con 0\"\"\"\n",
    "    pivot_table = pd.pivot_table(df, values='PEND_FACT_SOLES', index=index, aggfunc=pd.Series.sum) #Agrupo \n",
    "    grupos = pivot_table.groupby('NOMPROVEEDOR') #creo un data frame para cada contrata\n",
    "    # Para cada grupo, crea un nuevo dataframe y guárdalo en un diccionario\n",
    "    dataframes = {}\n",
    "    for nombre, datos in grupos: # Creo los df dentro del dic\n",
    "        dataframes[nombre] = datos\n",
    "    for key in dataframes.keys(): # para cada df en el dic\n",
    "        df = dataframes[key].reset_index()\n",
    "        df1 = df.set_index('ESTADO').reindex(clases_unicas).reset_index() #normalizo, creando filas para todos los ESTADOS\n",
    "        df1.NOMPROVEEDOR = key  #Relleno los NAN\n",
    "        df1 = df1.fillna(0)  \n",
    "        dataframes[key] = df1   #Reescribo los Df para cada clave\n",
    "    df_concat = pd.concat(dataframes.values(),ignore_index=True) # Compacto todo los df del dic en uno grande \n",
    "    pivot_table_1 = pd.pivot_table(df_concat, values='PEND_FACT_SOLES', index=index, aggfunc=pd.Series.sum) #agrupo de nuevo\n",
    "    return pivot_table_1\n",
    "################################################################################################3\n",
    "def process_df_V2(df: pd.DataFrame,clases_unicas: list,index=['NOMPROVEEDOR','ESTADO'],agg = pd.Series.sum):\n",
    "    \"\"\" La funcion recibe un df, y las clases unicas\n",
    "        Y devuelve el df uniformizado para cada clase\n",
    "        Llena los valores faltantes con 0\"\"\"\n",
    "    pivot_table = pd.pivot_table(df, values='PEND_FACT_SOLES', index=index, aggfunc=agg) #Agrupo \n",
    "    grupos = pivot_table.groupby(index[:-1]) #creo un data frame para cada index mayor\n",
    "    # Para cada grupo, crea un nuevo dataframe y guárdalo en un diccionario\n",
    "    dataframes = {}\n",
    "    for nombre, datos in grupos: # Creo los df dentro del dic\n",
    "        dataframes[nombre] = datos\n",
    "    for key in dataframes.keys(): # para cada df en el dic\n",
    "        df = dataframes[key].reset_index()\n",
    "        df1 = df.set_index(index[-1]).reindex(clases_unicas).reset_index() #normalizo, creando filas para todos los ESTADOS\n",
    "        df1[index[:-1]] = key  #Relleno los NAN para normalizar estados\n",
    "        df1 = df1.fillna(0)  \n",
    "        dataframes[key] = df1   #Reescribo los Df para cada clave\n",
    "    df_concat = pd.concat(dataframes.values(),ignore_index=True) # Compacto todo los df del dic en uno grande \n",
    "    pivot_table_1 = pd.pivot_table(df_concat, values='PEND_FACT_SOLES', index=index, aggfunc=pd.Series.sum) #agrupo de nuevo\n",
    "    return pivot_table_1\n",
    "def calc_diff_V2(df1: pd.DataFrame,df2: pd.DataFrame, index= ['NOMPROVEEDOR', 'ESTADO'],OutC = 'DIFERENCIA EN SOLES',agg = pd.Series.sum) -> pd.DataFrame:\n",
    "    \"\"\" entran 2 dataframes , los proceso , y en base a los df procesados \n",
    "        calculo la diferencia en una nueva columna, \n",
    "        y añado esta columna de diferencia al df actual \"\"\"\n",
    "    df1 = cleanrows(df1)\n",
    "    df2 = cleanrows(df2)\n",
    "    clases_unicas = pd.concat([df1['ESTADO'].dropna(), df2['ESTADO'].dropna()]).unique() #Creo listas de claves unicas\n",
    "    df1_proces = process_df_V2(df1,clases_unicas)\n",
    "    df2_proces = process_df_V2(df2,clases_unicas)\n",
    "    \n",
    "    df_diff = df1_proces - df2_proces\n",
    "    diff_pivot_table_reset = df_diff.reset_index()\n",
    "    diff_pivot_table_reset.rename(columns={'PEND_FACT_SOLES': OutC}, inplace=True)\n",
    "    # Merge diff_pivot_table_filled_reset con PrePa_O_EI\n",
    "    PrePa_O_EI = pd.merge(df1, diff_pivot_table_reset, on=index, how='left')    \n",
    "    return PrePa_O_EI\n",
    "def calc_diff_V1(df1: pd.DataFrame,df2: pd.DataFrame, index= ['NOMPROVEEDOR', 'ESTADO']) -> pd.DataFrame:\n",
    "    \"\"\" entran 2 dataframes , los proceso , y en base a los df procesados \n",
    "        calculo la diferencia en una nueva columna, \n",
    "        y añado esta columna de diferencia al df actual \"\"\"\n",
    "    clases_unicas = pd.concat([df1['ESTADO'], df2['ESTADO']]).unique() #Creo listas de claves unicas\n",
    "    df1_proces = process_df_V1(df1,clases_unicas)\n",
    "    df2_proces = process_df_V1(df2,clases_unicas)\n",
    "    \n",
    "    df_diff = df1_proces - df2_proces\n",
    "    diff_pivot_table_reset = df_diff.reset_index()\n",
    "    diff_pivot_table_reset.rename(columns={'PEND_FACT_SOLES': 'DIFERENCIA EN SOLES'}, inplace=True)\n",
    "    # Merge diff_pivot_table_filled_reset con PrePa_O_EI\n",
    "    PrePa_O_EI = pd.merge(df1, diff_pivot_table_reset, on=index, how='left')    \n",
    "    return PrePa_O_EI\n",
    "\n",
    "def filtro(df):\n",
    "    return df[df.RESPONSABLE2 == 'Eduardo Iberico'] #Filtro \n",
    "    \n",
    "def get_2_df_from_path(path: str,extension: str, Week_Int: int,sheet_name: str) -> list[pd.DataFrame]:\n",
    "    \"\"\" Extrae los archivos mas recientes de un path\"\"\"\n",
    "    path= path+'/*'\n",
    "    tipo_de_archivo = '*'+ extension\n",
    "    archivos = glob.glob(path + tipo_de_archivo)\n",
    "    archivos_ordenados = sorted(archivos, key=os.path.getmtime, reverse=True)\n",
    "    nombres_de_archivos = [os.path.basename(archivo) for archivo in archivos_ordenados[:1] + archivos_ordenados[Week_Int-1:Week_Int]] \n",
    "    print(nombres_de_archivos)\n",
    "    dataframes = []  # Lista para almacenar los DataFrames\n",
    "    for nombre in nombres_de_archivos:\n",
    "        path_of_e = path[:-1] + '{}'.format(nombre)\n",
    "        df = pd.read_excel(path_of_e, sheet_name=sheet_name, dtype={'COMENTARIO': str})\n",
    "        dataframes.append(df)\n",
    "    return dataframes\n",
    "        \n",
    "def convert_to_date(val):\n",
    "    try:\n",
    "        return pd.Timestamp('1899-12-30') + pd.Timedelta(int(float(val)), 'D')\n",
    "    except ValueError:\n",
    "        return val\n",
    "\n",
    "def get_recent_df(Carpeta_path: str, sheet_name: str):\n",
    "    \"\"\" Devuelve el df de la hoja especifica, del archivo mas reciente de la carpeta especificada\"\"\"\n",
    "    Path_n= Carpeta_path + '/*'\n",
    "    tipo_de_archivo = '*.xlsx'\n",
    "    # Busca el archivo más reciente\n",
    "    archivos = glob.glob(Path_n + tipo_de_archivo)\n",
    "    archivo_mas_reciente = max(archivos, key=os.path.getmtime)\n",
    "    nombre_del_archivo_N = os.path.basename(archivo_mas_reciente)\n",
    "    print(archivo_mas_reciente)\n",
    "    # Lee el archivo sin especificar el tipo de datos\n",
    "    df = pd.read_excel(archivo_mas_reciente , sheet_name=sheet_name)\n",
    "    # Si la columna \"COMENTARIO\" existe, cambia su tipo de datos a str\n",
    "    if 'COMENTARIO' in df.columns:\n",
    "        df['COMENTARIO'] = df['COMENTARIO'].astype(str)\n",
    "    return df,nombre_del_archivo_N\n",
    "def get_recent_c_df(Carpeta_path: str, sheet_name: str):\n",
    "    \"\"\" Devuelve el df de la hoja especifica, del archivo mas reciente de la carpeta especificada\"\"\"\n",
    "    Path_n= Carpeta_path + '/*'\n",
    "    tipo_de_archivo = '*.xlsx'\n",
    "    # Busca el archivo más reciente\n",
    "    archivos = glob.glob(Path_n + tipo_de_archivo)\n",
    "    archivo_mas_reciente = max(archivos, key=os.path.getctime)\n",
    "    nombre_del_archivo_N = os.path.basename(archivo_mas_reciente)\n",
    "    print(archivo_mas_reciente)\n",
    "    # Lee el archivo sin especificar el tipo de datos\n",
    "    df = pd.read_excel(archivo_mas_reciente , sheet_name=sheet_name)\n",
    "    # Si la columna \"COMENTARIO\" existe, cambia su tipo de datos a str\n",
    "    if 'COMENTARIO' in df.columns:\n",
    "        df['COMENTARIO'] = df['COMENTARIO'].astype(str)\n",
    "    return df,nombre_del_archivo_N\n",
    "def cleanrows(df):\n",
    "    if 'STATUSFINAL' in df.columns:\n",
    "        df = df[df['STATUSFINAL'] == 'PENDIENTE']\n",
    "    elif 'STATUS FINAL' in df.columns:\n",
    "        df = df[df['STATUS FINAL'] == 'PENDIENTE']\n",
    "    return df\n",
    "\n",
    "def process_to_bcsv(df : pd.DataFrame,ruta_del_csv : str,Fecha: str):\n",
    "\n",
    "    df['TIME'] = Fecha\n",
    "    pivot_table = pd.pivot_table(df, values='PEND_FACT_SOLES', index=['TIME','MES-COMPROMISO','NOMPROVEEDOR','ESTADO'], aggfunc=pd.Series.sum) #Agrupo \n",
    "    df_reset = pivot_table.reset_index(drop=False)\n",
    "    #convetir la columna del agrupado al formato d efehca \n",
    "    # Convierte la columna 'Fecha' a datetime\n",
    "    df_reset['TIME'] = pd.to_datetime(df_reset['TIME'],format = '%d-%m-%Y')\n",
    "    \n",
    "    # Formatea la columna 'Fecha'\n",
    "    df_reset['TIME Format'] = df_reset['TIME'].apply(lambda x: format_date(x, 'EEE dd-MM-yyyy', locale=Locale('es', 'ES')))\n",
    "    df_reset.to_csv(ruta_del_csv, mode='a', header=False,index=False)\n",
    "def transformar_name(valor):\n",
    "    str(valor)\n",
    "    if valor.isupper():\n",
    "        return valor\n",
    "    else:\n",
    "        palabras = valor.split()\n",
    "        return ' '.join([palabras[0], palabras[2]])\n",
    "def process_PAP(value):\n",
    "    if any(map(str.isdigit, str(value))):\n",
    "        value_float = int(value)\n",
    "        value_str = str(value_float)\n",
    "        return value_str\n",
    "    else:\n",
    "        return value\n",
    "def load_df_by_name(directorios:str, cadena:str) -> pd.DataFrame:\n",
    "    df_list = []\n",
    "    for dictory in directorios:\n",
    "        ruta = buscar_archivo_mas_antiguo(dictory, cadena)\n",
    "        try:\n",
    "            df = pd.read_excel(ruta, sheet_name='Hoja2')\n",
    "            #print('Machea con nombre de hoja2')\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}. Trying with 'Sheet1'\")\n",
    "            df = pd.read_excel(ruta, sheet_name='Sheet1')\n",
    "            #print('Machea con nombre de Sheet1')\n",
    "        print(ruta)\n",
    "        df_list.append(df)\n",
    "    return df_list\n",
    "def buscar_archivo_mas_antiguo(directorio, cadena):\n",
    "    archivos_coincidentes = [os.path.join(directorio, nombre_archivo) for nombre_archivo in os.listdir(directorio) if cadena in nombre_archivo]\n",
    "    if not archivos_coincidentes:\n",
    "        return None\n",
    "    else:\n",
    "        archivo_mas_antiguo = min(archivos_coincidentes, key=os.path.getmtime)\n",
    "        return archivo_mas_antiguo\n",
    "def formated2bi(df_diff:pd.DataFrame,contrata_dic:dict,columns_dic:dict,colum_list):\n",
    "    df_fomarted = df_diff.copy()\n",
    "    # Aplicar la función a los nombres de las columnas\n",
    "    df_fomarted.columns = map(transformar_nombre, df_fomarted.columns) # Normalizo nombres de columna\n",
    "    df_fomarted['SITE'] = df_fomarted['SITE'].str.replace('_', ' ')\n",
    "    df_fomarted['SITE'] = df_fomarted['SITE'].str.title()\n",
    "    df_fomarted['TEXTO BREVE'] = df_fomarted['TEXTO BREVE'].str.title()\n",
    "    df_fomarted['NOMPROVEEDOR'].replace(contrata_dic,inplace=True)\n",
    "    df_fomarted['NOMPROVEEDOR'] = df_fomarted['NOMPROVEEDOR'].str.title()\n",
    "    df_fomarted.columns = df_fomarted.columns.str.strip()\n",
    "    #Cambio los nombres de las columnas con un Map\n",
    "    df_fomarted.rename(columns=columns_dic, inplace=True)\n",
    "\n",
    "    df_fomarted.columns = df_fomarted.columns.str.title()\n",
    "    \n",
    "    try:\n",
    "        df_fomarted = df_fomarted[colum_list] #@encapsular\n",
    "        print('Formato con CF')\n",
    "    except:\n",
    "        #colum_list.remove('Estado Cenfile')\n",
    "        colum_list.remove('Antiguamiento Proyectado')\n",
    "        df_fomarted = df_fomarted[colum_list]\n",
    "        print('Formato sin CF')\n",
    "    \n",
    "    #columns_2_drop = [col.title() for col in columns_2_drop]\n",
    "    \n",
    "    #df_fomarted.drop(columns=columns_2_drop,inplace=True)\n",
    "    return df_fomarted\n",
    "def transformar_nombre(nombre):\n",
    "    nombre = nombre.upper()\n",
    "    nombre = nombre.replace('_', ' ')\n",
    "    nombre = nombre.replace('-', ' ')\n",
    "    return nombre\n",
    "\n",
    "contrata_dic = {'DELTA ELECTRONICS (PERU) INC. S.R.L ELTEK PERU S.R.L.':'DELTA',\n",
    "                'COMUNICACION FUTURA SOCIEDAD ANONIM':'COMFUTURA'}\n",
    "columns_dic = {\"CONTRA.FEC\":\"Fecha Contrata\" , \"CONTRAT.COM\": \"Comentario Contrata\"\n",
    "               ,\"Dif Soles\":\"Diferencia Soles\", \"DOC COMPRAS\": \"Orden de Compra\" \n",
    "               , \"DOCUMENTO REFERENCIA\": \"DOCUMENTO REFERENCIA\",\"NOMPROVEEDOR\" :\"CONTRATISTA\"\n",
    "               , \"PAP\":\"N° PAP\", \"PEND FACT SOLES\":\"PENDIENTE EN SOLES\"\n",
    "               , \"TEXTO BREVE\": \"DESCRIPCION\"\n",
    "               , \"POSIC\":\"POSICIÓN\",\"DESCRIPCION\": \"DESCRIPCIÓN\"\n",
    "               , \"ANT PROYECTADO\": \"Antiguamiento Proyectado\", \"ANTIGUAMIENTO PAP\" : \"ANTIGUAMIENTO PAP\"\n",
    "               , \"FECHA REPORTE\": \"FECHA REPORTE\", \"MES COMPROMISO\" : \"MES DE COMPROMISO\"\n",
    "               , \"POS\": \"POSICION\", 'FECH CONTAB':'FECHA CONTABILIDAD'}\n",
    "def clean_date(df,column):\n",
    "    df.loc[:,column] = pd.to_datetime(df[column])\n",
    "    df[column] = pd.to_datetime(df[column], format='%Y-%m-%d')\n",
    "    df[column] = df[column].astype(str)\n",
    "    return df\n",
    "# Ordeno los KEYS\n",
    "def change_rows(df_concat:pd.DataFrame,df_duplicados:pd.DataFrame,Prepa_N:pd.DataFrame):\n",
    "    df_concat = df_concat.sort_values(by=['CONCA', 'DOCUMENTO_REFERENCIA'], ascending=[True, True])\n",
    "    # Crear un DataFrame con las keys y las columna que me interesa.\n",
    "    df_info = df_duplicados.drop_duplicates(subset=['CONCA', 'DOCUMENTO_REFERENCIA'], keep='first')[['CONCA', 'DOCUMENTO_REFERENCIA', 'FECHASCOMPROMISO']]\n",
    "    \n",
    "    #la base en la que voy a editar las columnas,borro esa columna\n",
    "    df_dupli = df_duplicados.drop('FECHASCOMPROMISO', axis=1).drop_duplicates(subset=['CONCA', 'DOCUMENTO_REFERENCIA'], keep='last')\n",
    "    df_final = merge_dataframes(df_dupli,df_info,['CONCA','DOCUMENTO_REFERENCIA'],index=df_dupli.index) #concateno, es como si intercambiara columnas\n",
    "    df_final\n",
    "    return Prepa_N.update(df_final,overwrite=True)#Finalmente con eso sobreescribro el original \n",
    "\n",
    "\n",
    "def drop_duplicatesR(df_duplicados:pd.DataFrame,BDF:pd.DataFrame,Prepa_N:pd.DataFrame):\n",
    "    df2Rem = df_duplicados.drop_duplicates(subset=['CONCA', 'DOCUMENTO_REFERENCIA'],keep='first')\n",
    "    list2Rem = df2Rem.index.tolist()\n",
    "    BDF_no_D = BDF.drop(list2Rem,axis=0)\n",
    "    Prepa_N = pd.concat([BDF_no_D, Prepa_N]) # este es un temporal para comprobar si hay filas duplicadas \n",
    "    print(f\"los siguientes index en el BDF estan en el reporte aun: {list2Rem}\")\n",
    "    Prepa_N = Prepa_N.drop_duplicates(subset=['CONCA', 'DOCUMENTO_REFERENCIA'])\n",
    "    return Prepa_N\n",
    "def merge_dataframes(df1, df2, merge_on, index):\n",
    "    \n",
    "    merged_df = pd.merge(df1, df2, on=merge_on, how='left')\n",
    "    display(len(merged_df))\n",
    "    #display('Filas duplicadas:',merged_df[merged_df.duplicated(subset=['DOCUMENTO_REFERENCIA','CONCA'])]) \n",
    "    #return merged_df\n",
    "    #merged_df = merged_df.drop_duplicates()\n",
    "    merged_df = merged_df.drop_duplicates(subset=['CONCA','DOCUMENTO_REFERENCIA'],keep='first')\n",
    "    #return merged_df\n",
    "    print(\"filas del merge: \",len(merged_df))\n",
    "    print(\"filas index: \",len(index))\n",
    "    merged_df.set_index(index, inplace=True)\n",
    "    return merged_df\n",
    "def normalize_company_names(df, column):\n",
    "    \"\"\"Normaliza los nombres de las empresas en la columna especificada del DataFrame.\"\"\"\n",
    "    # Reemplaza \"SAC\" o \"S. A. C.\" al final de los nombres de las empresas con \"S.A.C\"\n",
    "    df[column] = df[column].str.replace(r\"(SAC|S\\. ?A\\. ?C\\.)$\", \"S.A.C.\", regex=True)\n",
    "    return df\n",
    "def compact_rows(df:pd.DataFrame, columns:list, delimiter:str ='/'):\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "    \n",
    "    # Usar 'join' como función de agregación para concatenar los valores\n",
    "    agg_func = {col: lambda x: '/'.join(x.unique()) for col in df.columns if col not in  columns}\n",
    "    \n",
    "    # Agrupar por 'OC Posición' y aplicar la función de agregación\n",
    "    df1 = df.groupby(columns).agg(agg_func).reset_index()\n",
    "    return df1\n",
    "def update_and_rename(df1, update_cols, new_names):\n",
    "    df = df1.copy()\n",
    "    for col in update_cols:\n",
    "        df[col[0]].update(df[col[1]])\n",
    "    df.rename(columns=new_names, inplace=True)\n",
    "    df.drop(columns=[col for sublist in update_cols for col in sublist[1:]], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f6562-3386-443b-b445-a54c7f61816f",
   "metadata": {},
   "source": [
    "# Cargo Archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae30cf-d755-40d3-9fe4-dd55e9d2e911",
   "metadata": {},
   "source": [
    "## Cargo Prepa_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31d45795-4df6-4be3-9dab-90dd09d52b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\Resultados_prepa\\PREPA_UPDATE28.06.xlsx\n"
     ]
    }
   ],
   "source": [
    "PrePa_O,_ = get_recent_c_df(r'\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\Resultados_prepa','Sheet1')\n",
    "#PrePa_O,_ = get_recent_df(r'D:\\Prepa_local','Sheet1')\n",
    "\n",
    "## Manejo los comentarios por si tiene fechas \n",
    "PrePa_O['COMENTARIO'] = PrePa_O['COMENTARIO'].apply(convert_to_date)\n",
    "#CPX['FECHA'] = CPX['FECHA'].apply(convert_to_date)\n",
    "PrePa_O = PrePa_O.dropna(subset=['CONCA'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5659dc24-2d81-49e4-9b39-419d0c25081f",
   "metadata": {},
   "source": [
    "## OTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "237c9e37-495f-4025-ada2-6b25b91b9d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Scripts1\\Code\\ActPEA\\archvis\\OTs\\OTS 24.06.2024.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\C26764\\AppData\\Local\\Temp\\ipykernel_28132\\3491332154.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  OT['Fecha de Creación'] = pd.to_datetime(OT['Fecha de Creación'],dayfirst=True).dt.date\n"
     ]
    }
   ],
   "source": [
    "OT,_= get_recent_df(r'D:\\Scripts1\\Code\\ActPEA\\archvis\\OTs','Hoja2')\n",
    "OT = OT[OT['Status OT'] != 'PDTE CONTRATA'] #\t\n",
    "OT = OT[OT['Status OT'] != 'PDTE RESPONSABLE'].copy()\n",
    "OT['Fecha de Creación'] = pd.to_datetime(OT['Fecha de Creación'],dayfirst=True).dt.date\n",
    "OT['Fecha de Creación'] = pd.to_datetime(OT['Fecha de Creación'], format='%d-%m-%Y')\n",
    "fecha_limite = pd.Timestamp('2021-11-01')\n",
    "OT_cut = OT[OT['Fecha de Creación'] > fecha_limite].copy() # Corto las OTs solo de este año \n",
    "OT_cut = normalize_company_names(OT_cut,'Contrata')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bbc2c3-5cbd-4b66-8296-46ca7bf617e2",
   "metadata": {},
   "source": [
    "## Cargo Prepa_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73ae3c06-fee4-418f-b065-4a5748e4d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28212c51-7094-458f-a553-abddd66637c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/Scripts1/Code/ActPEA/archvis/Pre_pa/NEW\\Prepasivo completo (Finanzas Junio) 27.06.24.xlsx\n",
      "El reporte es de hace 4 dias\n"
     ]
    }
   ],
   "source": [
    "Prepa_N,nombre_del_archivo_N = get_recent_df('D:/Scripts1/Code/ActPEA/archvis/Pre_pa/NEW/','DATA')\n",
    "\n",
    "Prepa_N = cleanrows(Prepa_N)\n",
    "\n",
    "Fecha_reporte = nombre_del_archivo_N[-13:-8].replace('.','-')\n",
    "#Prepa_N.ESTADO_CENFILE.replace(0,'No Aplica',inplace=True)\n",
    "#aarchivos = glob.glob(Path_n + tipo_de_archivo)\n",
    "Fecha_reporte_DT = datetime.strptime(Fecha_reporte, \"%d-%m\")\n",
    "Today2C = datetime.strptime(Today_str[:-5], \"%d-%m\")\n",
    "a = Today2C - Fecha_reporte_DT\n",
    "if Fecha_reporte_DT == Today2C:\n",
    "    reporte_old = False\n",
    "    print(\"El reporte es actual\")\n",
    "elif Fecha_reporte_DT < Today2C:\n",
    "    report_old = True\n",
    "    print(f\"El reporte es de hace {a.days} dias\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3737d0b0-1712-4774-baf6-e3e79330f151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Hay una variacion de columnas en la Base,['AVANCE AL 21.06', 'INDICADOR2', 'ÁREA', 'AVANCE AL 19.06', 'CODIGO_SITE', 'NOMBRE_SITE', 'PENDIENTE', 'AVANCE AL 24.06', 'STATUS FINAL', 'RESPON', 'ESTADO_CENFILE', 'CENFILE', 'AVANCE AL 17.06', 'AVANCE AL 10.06', 'AVANCE AL 27.06', 'FECHA - PROY', 'Comentario', 'AVANCE AL 26.06', 'AGRUPADOR', 'AVANCE AL 12.06'] \n",
      "¿Deseas continuar con la ejecución? (s/n):  s\n"
     ]
    }
   ],
   "source": [
    "with open(r\"D:\\Scripts1\\Code\\ActPEA\\CODE\\Temps\\Standar_columns_PREPA.pickle\", \"rb\") as archivo:\n",
    "    lista_recuperada = pickle.load(archivo)\n",
    "columns_E = lista_recuperada\n",
    "columns_Ac = Prepa_N.columns.tolist() \n",
    "\n",
    "if columns_Ac != columns_E:\n",
    "    Columns_diff = list(set(columns_Ac) - set(columns_E))\n",
    "    respuesta = input(f'Hay una variacion de columnas en la Base,{Columns_diff} \\n¿Deseas continuar con la ejecución? (s/n): ')\n",
    "    if respuesta.lower() != 's':\n",
    "            sys.exit()\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cfe123-bafe-41a1-9cf3-e0c7f921d659",
   "metadata": {},
   "source": [
    "## Cargo PAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1487e057-4c24-4291-b916-3335cfdfe0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Scripts1\\Code\\ActPEA\\archvis\\PAP\\PAP 01.07.xlsx\n",
      "D:\\Scripts1\\Code\\ActPEA\\archvis\\PAP\\Administrativo\\PAP_A_29.06.2024.xlsx\n"
     ]
    }
   ],
   "source": [
    "PAP_O,_ = get_recent_df(r'D:\\Scripts1\\Code\\ActPEA\\archvis\\PAP','Hoja2')# PAP de Otros\n",
    "\n",
    "PAP_A,_ = get_recent_df(r'D:\\Scripts1\\Code\\ActPEA\\archvis\\PAP\\Administrativo','Hoja2')# PAP de Admin\n",
    "\n",
    "PAP = pd.concat([PAP_O,PAP_A])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215bb0a3-b225-41cb-8f03-a2136e66e302",
   "metadata": {},
   "source": [
    "## Cargo los Bloqueos de Factura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08fc6b25-4092-4cb1-a88e-8d17a56bf8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BDF = pd.read_csv(r'D:\\Scripts1\\Code\\ActPEA\\CODE\\Temps\\BAF_4FC.csv')\n",
    "if 'STATUSFINAL' in Prepa_N.columns:\n",
    "    BDF['STATUSFINAL'] == 'PENDIENTE'\n",
    "    deep_columns = BDF[['MES - COMPROMISO','STATUS FINAL','FECHASCOMPROMISO']]  \n",
    "\n",
    "elif 'STATUS FINAL' in Prepa_N.columns:\n",
    "    BDF['STATUS FINAL'] == 'PENDIENTE'\n",
    "    deep_columns = BDF[['MES - COMPROMISO','STATUS FINAL','FECHASCOMPROMISO']]  \n",
    "else:\n",
    "    deep_columns = BDF[['MES - COMPROMISO','FECHASCOMPROMISO']]  \n",
    "\n",
    "BDF.drop_duplicates(subset=['CONCA','DOCUMENTO_REFERENCIA'],inplace=True)\n",
    "BDF.drop([16],axis=0,inplace=True) # duplicado\n",
    "# Selecciona las primeras 10 columnas\n",
    "first_ten = BDF.iloc[:, :8]\n",
    "\n",
    "# Selecciona las columnas 49 y 76\n",
    "\n",
    "# Combina las columnas seleccionadas en un nuevo DataFrame\n",
    "BDF_cut = pd.concat([first_ten, deep_columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9b8d3b9-4bbd-4354-82c9-64459dec73e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 112 entries, 0 to 112\n",
      "Data columns (total 11 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   DOC_COMPRAS           112 non-null    float64\n",
      " 1   POSIC                 112 non-null    float64\n",
      " 2   CONCA                 112 non-null    int64  \n",
      " 3   DOCUMENTO_REFERENCIA  112 non-null    float64\n",
      " 4   TEXTO_BREVE           112 non-null    object \n",
      " 5   PEND_FACT_SOLES       112 non-null    float64\n",
      " 6   NOMPROVEEDOR          112 non-null    object \n",
      " 7   RESPONSABLE2          112 non-null    object \n",
      " 8   MES - COMPROMISO      112 non-null    object \n",
      " 9   STATUS FINAL          112 non-null    object \n",
      " 10  FECHAS COMPROMISO     112 non-null    object \n",
      "dtypes: float64(4), int64(1), object(6)\n",
      "memory usage: 10.5+ KB\n"
     ]
    }
   ],
   "source": [
    "BDF_cut.dropna(subset='CONCA',inplace=True)\n",
    "BDF_cut.CONCA = BDF_cut.CONCA.astype('int64')\n",
    "BDF_cut.rename(columns={'FECHASCOMPROMISO':'FECHAS COMPROMISO'},inplace=True)\n",
    "BDF_cut.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a566b6a9-a2b8-4819-9e6d-0a5bdeb667ac",
   "metadata": {},
   "source": [
    "## Compruebo las filas que voy a insertar\n",
    "- La variable reporte_old(boolean) indica si el reporte de prepa es nuevo o antiguo\n",
    "- La variable row_e indica si las filas del bloque existen en el PREPA_N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ffc0c-c659-4fac-a16a-29fb256ad2a2",
   "metadata": {},
   "source": [
    "### Para crear la variable de filas duplicadas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5ad3d93-7c83-4d8d-bf64-1c73ad353f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenar los dos DataFrames\n",
    "df_concat_0 = pd.concat([BDF_cut, Prepa_N]) # este es un temporal para comprobar si hay filas duplicadas \n",
    "# Crear una copia del DataFrame concatenado\n",
    "df_copia = df_concat_0.copy()\n",
    "# Eliminar las filas duplicadas basándose en las columnas de identificación\n",
    "df_final_0 = df_concat_0.drop_duplicates(subset=['CONCA', 'DOCUMENTO_REFERENCIA'], keep='last')\n",
    "# Obtener las filas duplicadas\n",
    "df_duplicados = df_copia[df_copia.duplicated(subset=['CONCA', 'DOCUMENTO_REFERENCIA'], keep=False)] ## Estas son las duplicas, \n",
    "#si este no es vacio entonces mi variable True Para duplicados\n",
    "row_e = not(df_duplicados.empty)\n",
    "df_duplicados.PEND_FACT_SOLES.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8cf6d2d-592b-42cd-8f49-f03abb323d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las filas no existen en el reporte, concateno\n"
     ]
    }
   ],
   "source": [
    "if (row_e == True) and (report_old==True):\n",
    "    print(\"Las filas existen en el Prepa_N, solo cambio datos\")\n",
    "    Prepa_N = change_rows(df_concat_0,df_duplicados,Prepa_N)\n",
    "elif (row_e == True) and (report_old==False):\n",
    "    Prepa_N = drop_duplicatesR(df_duplicados,BDF_cut,Prepa_N)\n",
    "else: \n",
    "    print(\"Las filas no existen en el reporte, concateno\")\n",
    "    Prepa_N = pd.concat([Prepa_N, BDF_cut]) \n",
    "    Prepa_N.reset_index(drop=True,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4165886c-3dc4-41a7-a00e-540e16caf639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MES - COMPROMISO\n",
       "Julio            230\n",
       "Bloqueo de AF    112\n",
       "Agosto            98\n",
       "Setiembre         55\n",
       "Junio             24\n",
       "Mayo              23\n",
       "Octubre            8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Prepa_N['MES - COMPROMISO'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312ce782-ef66-4cef-a150-c6be23f734b2",
   "metadata": {},
   "source": [
    "# Prepocesing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11175001-27e7-4797-a6b4-0d03b8289aa3",
   "metadata": {},
   "source": [
    "## Preprosecing of OTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4483775f-3f56-474d-8926-a82b5f1e7dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2957 entries, 95 to 3787\n",
      "Data columns (total 18 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   OT                    2957 non-null   int64         \n",
      " 1   Origen                2957 non-null   object        \n",
      " 2   Fecha de Creación     2957 non-null   datetime64[ns]\n",
      " 3   N. Req                2957 non-null   int64         \n",
      " 4   Codigo de Site        2957 non-null   object        \n",
      " 5   Nombre de Site        2957 non-null   object        \n",
      " 6   Región                2957 non-null   object        \n",
      " 7   Tipo Req              2957 non-null   object        \n",
      " 8   Proyecto              2957 non-null   object        \n",
      " 9   Subproyecto           2957 non-null   object        \n",
      " 10  Etiqueta              209 non-null    object        \n",
      " 11  Prioridad             2955 non-null   float64       \n",
      " 12  Solicitante           2957 non-null   object        \n",
      " 13  Gerente Construcción  2944 non-null   object        \n",
      " 14  Contrata              2951 non-null   object        \n",
      " 15  Fecha Asig. Contrata  2950 non-null   object        \n",
      " 16  Status OT             2957 non-null   object        \n",
      " 17  FC Status OT          69 non-null     object        \n",
      "dtypes: datetime64[ns](1), float64(1), int64(2), object(14)\n",
      "memory usage: 438.9+ KB\n"
     ]
    }
   ],
   "source": [
    "OT_cut.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e976bf1-e884-4b16-aca8-ab2a2728bb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Proyectos = ['ROLLOUT - 2023','ROLLOUT - 2022','ROLLOUT - 2024','CANON - 2023',]\n",
    "OT_cut_mant = OT_cut[OT_cut['Tipo Req'] == 'MANT. MEJORA DE RED'].copy()\n",
    "OT_cut_M = OT_cut[OT_cut.Proyecto == 'EXPANSIÓN'].copy()\n",
    "OT_cut_RRL = OT_cut[OT_cut.Etiqueta.isin(Proyectos)].copy()\n",
    "\n",
    "OT_concat = pd.concat([OT_cut_M,OT_cut_RRL,OT_cut_mant],axis=0)\n",
    "OT_concat.rename(columns={'Codigo de Site':'ID_SITIO',\n",
    "                           'Contrata':'NOMPROVEEDOR',\n",
    "                           'Nombre de Site':'SITE'},inplace=True)\n",
    "OT_concat = OT_concat[['OT','ID_SITIO','SITE','Proyecto','NOMPROVEEDOR','Etiqueta','Status OT']].copy()\n",
    "\n",
    "OT_agg_ID_PRO = compact_rows(OT_concat,['SITE','NOMPROVEEDOR'],'/')\n",
    "#OT_agg_ID_NM_PRO = compact_rows(OT_cut_RRL_2M,['ID_SITIO','SITE','PROVEEDOR'],'/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ded2a71-4f35-4f85-bf51-c35c160f516d",
   "metadata": {},
   "source": [
    "## Preprosecin of PAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2db256c-7158-4908-8409-e9105ef99dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAP_f = PAP[['OC Posición','N° Sol','Estado','Acción','Nombre responsable','Id.SIte',\n",
    "             'SIte','# Días','F.Creación','F.Modifica']]\n",
    "##Filtro solo del norte\n",
    "PAP_f = PAP_f.dropna(subset=['Id.SIte'])\n",
    "PAP_f = PAP_f[PAP_f['# Días'] < 500]\n",
    "\n",
    "PAP_f_N = PAP_f[PAP_f['Id.SIte'].str.startswith(('L','T','SAD','CL','CAC'))].copy()\n",
    "PAP_f_N = PAP_f_N.rename(columns={'N° Sol': 'PAP', #Rename\n",
    "                               'SIte': 'SITE',\n",
    "                               'Estado': 'ESTADO_PAP',\n",
    "                                '# Días': 'ANTIGUAMIENTO_PAP',\n",
    "                                'Id.SIte': 'ID Site',\n",
    "                             'Nombre responsable': 'RESPONSABLE_PAP' })\n",
    "PAP_S = Ac.split_ocs(PAP_f_N) # Spliteo OCs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6379922-1120-468d-90db-63ce76f7d423",
   "metadata": {},
   "source": [
    "## Preprosecing of PREP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daea28ca-db69-4ffb-ba99-8d80992faebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_proces(df: pd.DataFrame ,columns_2str: list[str] ,column_filter: str,C_format) -> pd.DataFrame: \n",
    "    \"\"\"Anotaciones del tipo de cada parametro para hacer la funcin mas descriptiva\"\"\"\n",
    "    ## Preprosecing of PREP_NEW\n",
    "    df = convert_columns(df.copy(),columns_2str,C_format ) #Convert to str a key column\n",
    "    df_EI = df.loc[df[column_filter] == 'Eduardo Iberico']#Filter\n",
    "    df_EI = df_EI.copy()  # Crea una copia del DataFrame original para evitar modificar los datos originales\n",
    "    return df_EI\n",
    "def convert_columns(df, columns,type):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].astype(type)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7a470ac-2dea-4e2b-9ca0-5571f57b140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################### PASO 01#####################################################################\n",
    "Prepa_EI_NC = pre_proces(Prepa_N,['DOC_COMPRAS', 'POSIC','CONCA'],'RESPONSABLE2','int64')\n",
    "#####################################################################################################\n",
    "###############################################################################################3\n",
    "Prepa_EI = pre_proces(Prepa_EI_NC,['DOC_COMPRAS', 'POSIC','CONCA'],'RESPONSABLE2','str')\n",
    "                    \n",
    "Prepa_EI.loc[:, \"OC Posición\"] = Prepa_EI.loc[:, \"DOC_COMPRAS\"].str.cat(Prepa_EI.loc[:, \"POSIC\"], sep= \":\")\n",
    "Monto_In = Prepa_EI[\"PEND_FACT_SOLES\"].sum() #Mont of USD \n",
    "#Prepa_EI = Prepa_EI.drop(columns=['FECHAS COMPROMISO','MES - COMPROMISO'])\n",
    "#Prepa_EI.DOCUMENTO_REFERENCIA = Prepa_EI.DOCUMENTO_REFERENCIA.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64d4cdda-b3e2-49f0-ae71-4087cf9ade04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Prepa_EI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05eb9571-e555-4395-b425-1e2390307972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Preprosecing of PREP_OLD\n",
    "PrePa_O_EI = pre_proces(PrePa_O,['DOC_COMPRAS', 'POSIC','CONCA'],'RESPONSABLE2','str')\n",
    "PrePa_O_EI.replace('0x2a', np.nan, inplace=True)\n",
    "PrePa_O_EI['PAP'] = PrePa_O_EI['PAP'].replace('sin pap', np.nan)\n",
    "PrePa_O_EI = PrePa_O_EI.rename(columns={'FECHA': 'F.Creación'}) #Rename\n",
    "#Filter of PEA_old\n",
    "PrePa_O_EI = PrePa_O_EI[['DOCUMENTO_REFERENCIA','PAP','CONCA', 'SITE', 'COMENTARIO','ESTADO', 'RESPONSABLE3','CONTRA.FEC','CONTRAT.COM']] #Columns i need 4 heredar\n",
    "PrePa_O_EI.CONCA = PrePa_O_EI.CONCA.astype('int64')\n",
    "Prepa_EI.CONCA = Prepa_EI.CONCA.astype('int64')\n",
    "Prepa_EI = Prepa_EI.drop_duplicates(subset=['DOCUMENTO_REFERENCIA','CONCA'],keep='first')\n",
    "len(PrePa_O_EI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69694f1d-9486-42cf-9f83-97a21970afd8",
   "metadata": {},
   "source": [
    "# Paso 02 (heredo info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20f4332d-528a-40e0-bc56-d4d4ad8d46fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filas del merge:  179\n",
      "filas index:  179\n"
     ]
    }
   ],
   "source": [
    "## ########################### # Merge new con el old (PASO 02) ####################################################################################\n",
    "#####################################################################################################################3\n",
    "PrePa_act = merge_dataframes(Prepa_EI,PrePa_O_EI,['CONCA','DOCUMENTO_REFERENCIA'], index=Prepa_EI.index)### Info heredada del old REDY \n",
    "\n",
    "#Mantengo el index para luego\n",
    "###### Creo columnas para el que actualice el PAP####################### ///\n",
    "PrePa_act[['ANTIGUAMIENTO_PAP','F.Creación','Acción','RESPONSABLE_PAP','F.Modifica']] = ''\n",
    "PrePa_act.rename(columns={'ESTADO' : 'ESTADO_PAP'},inplace=True)\n",
    "PrePa_act['RESPONSABLE_PAP'] =PrePa_act['RESPONSABLE_PAP'].astype(str)\n",
    "Cash_in = PrePa_act.PEND_FACT_SOLES.sum() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e33fb21-5934-4bc0-afd7-62d5f0937b6c",
   "metadata": {},
   "source": [
    "# Paso 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "552ccc20-2125-4b98-a318-77ff4fb16158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2314466.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ESTADO_PAP\n",
       "Pre registro        53\n",
       "Finalizado          39\n",
       "Visita ejecutada    20\n",
       "Aprobaciones FAC    17\n",
       "FAC                 17\n",
       "Observado           11\n",
       "Registrado           8\n",
       "Aprobaciones PAC     6\n",
       "PAC                  5\n",
       "Programado           2\n",
       "Sin pap              1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRE_all_act =  Ac.update_values_optimized_V2(PrePa_act, PAP_S, \"OC Posición\",['PAP','SITE','ESTADO_PAP','Acción','F.Creación','F.Modifica',##añadir una columna nueva\n",
    "                                            'RESPONSABLE_PAP','ANTIGUAMIENTO_PAP']) \n",
    "#PRE_all_act.drop(columns = 'OC Posición',inplace=True)\n",
    "display(PRE_all_act.PEND_FACT_SOLES.sum())\n",
    "\n",
    "\n",
    "duplicated_index = PRE_all_act.index.duplicated(keep=False)\n",
    "df_duplicated = PRE_all_act[duplicated_index].copy()\n",
    "PRE_all_act.drop(PRE_all_act[duplicated_index].index, inplace=True) # Borro duplicados del principal para tratarlos en el otro datafra \n",
    "\n",
    "\n",
    "\n",
    "# Ordena el DataFrame por la columna de fechas  LA FECHA DE CREACION ME SIRVE PARA ELMINAR DUPLICADOS\n",
    "df_duplicated['F.Modifica'] = pd.to_datetime(df_duplicated['F.Modifica'], format=\"%d/%m/%Y %I:%M:%S %p\") # Paso a fecha para ordenar\n",
    "df_duplicated.sort_values('F.Modifica',ascending=False, inplace=True)\n",
    "df_duplicated_NR = df_duplicated[df_duplicated.ESTADO_PAP != 'Rechazado'].sort_index(ascending=False)\n",
    "\n",
    "df_duplicated_NR = df_duplicated_NR.loc[~df_duplicated_NR.index.duplicated(keep='first')] # niego la condicional que me selecciona cada indice duplicado excepto el primero\n",
    "#df_duplicated['F.Creación'] = df_duplicated['F.Creación'].dt.strftime(\"%m/%d/%Y %I:%M:%S %p\") # Regreso a str \n",
    "PRE_all_act = pd.concat([PRE_all_act, df_duplicated_NR])\n",
    "PRE_all_act = cleanrows(PRE_all_act)\n",
    "\n",
    "#PRE_all_act = PRE_all_act.drop(columns=['F.Modifica'])\n",
    "Cash_out = PRE_all_act.PEND_FACT_SOLES.sum()\n",
    "print(Cash_in - Cash_out)\n",
    "#PRE_all_act['ESTADO_PAP'] = PRE_all_act['ESTADO_PAP'].fillna('Finalizado')\n",
    "PRE_all_act.ESTADO_PAP.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "605d02a5-3acf-4bcd-9a41-250d6eb81319",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_all_act['RESPONSABLE_PAP'] = PRE_all_act['RESPONSABLE_PAP'].replace('','SIN RESPONSABLE')\n",
    "# Aplicar la transformación\n",
    "PRE_all_act['RESPONSABLE_PAP'] = PRE_all_act['RESPONSABLE_PAP'].apply(transformar_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f17970c-eecc-4c1d-95c6-109b8403614a",
   "metadata": {},
   "source": [
    "### Mapeo Compuesto (Tratamiento de Estados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14c962a7-15cf-4a2a-b723-449c5621be61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ESTADO</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pre registro</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Finalizado</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Registrar lista de pendientes</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Solic FAC</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edwin</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Observado</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Eduardo Iberico</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CHOCOS</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PAC</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Por Asignar</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>WILBERT ATP</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>JULIO ARCENIEGA</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sin pap</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Marco</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           ESTADO  count\n",
       "0                    Pre registro     53\n",
       "1                      Finalizado     39\n",
       "2   Registrar lista de pendientes     20\n",
       "3                       Solic FAC     17\n",
       "4                           Edwin     14\n",
       "5                       Observado     11\n",
       "6                 Eduardo Iberico      6\n",
       "7                          CHOCOS      5\n",
       "8                             PAC      5\n",
       "9                     Por Asignar      3\n",
       "10                    WILBERT ATP      2\n",
       "11                JULIO ARCENIEGA      2\n",
       "12                        Sin pap      1\n",
       "13                          Marco      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definimos los valores que estamos buscando en una lista\n",
    "valores_buscados = ['Registrado', 'Aprobaciones FAC', 'Aprobaciones PAC','Lista Pendientes']\n",
    "\n",
    "# Creamos la nueva columna basada en la condición\n",
    "PRE_all_act['ESTADO'] = np.where(PRE_all_act['ESTADO_PAP'].isin(valores_buscados), \n",
    "                                 PRE_all_act['RESPONSABLE_PAP'], \n",
    "                                 PRE_all_act['ESTADO_PAP'])\n",
    "map_estado = {'Auto ATP' : 'Registrar lista de pendientes',\n",
    "       'Visita ejecutada' : 'Registrar lista de pendientes',\n",
    "        'Programado' : 'WILBERT ATP',\n",
    "        'FAC' : 'Solic FAC',\n",
    "        'Gustavo Chocos':'CHOCOS',\n",
    "        'Edwin Hurtado':'Edwin',\n",
    "        'Marco Mejia':'Marco',\n",
    "        'Wilbert Pintado':'Wilbert',\n",
    "        'Orlando Zapata':'ORLANDO Z.',\n",
    "        'Miguel Rojas':'MIGEL ROJAS',\n",
    "        'Julio Arciniega':'JULIO ARCENIEGA',\n",
    "        'Juan Trigo':'JUAN JOSE'}\n",
    "PRE_all_act['ESTADO'] = PRE_all_act['ESTADO'].replace(map_estado)\n",
    "#Modifico el valor par adetemrina condicion\n",
    "PRE_all_act.loc[PRE_all_act['ESTADO'] == 'SIN RESPONSABLE', 'ESTADO'] = 'Por Asignar'\n",
    "\n",
    "a = PRE_all_act.ESTADO.value_counts(dropna=False).reset_index()\n",
    "a.to_csv(f'D:/Prepa_R/Recuento_estados-{Today_D_M}.csv',index_label=False)\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "beb94b02-05c6-4916-a932-f0fa1899a664",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PRE_all_act = PRE_all_act.drop(columns=['F.Creación','RESPONSABLE_PAP'\n",
    "                                        ,'ANTIGUAMIENTO','DEMORA','FECH_LIB_AF','INDICADOR','MON'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1f681d3-e889-4e61-a2ee-5816329d3ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOC_COMPRAS</th>\n",
       "      <th>POSIC</th>\n",
       "      <th>CONCA</th>\n",
       "      <th>DOCUMENTO_REFERENCIA</th>\n",
       "      <th>FONDO</th>\n",
       "      <th>TIPO_MAT</th>\n",
       "      <th>TEXTO_BREVE</th>\n",
       "      <th>CODPROVEEDOR</th>\n",
       "      <th>NOMPROVEEDOR</th>\n",
       "      <th>FECH_CONTAB</th>\n",
       "      <th>...</th>\n",
       "      <th>PAP</th>\n",
       "      <th>SITE</th>\n",
       "      <th>COMENTARIO</th>\n",
       "      <th>ESTADO_PAP</th>\n",
       "      <th>RESPONSABLE3</th>\n",
       "      <th>CONTRA.FEC</th>\n",
       "      <th>CONTRAT.COM</th>\n",
       "      <th>Acción</th>\n",
       "      <th>F.Modifica</th>\n",
       "      <th>ESTADO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [DOC_COMPRAS, POSIC, CONCA, DOCUMENTO_REFERENCIA, FONDO, TIPO_MAT, TEXTO_BREVE, CODPROVEEDOR, NOMPROVEEDOR, FECH_CONTAB, EA_EM, FACTURADO, PEND_FACT, PEND_FACT_SOLES, ESTATUS_ACEPT, RESPONSABLE, RESPONSABLE2, CE_GESTOR, SUB_DIRECCION, FECHA_REPORTE, SOLICITUD, ESTATUS_SOLICITUD, AREA_RESPONSABLE, ACCION, ANTIGUAMIENTO_PAP, CENFILE, ESTADO_CENFILE, CODIGO_SITE, NOMBRE_SITE, ELEMENTO_PEP, FECHAS COMPROMISO, MES - COMPROMISO, FECHA - PROY, DÍAS -2, ANT-PROYECTADO, STATUS FINAL, INDICADOR2, ÁREA, AGRUPADOR, RESPON, AVANCE AL 10.06, AVANCE AL 12.06, AVANCE AL 17.06, AVANCE AL 19.06, AVANCE AL 21.06, AVANCE AL 24.06, AVANCE AL 26.06, AVANCE AL 27.06, PENDIENTE, Comentario, OC Posición, PAP, SITE, COMENTARIO, ESTADO_PAP, RESPONSABLE3, CONTRA.FEC, CONTRAT.COM, Acción, F.Modifica, ESTADO]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 61 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(PRE_all_act[PRE_all_act.ESTADO.isna()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b51c74c-7074-4950-bc08-0ebb86e132db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(PRE_all_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80e72332-2dea-46c6-9969-8b3117b9a2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Supongamos que df es tu DataFrame\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa88fd2e-5e58-457c-a643-9da096a70d11",
   "metadata": {},
   "source": [
    "# Enbullo la info actualizada del norte en el global\n",
    "    04. Para ello necesito primero darle formato al global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "696e1b59-59bb-4422-8045-dffb73450018",
   "metadata": {},
   "outputs": [],
   "source": [
    "Resultado = PRE_all_act\n",
    "columnas_para_actualizar = ['ANTIGUAMIENTO_PAP', 'PAP','SITE','COMENTARIO','RESPONSABLE3','CONTRA.FEC','CONTRAT.COM','ESTADO_PAP','ESTADO']\n",
    "Resultado_reducido = Resultado[columnas_para_actualizar]\n",
    "#############################################################\n",
    "Prepa_N = Prepa_N.dropna(axis=1, how='all')\n",
    "unhavent_columns = set(Resultado.columns.tolist()) - set(Prepa_N.columns.tolist()) # Columnas que le faltan al grande del chico\n",
    "# Crear las columnas necesarias con dtype 'object'\n",
    "for col in list(unhavent_columns):\n",
    "    Prepa_N[col] = pd.Series(dtype='object')\n",
    "## Df pequeño en el Df grande \n",
    "Resultado.columns = Resultado.columns.str.replace(' ', '')\n",
    "Prepa_N.columns = Prepa_N.columns.str.replace(' ', '')\n",
    "Prepa_N.update(Resultado_reducido, overwrite=True) ### Aqui es dond eembullo la data.\n",
    "Prepa_N.columns = Prepa_N.columns.astype(str)\n",
    "Prepa_N_1 = Ac.convert_columns_to_str(Prepa_N.copy(), ['COMENTARIO'])\n",
    "Prepa_N_1 = Prepa_N_1.drop(columns=['ESTADO_PAP','Acción','SOLICITUD','ESTATUS_SOLICITUD','AREA_RESPONSABLE','ACCION','TIPO_MAT','CE_GESTOR'])\n",
    "Prepa_N_1 = Prepa_N_1.replace({'nan':''})\n",
    "cols = list(Prepa_N_1.columns)\n",
    "Prepa_N_1 = Prepa_N_1[cols[:-9] + ['ANTIGUAMIENTO_PAP','PAP', 'SITE', 'ESTADO','COMENTARIO','RESPONSABLE3','CONTRA.FEC','CONTRAT.COM']]\n",
    "Prepa_N_1.COMENTARIO = Prepa_N_1.COMENTARIO.replace({'1899-12-30 00:00:00' : ''})\n",
    "Prepa_N_1 = Prepa_N_1.dropna(axis=1, how='all') # elimino columnas vacias\n",
    "Prepa_N_1 = Prepa_N_1.loc[:,~Prepa_N_1.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c5e1ef-b52b-43e2-8161-34f991bad91b",
   "metadata": {},
   "source": [
    "# to Excel\n",
    " #### 1. 4all\n",
    " #### 2. 4me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b900815a-6358-4d80-9bff-fce083b932a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4227a565-0a4f-4c3e-828f-bea5f06c8b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutado:  01-07-2024\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "useless_columns= ['DOCUMENTO_REFERENCIA','FONDO','FECH_CONTAB','MON','INDICADOR',\n",
    "                      'PEND_FACT','EA_EM','ESTATUS_ACEPT','RESPONSABLE','DEMORA',\n",
    "                  'SUB_DIRECCION','FECHA_REPORTE','ELEMENTO_PEP','CENFILE','ESTADO_CENFILE','CODIGO_SITE','INDICADOR2',\n",
    "                      'ÁREA','AGRUPADOR','RESPON','NOMBRE SITE','RESPONSABLE_PAP']\n",
    "## Formato Tabla en el excel \n",
    "#Today_D_M=Today_D_M+'_2'\n",
    "Ac.Excel_format(Prepa_N_1,r'\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\Resultados_prepa\\PREPA_UPDATE{}.xlsx'.format(Today_D_M),useless_columns)\n",
    "Ac.Excel_format(Prepa_N_1,r'D:\\Prepa_local\\PREPA_UPDATE{}.xlsx'.format(Today_D_M),useless_columns)\n",
    "\n",
    "filename = r'D:\\Scripts1\\Code\\ActPEA\\CODE\\last_run.json'\n",
    "# Carga la última fecha de ejecución\n",
    "last_run_date = Ac.load_last_run_date(filename)\n",
    "\n",
    "# Comprueba si la celda ya se ha ejecutado hoy\n",
    "if last_run_date != datetime.datetime.now().date():\n",
    "    # Tu código aquí\n",
    "    print('Ejecutado: ',Today_str)\n",
    "    #process_to_bcsv(PRE_all_act,'D:/Prepa/TIME.S/Prepa_TS1.csv',Today_str)\n",
    "    process_to_bcsv(PRE_all_act,r'\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\PowerBI\\Prepa_TS.csv',Today_str)# añado al acumulado\n",
    "\n",
    "    # Guarda la fecha de hoy como la última fecha de ejecución\n",
    "    Ac.save_last_run_date(filename)\n",
    "else:\n",
    "    print(\"El código ya se ha ejecutado hoy.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defc9c79-6752-4a5c-9068-6ab1739038e7",
   "metadata": {},
   "source": [
    "# To BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "539fd14b-1750-4634-b451-0cbacee70a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formated2bi(df_diff:pd.DataFrame,contrata_dic:dict,columns_dic:dict,colum_list):\n",
    "    df_fomarted = df_diff.copy()\n",
    "    # Aplicar la función a los nombres de las columnas\n",
    "    df_fomarted.columns = map(transformar_nombre, df_fomarted.columns) # Normalizo nombres de columna\n",
    "    df_fomarted['SITE'] = df_fomarted['SITE'].str.replace('_', ' ')\n",
    "    df_fomarted['SITE'] = df_fomarted['SITE'].str.title()\n",
    "    df_fomarted['TEXTO BREVE'] = df_fomarted['TEXTO BREVE'].str.title()\n",
    "    df_fomarted['NOMPROVEEDOR'].replace(contrata_dic,inplace=True)\n",
    "    df_fomarted['NOMPROVEEDOR'] = df_fomarted['NOMPROVEEDOR'].str.title()\n",
    "    df_fomarted['MES COMPROMISO'].replace('Setiembre','Septiembre',inplace=True)\n",
    "    df_fomarted.columns = df_fomarted.columns.str.strip()\n",
    "    #Cambio los nombres de las columnas con un Map\n",
    "    df_fomarted.rename(columns=columns_dic, inplace=True)\n",
    "\n",
    "    df_fomarted.columns = df_fomarted.columns.str.title()\n",
    "    \n",
    "    try:\n",
    "        df_fomarted = df_fomarted[colum_list] #@encapsular\n",
    "        print('Formato con CF')\n",
    "    except:\n",
    "        #colum_list.remove('Estado Cenfile')\n",
    "        #colum_list.remove('Antiguamiento Proyectado')\n",
    "        colum_list.remove('Estado Cenfile')\n",
    "        df_fomarted = df_fomarted[colum_list]\n",
    "        print('Formato sin CF')\n",
    "    \n",
    "    #columns_2_drop = [col.title() for col in columns_2_drop]\n",
    "    \n",
    "    #df_fomarted.drop(columns=columns_2_drop,inplace=True)\n",
    "    return df_fomarted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d71190f-af7f-46c5-88e9-576786f4a8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PREPA_UPDATE01.07.xlsx', 'PREPA_UPDATE27.06.xlsx']\n",
      "Formato con CF\n"
     ]
    }
   ],
   "source": [
    "## Revisar esto porque se ejecuto 2 veces el mismo dia pero en diferentes idles\n",
    "dataframes = get_2_df_from_path(r'\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\Resultados_prepa','.xlsx',4,'Sheet1')\n",
    "#dataframes = get_2_df_from_path(r'D:\\Prepa_local','.xlsx',4,'Sheet1')\n",
    "\n",
    "# Ruta de donde extraigo los archivos a comparar,intervalo,Nombre de la hoja\n",
    "dataframes_filt = list(map(filtro,dataframes)) # filtro solo del norte\n",
    "df_diff = calc_diff_V1(dataframes_filt[0],dataframes_filt[1]) # Funcion que me crea la columna de diferencia.\n",
    "df_diff = cleanrows(df_diff)\n",
    "#display(df_diff.info()) \n",
    "\n",
    "df_diff['DIFERENCIA EN SOLES'] = df_diff['DIFERENCIA EN SOLES'].transform(lambda x: x / df_diff['DIFERENCIA EN SOLES'].value_counts()[x] if pd.notnull(x) else x)\n",
    "#display(df_diff.info())\n",
    "#columns_2_drop = [\"CODPROVEEDOR\",\"EA EM\", \"MON\",\"PEND FACT\", \"CONCA\"\n",
    "                         # ,\"ESTATUS ACEPT\", \"ELEMENTO PEP\", \"SUB DIRECCION\",\"FONDO\",\"INDICADOR\", \"RESPONSABLE\",\n",
    "                     # \"DÍAS 2\",\"FECHA CONTRATA\",'Responsable Pap','Codigo Site','Nombre Site','Cenfile']\n",
    "with open(r'D:\\Scripts1\\Code\\ActPEA\\CODE\\Temps\\ListaColumnsBI.pkl', \"rb\") as archivo:\n",
    "    column_list = pickle.load(archivo)\n",
    "df_diff = normalize_company_names(df_diff,'NOMPROVEEDOR')\n",
    "df_diff_m = pd.merge(df_diff,OT_agg_ID_PRO[['SITE','NOMPROVEEDOR','Status OT']],on=['SITE','NOMPROVEEDOR'],how='left')\n",
    "df_diff_m.drop(columns=['ANT-PROYECTADO'],inplace=True)\n",
    "df_diff_m.rename(columns={'Status OT':'ANTIGUAMIENTO PROYECTADO'},inplace = True)\n",
    "\n",
    "\n",
    "df_fomarted = formated2bi(df_diff_m,contrata_dic,columns_dic,column_list) \n",
    "#columns_E = [s.upper() for s in lista_recuperada]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "387836e0-a9d3-4d9a-8c16-3de56ab3fc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 179 entries, 0 to 178\n",
      "Data columns (total 56 columns):\n",
      " #   Column                    Non-Null Count  Dtype         \n",
      "---  ------                    --------------  -----         \n",
      " 0   DOC_COMPRAS               179 non-null    int64         \n",
      " 1   POSIC                     179 non-null    int64         \n",
      " 2   CONCA                     179 non-null    int64         \n",
      " 3   DOCUMENTO_REFERENCIA      179 non-null    int64         \n",
      " 4   FONDO                     67 non-null     object        \n",
      " 5   TEXTO_BREVE               179 non-null    object        \n",
      " 6   CODPROVEEDOR              67 non-null     float64       \n",
      " 7   NOMPROVEEDOR              179 non-null    object        \n",
      " 8   FECH_CONTAB               67 non-null     datetime64[ns]\n",
      " 9   MON                       67 non-null     object        \n",
      " 10  INDICADOR                 67 non-null     object        \n",
      " 11  EA_EM                     67 non-null     float64       \n",
      " 12  PEND_FACT                 67 non-null     float64       \n",
      " 13  PEND_FACT_SOLES           179 non-null    float64       \n",
      " 14  ESTATUS_ACEPT             67 non-null     object        \n",
      " 15  FECH_LIB_AF               0 non-null      datetime64[ns]\n",
      " 16  DEMORA                    67 non-null     float64       \n",
      " 17  ANTIGUAMIENTO             67 non-null     object        \n",
      " 18  RESPONSABLE               67 non-null     object        \n",
      " 19  RESPONSABLE2              179 non-null    object        \n",
      " 20  SUB_DIRECCION             67 non-null     object        \n",
      " 21  FECHA_REPORTE             67 non-null     datetime64[ns]\n",
      " 22  RESPONSABLE_PAP           67 non-null     object        \n",
      " 23  ANTIGUAMIENTO_PAP         178 non-null    object        \n",
      " 24  CENFILE                   67 non-null     object        \n",
      " 25  ESTADO_CENFILE            67 non-null     object        \n",
      " 26  CODIGO_SITE               67 non-null     object        \n",
      " 27  NOMBRE_SITE               67 non-null     object        \n",
      " 28  ELEMENTO_PEP              67 non-null     object        \n",
      " 29  FECHASCOMPROMISO          179 non-null    object        \n",
      " 30  MES-COMPROMISO            179 non-null    object        \n",
      " 31  FECHA-PROY                67 non-null     datetime64[ns]\n",
      " 32  DÍAS-2                    67 non-null     float64       \n",
      " 33  STATUSFINAL               179 non-null    object        \n",
      " 34  INDICADOR2                67 non-null     object        \n",
      " 35  ÁREA                      67 non-null     object        \n",
      " 36  AGRUPADOR                 67 non-null     object        \n",
      " 37  RESPON                    67 non-null     object        \n",
      " 38  AVANCEAL10.06             67 non-null     float64       \n",
      " 39  AVANCEAL12.06             67 non-null     float64       \n",
      " 40  AVANCEAL17.06             67 non-null     float64       \n",
      " 41  AVANCEAL19.06             67 non-null     float64       \n",
      " 42  AVANCEAL21.06             67 non-null     float64       \n",
      " 43  AVANCEAL24.06             67 non-null     float64       \n",
      " 44  AVANCEAL26.06             67 non-null     float64       \n",
      " 45  AVANCEAL27.06             67 non-null     float64       \n",
      " 46  PENDIENTE                 67 non-null     float64       \n",
      " 47  ESTADO                    179 non-null    object        \n",
      " 48  PAP                       179 non-null    object        \n",
      " 49  SITE                      179 non-null    object        \n",
      " 50  COMENTARIO                133 non-null    object        \n",
      " 51  RESPONSABLE3              159 non-null    object        \n",
      " 52  CONTRA.FEC                12 non-null     object        \n",
      " 53  CONTRAT.COM               3 non-null      object        \n",
      " 54  DIFERENCIA EN SOLES       179 non-null    float64       \n",
      " 55  ANTIGUAMIENTO PROYECTADO  96 non-null     object        \n",
      "dtypes: datetime64[ns](4), float64(16), int64(4), object(32)\n",
      "memory usage: 78.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_diff_m.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f2cf03f-482a-44cf-8c59-3c5a73da88cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 179 entries, 0 to 178\n",
      "Data columns (total 23 columns):\n",
      " #   Column                    Non-Null Count  Dtype         \n",
      "---  ------                    --------------  -----         \n",
      " 0   Orden De Compra           179 non-null    int64         \n",
      " 1   Posición                  179 non-null    int64         \n",
      " 2   Documento Referencia      179 non-null    int64         \n",
      " 3   Descripcion               179 non-null    object        \n",
      " 4   Contratista               179 non-null    object        \n",
      " 5   Fecha Contabilidad        67 non-null     datetime64[ns]\n",
      " 6   Pendiente En Soles        179 non-null    float64       \n",
      " 7   Demora                    67 non-null     float64       \n",
      " 8   Antiguamiento             67 non-null     object        \n",
      " 9   Responsable2              179 non-null    object        \n",
      " 10  Fecha Reporte             67 non-null     datetime64[ns]\n",
      " 11  Antiguamiento Pap         178 non-null    object        \n",
      " 12  Fechascompromiso          179 non-null    object        \n",
      " 13  Mes De Compromiso         179 non-null    object        \n",
      " 14  Antiguamiento Proyectado  96 non-null     object        \n",
      " 15  Estado                    179 non-null    object        \n",
      " 16  N° Pap                    179 non-null    object        \n",
      " 17  Site                      179 non-null    object        \n",
      " 18  Comentario                133 non-null    object        \n",
      " 19  Responsable3              159 non-null    object        \n",
      " 20  Comentario Contrata       3 non-null      object        \n",
      " 21  Diferencia En Soles       179 non-null    float64       \n",
      " 22  Estado Cenfile            67 non-null     object        \n",
      "dtypes: datetime64[ns](2), float64(3), int64(3), object(15)\n",
      "memory usage: 32.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_fomarted.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ce81ab-1a45-4462-86d0-c20d99e320d9",
   "metadata": {},
   "source": [
    "## Añado estado de proyecto por coincidencia de nombre y contrata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e566d5c8-536f-450c-bea3-30a6699750c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Estado Cenfile' not in df_fomarted.columns:\n",
    "    df_fomarted['Estado Cenfile'] = '0'\n",
    "else: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa843884-b119-4a33-ad1f-9497068eed7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi actualizado\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57091.359999999964"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_fomarted.to_csv(r'\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\PowerBI\\Reporte Prepasivo.csv',index=False)\n",
    "print('Bi actualizado')\n",
    "\n",
    "#df_fomarted.to_csv(r'C:\\Users\\C26764\\America Movil Peru S.A.C\\EAS - 2\\PBI Dashboard\\Reporte Prepasivo.csv',index=False)\n",
    "df_fomarted['Diferencia En Soles'].sum()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d25f20a-da4d-499d-8a6b-ab3532fe7e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 179 entries, 0 to 178\n",
      "Data columns (total 23 columns):\n",
      " #   Column                    Non-Null Count  Dtype         \n",
      "---  ------                    --------------  -----         \n",
      " 0   Orden De Compra           179 non-null    int64         \n",
      " 1   Posición                  179 non-null    int64         \n",
      " 2   Documento Referencia      179 non-null    int64         \n",
      " 3   Descripcion               179 non-null    object        \n",
      " 4   Contratista               179 non-null    object        \n",
      " 5   Fecha Contabilidad        67 non-null     datetime64[ns]\n",
      " 6   Pendiente En Soles        179 non-null    float64       \n",
      " 7   Demora                    67 non-null     float64       \n",
      " 8   Antiguamiento             67 non-null     object        \n",
      " 9   Responsable2              179 non-null    object        \n",
      " 10  Fecha Reporte             67 non-null     datetime64[ns]\n",
      " 11  Antiguamiento Pap         178 non-null    object        \n",
      " 12  Fechascompromiso          179 non-null    object        \n",
      " 13  Mes De Compromiso         179 non-null    object        \n",
      " 14  Antiguamiento Proyectado  96 non-null     object        \n",
      " 15  Estado                    179 non-null    object        \n",
      " 16  N° Pap                    179 non-null    object        \n",
      " 17  Site                      179 non-null    object        \n",
      " 18  Comentario                133 non-null    object        \n",
      " 19  Responsable3              159 non-null    object        \n",
      " 20  Comentario Contrata       3 non-null      object        \n",
      " 21  Diferencia En Soles       179 non-null    float64       \n",
      " 22  Estado Cenfile            67 non-null     object        \n",
      "dtypes: datetime64[ns](2), float64(3), int64(3), object(15)\n",
      "memory usage: 32.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_fomarted.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "061c356c-5707-42c5-9d30-e8f67ba00d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mes De Compromiso\n",
      "Bloqueo de AF    112\n",
      "Agosto            37\n",
      "Julio             18\n",
      "Septiembre        12\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_fomarted['Mes De Compromiso'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b319e1-9ece-4553-a4f3-e27123970600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935c9ef2-0d7b-4792-bc29-bbc58abd91b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
