{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "71d2565c-69ee-4f9e-adf0-84d84601d24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"D:\\Scripts1\\Code\\ActPEA\\CODE\\Actu_Colums.py\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Actu_Colums as Ac\n",
    "import win32com.client as win32\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from openpyxl.worksheet.table import Table, TableStyleInfo\n",
    "import datetime\n",
    "import os,time\n",
    "import glob\n",
    "from typing import Optional\n",
    "from babel.dates import format_date, format_datetime, Locale\n",
    "import re\n",
    "import pickle\n",
    "import subprocess\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c4d6b5-0b0a-40dd-a282-e42b0332c9ab",
   "metadata": {},
   "source": [
    "### El flujo de actualizacion es el siguiente : \n",
    "    -01.Extraigo un fragmente del prepa_global_N solo de EI\n",
    "    -01,5. Concateno los Bloqueos de Factura\n",
    "    -02.Hereda informacion del Prepa_old\n",
    "    -03.Actualizo info del PAP\n",
    "    -03,5. Añado analistas\n",
    "    -04.Vuelvo a meter el fragmento en el prepa_global_N\n",
    "    -05.To Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b5f85d-3035-48b0-9aa4-9ae2dd8603de",
   "metadata": {},
   "source": [
    "## Time pointer and funcions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8f28c3fb-c860-476b-9646-1ed553a65049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hoy es: 29.08\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "Today_date = datetime.date.today()\n",
    "filas_in = Today_date\n",
    "Today_str = datetime.date.today().strftime('%d-%m-%Y')\n",
    "Today_D_M = Today_str[0:5]\n",
    "Today_D_M = [Today_D_M[0:2],'.',Today_D_M[3:6]]\n",
    "Today_D_M = ''.join(Today_D_M)\n",
    "with open(r\"\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\PowerBI\\Dimensiones\\temp.pkl\", \"rb\") as f:\n",
    "    filas_out = pickle.load(f)\n",
    "print('Hoy es:',Today_D_M)\n",
    "def process_df_V1(df: pd.DataFrame,clases_unicas: list,index=['NOMPROVEEDOR','ESTADO']):\n",
    "    \"\"\" La funcion recibe un df, y las clases unicas\n",
    "        Y devuelve el df uniformizado para cada clase\n",
    "        Llena los valores faltantes con 0\"\"\"\n",
    "    pivot_table = pd.pivot_table(df, values='PEND_FACT_SOLES', index=index, aggfunc=pd.Series.sum) #Agrupo \n",
    "    grupos = pivot_table.groupby('NOMPROVEEDOR') #creo un data frame para cada contrata\n",
    "    # Para cada grupo, crea un nuevo dataframe y guárdalo en un diccionario\n",
    "    dataframes = {}\n",
    "    for nombre, datos in grupos: # Creo los df dentro del dic\n",
    "        dataframes[nombre] = datos\n",
    "    for key in dataframes.keys(): # para cada df en el dic\n",
    "        df = dataframes[key].reset_index()\n",
    "        df1 = df.set_index('ESTADO').reindex(clases_unicas).reset_index() #normalizo, creando filas para todos los ESTADOS\n",
    "        df1.NOMPROVEEDOR = key  #Relleno los NAN\n",
    "        df1 = df1.fillna(0)  \n",
    "        dataframes[key] = df1   #Reescribo los Df para cada clave\n",
    "    df_concat = pd.concat(dataframes.values(),ignore_index=True) # Compacto todo los df del dic en uno grande \n",
    "    pivot_table_1 = pd.pivot_table(df_concat, values='PEND_FACT_SOLES', index=index, aggfunc=pd.Series.sum) #agrupo de nuevo\n",
    "    return pivot_table_1\n",
    "################################################################################################3\n",
    "\n",
    "def calc_diff_V1(df1: pd.DataFrame,df2: pd.DataFrame, index= ['NOMPROVEEDOR', 'ESTADO']) -> pd.DataFrame:\n",
    "    \"\"\" entran 2 dataframes , los proceso , y en base a los df procesados \n",
    "        calculo la diferencia en una nueva columna, \n",
    "        y añado esta columna de diferencia al df actual \"\"\"\n",
    "    clases_unicas = pd.concat([df1['ESTADO'], df2['ESTADO']]).unique() #Creo listas de claves unicas\n",
    "    df1_proces = process_df_V1(df1,clases_unicas)\n",
    "    df2_proces = process_df_V1(df2,clases_unicas)\n",
    "    \n",
    "    df_diff = df1_proces - df2_proces\n",
    "    diff_pivot_table_reset = df_diff.reset_index()\n",
    "    diff_pivot_table_reset.rename(columns={'PEND_FACT_SOLES': 'DIFERENCIA EN SOLES'}, inplace=True)\n",
    "    # Merge diff_pivot_table_filled_reset con PrePa_O_EI\n",
    "    PrePa_O_EI = pd.merge(df1, diff_pivot_table_reset, on=index, how='left')    \n",
    "    return PrePa_O_EI\n",
    "\n",
    "def filtro(df):\n",
    "    return df[df.RESPONSABLE2 == 'Eduardo Iberico'] #Filtro \n",
    "    \n",
    "def get_2_df_from_path(path: str,extension: str, Week_Int: int,sheet_name: str) -> list[pd.DataFrame]:\n",
    "    \"\"\" Extrae los archivos mas recientes de un path\"\"\"\n",
    "    path= path+'/*'\n",
    "    tipo_de_archivo = '*'+ extension\n",
    "    archivos = glob.glob(path + tipo_de_archivo)\n",
    "    archivos_ordenados = sorted(archivos, key=os.path.getctime, reverse=True)\n",
    "    nombres_de_archivos = [os.path.basename(archivo) for archivo in archivos_ordenados[:1] + archivos_ordenados[Week_Int-1:Week_Int]] \n",
    "    print(nombres_de_archivos)\n",
    "    dataframes = []  # Lista para almacenar los DataFrames\n",
    "    for nombre in nombres_de_archivos:\n",
    "        path_of_e = path[:-1] + '{}'.format(nombre)\n",
    "        df = pd.read_excel(path_of_e, sheet_name=sheet_name, dtype={'COMENTARIO': str})\n",
    "        dataframes.append(df)\n",
    "    return dataframes\n",
    "        \n",
    "def convert_to_date(val):\n",
    "    try:\n",
    "        return pd.Timestamp('1899-12-30') + pd.Timedelta(int(float(val)), 'D')\n",
    "    except ValueError:\n",
    "        return val\n",
    "\n",
    "def get_recent_df(Carpeta_path: str, sheet_name: str):\n",
    "    \"\"\" Devuelve el df de la hoja especifica, del archivo mas reciente modificado de la carpeta especificada\"\"\"\n",
    "    Path_n= Carpeta_path + '/*'\n",
    "    tipo_de_archivo = '*.xlsx'\n",
    "    # Busca el archivo más reciente\n",
    "    archivos = glob.glob(Path_n + tipo_de_archivo)\n",
    "    archivo_mas_reciente = max(archivos, key=os.path.getmtime)\n",
    "    nombre_del_archivo_N = os.path.basename(archivo_mas_reciente)\n",
    "    print(archivo_mas_reciente)\n",
    "    # Lee el archivo sin especificar el tipo de datos\n",
    "    df = pd.read_excel(archivo_mas_reciente , sheet_name=sheet_name)\n",
    "    # Si la columna \"COMENTARIO\" existe, cambia su tipo de datos a str\n",
    "    if 'COMENTARIO' in df.columns:\n",
    "        df['COMENTARIO'] = df['COMENTARIO'].astype(str)\n",
    "    return df,archivo_mas_reciente\n",
    "def get_recent_c_df(Carpeta_path: str, sheet_name: str):\n",
    "    \"\"\" Devuelve el df de la hoja especifica, del archivo mas reciente creado de la carpeta especificada\"\"\"\n",
    "    Path_n= Carpeta_path + '/*'\n",
    "    tipo_de_archivo = '*.xlsx'\n",
    "    # Busca el archivo más reciente\n",
    "    archivos = glob.glob(Path_n + tipo_de_archivo)\n",
    "    archivo_mas_reciente = max(archivos, key=os.path.getctime)\n",
    "    nombre_del_archivo_N = os.path.basename(archivo_mas_reciente)\n",
    "    print(archivo_mas_reciente)\n",
    "    # Lee el archivo sin especificar el tipo de datos\n",
    "    df = pd.read_excel(archivo_mas_reciente , sheet_name=sheet_name)\n",
    "    # Si la columna \"COMENTARIO\" existe, cambia su tipo de datos a str\n",
    "    if 'COMENTARIO' in df.columns:\n",
    "        df['COMENTARIO'] = df['COMENTARIO'].astype(str)\n",
    "    return df,nombre_del_archivo_N\n",
    "def cleanrows(df):\n",
    "    if 'STATUSFINAL' in df.columns:\n",
    "        df = df[df['STATUSFINAL'] == 'PENDIENTE']\n",
    "    elif 'STATUS FINAL' in df.columns:\n",
    "        df = df[df['STATUS FINAL'] == 'PENDIENTE']\n",
    "    return df\n",
    "\n",
    "def process_to_bcsv(df : pd.DataFrame,ruta_del_csv : str,Fecha: str):\n",
    "\n",
    "    df['TIME'] = Fecha\n",
    "    pivot_table = pd.pivot_table(df, values='PEND_FACT_SOLES', index=['TIME','MES-COMPROMISO','NOMPROVEEDOR','ESTADO'], aggfunc=pd.Series.sum) #Agrupo \n",
    "    df_reset = pivot_table.reset_index(drop=False)\n",
    "    #convetir la columna del agrupado al formato d efehca \n",
    "    # Convierte la columna 'Fecha' a datetime\n",
    "    df_reset['TIME'] = pd.to_datetime(df_reset['TIME'],format = '%d-%m-%Y')\n",
    "    \n",
    "    # Formatea la columna 'Fecha'\n",
    "    df_reset['TIME Format'] = df_reset['TIME'].apply(lambda x: format_date(x, 'EEE dd-MM-yyyy', locale=Locale('es', 'ES')))\n",
    "    df_reset.to_csv(ruta_del_csv, mode='a', header=False,index=False)\n",
    "def transformar_name(valor):\n",
    "    str(valor)\n",
    "    if valor.isupper():\n",
    "        return valor\n",
    "    else:\n",
    "        palabras = valor.split()\n",
    "        return ' '.join([palabras[0], palabras[2]])\n",
    "def process_PAP(value):\n",
    "    if any(map(str.isdigit, str(value))):\n",
    "        value_float = int(value)\n",
    "        value_str = str(value_float)\n",
    "        return value_str\n",
    "    else:\n",
    "        return value\n",
    "def load_df_by_name(directorios:str, cadena:str) -> pd.DataFrame:\n",
    "    df_list = []\n",
    "    for dictory in directorios:\n",
    "        ruta = buscar_archivo_mas_antiguo(dictory, cadena)\n",
    "        try:\n",
    "            df = pd.read_excel(ruta, sheet_name='Hoja2')\n",
    "            #print('Machea con nombre de hoja2')\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}. Trying with 'Sheet1'\")\n",
    "            df = pd.read_excel(ruta, sheet_name='Sheet1')\n",
    "            #print('Machea con nombre de Sheet1')\n",
    "        print(ruta)\n",
    "        df_list.append(df)\n",
    "    return df_list\n",
    "def buscar_archivo_mas_antiguo(directorio, cadena):\n",
    "    archivos_coincidentes = [os.path.join(directorio, nombre_archivo) for nombre_archivo in os.listdir(directorio) if cadena in nombre_archivo]\n",
    "    if not archivos_coincidentes:\n",
    "        return None\n",
    "    else:\n",
    "        archivo_mas_antiguo = min(archivos_coincidentes, key=os.path.getmtime)\n",
    "        return archivo_mas_antiguo\n",
    "def transformar_nombre(nombre):\n",
    "    nombre = nombre.upper()\n",
    "    nombre = nombre.replace('_', ' ')\n",
    "    nombre = nombre.replace('-', ' ')\n",
    "    return nombre\n",
    "\n",
    "contrata_dic = {'DELTA ELECTRONICS (PERU) INC. S.R.L ELTEK PERU S.R.L.':'DELTA',\n",
    "                'COMUNICACION FUTURA SOCIEDAD ANONIM':'COMFUTURA'}\n",
    "columns_dic = {\"CONTRA.FEC\":\"Fecha Contrata\" , \"CONTRAT.COM\": \"Comentario Contrata\"\n",
    "               ,\"Dif Soles\":\"Diferencia Soles\", \"DOC COMPRAS\": \"Orden de Compra\" \n",
    "               , \"DOCUMENTO REFERENCIA\": \"DOCUMENTO REFERENCIA\",\"NOMPROVEEDOR\" :\"CONTRATISTA\"\n",
    "               , \"PAP\":\"N° PAP\", \"PEND FACT SOLES\":\"PENDIENTE EN SOLES\"\n",
    "               , \"TEXTO BREVE\": \"DESCRIPCION\"\n",
    "               , \"POSIC\":\"POSICIÓN\",\"DESCRIPCION\": \"DESCRIPCIÓN\"\n",
    "               , \"ANT PROYECTADO\": \"Antiguamiento Proyectado\", \"ANTIGUAMIENTO PAP\" : \"ANTIGUAMIENTO PAP\"\n",
    "               , \"FECHA REPORTE\": \"FECHA REPORTE\", \"MES COMPROMISO\" : \"MES DE COMPROMISO\"\n",
    "               , \"POS\": \"POSICION\", 'FECH CONTAB':'FECHA CONTABILIDAD'}\n",
    "def clean_date(df,column):\n",
    "    df.loc[:,column] = pd.to_datetime(df[column])\n",
    "    df[column] = pd.to_datetime(df[column], format='%Y-%m-%d')\n",
    "    df[column] = df[column].astype(str)\n",
    "    return df\n",
    "# Ordeno los KEYS\n",
    "def change_rows(df_concat:pd.DataFrame,df_duplicados:pd.DataFrame,Prepa_N:pd.DataFrame):\n",
    "    df_concat = df_concat.sort_values(by=['CONCA', 'DOCUMENTO_REFERENCIA'], ascending=[True, True])\n",
    "    # Crear un DataFrame con las keys y las columna que me interesa.\n",
    "    df_info = df_duplicados.drop_duplicates(subset=['CONCA', 'DOCUMENTO_REFERENCIA'], keep='first')[['CONCA', 'DOCUMENTO_REFERENCIA', 'FECHASCOMPROMISO']]\n",
    "    \n",
    "    #la base en la que voy a editar las columnas,borro esa columna\n",
    "    df_dupli = df_duplicados.drop('FECHASCOMPROMISO', axis=1).drop_duplicates(subset=['CONCA', 'DOCUMENTO_REFERENCIA'], keep='last')\n",
    "    df_final = merge_dataframes(df_dupli,df_info,['CONCA','DOCUMENTO_REFERENCIA'],index=df_dupli.index) #concateno, es como si intercambiara columnas\n",
    "    df_final\n",
    "    return Prepa_N.update(df_final,overwrite=True)#Finalmente con eso sobreescribro el original \n",
    "\n",
    "\n",
    "def drop_duplicatesR(df_duplicados:pd.DataFrame,BDF:pd.DataFrame,Prepa_N:pd.DataFrame):\n",
    "    df2Rem = df_duplicados.drop_duplicates(subset=['CONCA', 'DOCUMENTO_REFERENCIA'],keep='first')\n",
    "    list2Rem = df2Rem.index.tolist()\n",
    "    BDF_no_D = BDF.drop(list2Rem,axis=0)\n",
    "    Prepa_N = pd.concat([BDF_no_D, Prepa_N]) # este es un temporal para comprobar si hay filas duplicadas \n",
    "    print(f\"los siguientes index en el BDF estan en el reporte aun: {list2Rem}\")\n",
    "    Prepa_N = Prepa_N.drop_duplicates(subset=['CONCA', 'DOCUMENTO_REFERENCIA'])\n",
    "    return Prepa_N\n",
    "def merge_dataframes(df1, df2, merge_on, index):\n",
    "    \n",
    "    merged_df = pd.merge(df1, df2, on=merge_on, how='left')\n",
    "    display(len(merged_df))\n",
    "    #display('Filas duplicadas:',merged_df[merged_df.duplicated(subset=['DOCUMENTO_REFERENCIA','CONCA'])]) \n",
    "    #return merged_df\n",
    "    #merged_df = merged_df.drop_duplicates()\n",
    "    merged_df = merged_df.drop_duplicates(subset=['CONCA','DOCUMENTO_REFERENCIA'],keep='first')\n",
    "    #return merged_df\n",
    "    print(\"filas del merge: \",len(merged_df))\n",
    "    print(\"filas index: \",len(index))\n",
    "    merged_df.set_index(index, inplace=True)\n",
    "    return merged_df\n",
    "def normalize_company_names(df, column):\n",
    "    \"\"\"Normaliza los nombres de las empresas en la columna especificada del DataFrame.\"\"\"\n",
    "    # Reemplaza \"SAC\" o \"S. A. C.\" al final de los nombres de las empresas con \"S.A.C\"\n",
    "    df[column] = df[column].str.replace(r\"(SAC|S\\. ?A\\. ?C\\.)$\", \"S.A.C.\", regex=True)\n",
    "    return df\n",
    "def compact_rows(df:pd.DataFrame, columns:list, delimiter:str ='/'):\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "    \n",
    "    # Usar 'join' como función de agregación para concatenar los valores\n",
    "    agg_func = {col: lambda x: '/'.join(x.unique()) for col in df.columns if col not in  columns}\n",
    "    \n",
    "    # Agrupar por 'OC Posición' y aplicar la función de agregación\n",
    "    df1 = df.groupby(columns).agg(agg_func).reset_index()\n",
    "    return df1\n",
    "def update_and_rename(df1, update_cols, new_names):\n",
    "    df = df1.copy()\n",
    "    for col in update_cols:\n",
    "        df[col[0]].update(df[col[1]])\n",
    "    df.rename(columns=new_names, inplace=True)\n",
    "    df.drop(columns=[col for sublist in update_cols for col in sublist[1:]], inplace=True)\n",
    "    return df\n",
    "def data_date(path:str):\n",
    "    modification_time = os.path.getmtime(path)\n",
    "    dt = datetime.fromtimestamp(modification_time)  \n",
    "    dt = dt.date()\n",
    "    # Formatea la fecha en el formato deseado\n",
    "    return dt\n",
    "def combine_and_rename(df1, combine_cols, new_names):\n",
    "    df = df1.copy()\n",
    "    for cols in combine_cols:\n",
    "        df[cols[0]] = df[cols[0]].combine_first(df[cols[1]])\n",
    "    df.rename(columns=new_names, inplace=True)\n",
    "    df.drop(columns=[col for sublist in combine_cols for col in sublist[1:]], inplace=True)\n",
    "    return df\n",
    "def limpio_duplicados(filas_in,filas_out):\n",
    "    if filas_in <= filas_out: #Corrigo las filas duplicadas//\n",
    "        with open(r'\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\PowerBI\\list.pkl', \"rb\") as f:\n",
    "            Colums_2Act = pickle.load(f)\n",
    "    else:\n",
    "        with open(r'\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\PowerBI\\list_V2.pkl', \"rb\") as f:\n",
    "            Colums_2Act = pickle.load(f)\n",
    "    return Colums_2Act\n",
    "    \n",
    "lista_columns_standar = ['DOC_COMPRAS','POSIC','CONCA','DOCUMENTO_REFERENCIA','FONDO','TIPO_MAT','TEXTO_BREVE','CODPROVEEDOR','NOMPROVEEDOR','FECH_CONTAB',\n",
    "                         'MON','INDICADOR','EA_EM','FACTURADO','PEND_FACT','PEND_FACT_SOLES','ESTATUS_ACEPT','FECH_LIB_AF','DEMORA','ANTIGUAMIENTO','RESPONSABLE',\n",
    "                         'RESPONSABLE2','CE_GESTOR','SUB_DIRECCION','FECHA_REPORTE','SOLICITUD','ESTATUS_SOLICITUD','AREA_RESPONSABLE','RESPONSABLE_PAP','ACCION','ANTIGUAMIENTO_PAP',\n",
    "                         'ELEMENTO_PEP','FECHAS COMPROMISO','MES - COMPROMISO','DÍAS -2','ANT-PROYECTADO']\n",
    "def get_recent_df_B(Carpeta_path: str, sheet_name: str):\n",
    "    \"\"\" Devuelve el df de la hoja especifica, del archivo mas reciente sin guion bajo creado de la carpeta especificada\"\"\"\n",
    "    Path_n= Carpeta_path + '/*'\n",
    "    tipo_de_archivo = '*.xlsx'\n",
    "    # Busca el archivo más reciente\n",
    "    archivos = glob.glob(Path_n + tipo_de_archivo)\n",
    "    # Filtra los archivos que no contienen \"_\" en su nombre\n",
    "    archivos_sin_guion_bajo = [archivo for archivo in archivos if \"_\" not in os.path.basename(archivo)]\n",
    "    archivo_mas_reciente = max(archivos_sin_guion_bajo, key=os.path.getctime)\n",
    "    nombre_del_archivo_N = os.path.basename(archivo_mas_reciente)\n",
    "    print(archivo_mas_reciente)\n",
    "    # Lee el archivo sin especificar el tipo de datos\n",
    "    df = pd.read_excel(archivo_mas_reciente , sheet_name=sheet_name)\n",
    "    # Si la columna \"COMENTARIO\" existe, cambia su tipo de datos a str\n",
    "    if 'COMENTARIO' in df.columns:\n",
    "        df['COMENTARIO'] = df['COMENTARIO'].astype(str)\n",
    "    return df,nombre_del_archivo_N\n",
    "\n",
    "def get_near_time_path(path2find:str,path2search:str,kind2search:str,time_jump:int):\n",
    "    Fecha_creacion2find = os.path.getctime(glob.glob(path2find)[0])\n",
    "    Fecha_N_before= Fecha_creacion2find - time_jump #2Meses\n",
    "    print(f\"Busco archivo cercano a:{time.ctime(Fecha_N_before)}\")\n",
    "    archivos = glob.glob(path2search+\"*\"+kind2search)\n",
    "    date_archivos = list(map(lambda x: os.path.getctime(x),archivos)) ## lista de times\n",
    "    resultado = numero_mas_cercano(date_archivos,Fecha_N_before) # busco el time mas close\n",
    "    ruta_EA = archivos[int(date_archivos.index(resultado))] # con ese valor obtengo la ruta \n",
    "    print(ruta_EA)\n",
    "    return ruta_EA\n",
    "def numero_mas_cercano(lista, objetivo):\n",
    "    return min(lista, key=lambda x: abs(x - objetivo))    \n",
    "def pre_proces(df: pd.DataFrame ,columns_2str: list[str] ,column_filter: str,C_format) -> pd.DataFrame: \n",
    "    \"\"\"Anotaciones del tipo de cada parametro para hacer la funcin mas descriptiva\"\"\"\n",
    "    ## Preprosecing of PREP_NEW\n",
    "    df = convert_columns(df.copy(),columns_2str,C_format ) #Convert to str a key column\n",
    "    df_EI = df.loc[df[column_filter] == 'Eduardo Iberico']#Filter\n",
    "    df_EI = df_EI.copy()  # Crea una copia del DataFrame original para evitar modificar los datos originales\n",
    "    return df_EI\n",
    "def convert_columns(df, columns,type):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].astype(type)\n",
    "    return df    \n",
    "\n",
    "def formated2bi(df_diff: pd.DataFrame, contrata_dic: dict, columns_dic: dict, colum_list: list):\n",
    "    df_formated = df_diff.copy()\n",
    "    \n",
    "    # Normalizo nombres de columna\n",
    "    df_formated.columns = map(transformar_nombre, df_formated.columns)\n",
    "    df_formated['SITE'] = df_formated['SITE'].str.replace('_', ' ').str.title()\n",
    "    df_formated['TEXTO BREVE'] = df_formated['TEXTO BREVE'].str.title()\n",
    "    df_formated['NOMPROVEEDOR'].replace(contrata_dic, inplace=True)\n",
    "    df_formated['NOMPROVEEDOR'] = df_formated['NOMPROVEEDOR'].str.title()\n",
    "    df_formated['MES COMPROMISO'].replace('Setiembre', 'Septiembre', inplace=True)\n",
    "    df_formated.columns = df_formated.columns.str.strip()\n",
    "    \n",
    "    # Cambio los nombres de las columnas con un Map\n",
    "    df_formated.rename(columns=columns_dic, inplace=True)\n",
    "    df_formated.columns = df_formated.columns.str.title()\n",
    "    df_formated = standar_columns(df_formated,column_list) # Seguro anti falta de columnas \n",
    "\n",
    "    return df_formated\n",
    "\n",
    "def standar_columns(df:pd.DataFrame,columns:list[str]):\n",
    "    try:\n",
    "        df1 = df[columns].copy()\n",
    "    except KeyError as e:\n",
    "        # Identificamos las columnas faltantes\n",
    "        missing_columns = list(set(columns) - set(df.columns))\n",
    "        # Rellenamos las columnas faltantes con valores vacíos\n",
    "        for col in missing_columns:\n",
    "            df.loc[:,col] = ''\n",
    "        # Seleccionamos las columnas nuevamente\n",
    "        df1 = df[columns].copy()   \n",
    "    return df1  \n",
    "with open(r'D:\\Scripts1\\Code\\ActPEA\\CODE\\Temps\\ListaColumnsBI.pkl', \"rb\") as archivo: # Columnas estadar para el PBI\n",
    "    column_list = pickle.load(archivo)\n",
    "\n",
    "    \n",
    "BDF_dntdo = [   450066161920,\n",
    "                450066161930,\n",
    "                450066161910,\n",
    "                \n",
    "                450066025560,\n",
    "                4500660255150,\n",
    "                4500660255140,\n",
    "                450066025540,\n",
    "                450066025550,\n",
    "                450063536120,\n",
    "                450063536130,\n",
    "                450063536110,\n",
    "    \n",
    "                450067094910,\n",
    "                4500660086160,\n",
    "                4500660087290,\n",
    "                4500660087470,\n",
    "                450065154150]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f6562-3386-443b-b445-a54c7f61816f",
   "metadata": {},
   "source": [
    "# Cargo Archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae30cf-d755-40d3-9fe4-dd55e9d2e911",
   "metadata": {},
   "source": [
    "## Cargo Prepa_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "31d45795-4df6-4be3-9dab-90dd09d52b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\Resultados_prepa\\PREPA_UPDATE29.08.xlsx\n"
     ]
    }
   ],
   "source": [
    "PrePa_O,_ = get_recent_c_df(r'\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\Resultados_prepa','Sheet1') \n",
    "#PrePa_O,_ = get_recent_df(r'D:\\Prepa_local','Sheet1')\n",
    "## Manejo los comentarios por si tiene fechas \n",
    "PrePa_O['COMENTARIO'] = PrePa_O['COMENTARIO'].apply(convert_to_date)\n",
    "#CPX['FECHA'] = CPX['FECHA'].apply(convert_to_date)\n",
    "PrePa_O = PrePa_O.dropna(subset=['CONCA'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5659dc24-2d81-49e4-9b39-419d0c25081f",
   "metadata": {},
   "source": [
    "## OTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "237c9e37-495f-4025-ada2-6b25b91b9d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Scripts1\\Code\\ActPEA\\archvis\\OTs\\OTS 21.08.2024.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\C26764\\AppData\\Local\\Temp\\ipykernel_27928\\3491332154.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  OT['Fecha de Creación'] = pd.to_datetime(OT['Fecha de Creación'],dayfirst=True).dt.date\n"
     ]
    }
   ],
   "source": [
    "OT,_= get_recent_df(r'D:\\Scripts1\\Code\\ActPEA\\archvis\\OTs','Hoja2')\n",
    "OT = OT[OT['Status OT'] != 'PDTE CONTRATA'] #\t\n",
    "OT = OT[OT['Status OT'] != 'PDTE RESPONSABLE'].copy()\n",
    "OT['Fecha de Creación'] = pd.to_datetime(OT['Fecha de Creación'],dayfirst=True).dt.date\n",
    "OT['Fecha de Creación'] = pd.to_datetime(OT['Fecha de Creación'], format='%d-%m-%Y')\n",
    "fecha_limite = pd.Timestamp('2021-11-01')\n",
    "OT_cut = OT[OT['Fecha de Creación'] > fecha_limite].copy() # Corto las OTs solo de este año \n",
    "OT_cut = normalize_company_names(OT_cut,'Contrata')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bbc2c3-5cbd-4b66-8296-46ca7bf617e2",
   "metadata": {},
   "source": [
    "## Cargo Prepa_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8fc4fb0d-996b-4996-8ab6-f05473195009",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/Scripts1/Code/ActPEA/archvis/Pre_pa/NEW\\Prepasivo completo al 27.08.24.xlsx\n",
      "El reporte es de hace 2 dias\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "#Carpeta de informes base\n",
    "Prepa_N,path_archivo = get_recent_df('D:/Scripts1/Code/ActPEA/archvis/Pre_pa/NEW/','DATA') \n",
    "Prepa_N = cleanrows(Prepa_N)\n",
    "# Obtén la última fecha de modificación del archivo\n",
    "data_time = data_date(path_archivo)\n",
    "a = Today_date-data_time\n",
    "if data_time == Today_date:\n",
    "    reporte_old = False\n",
    "    print(\"El reporte es actual\")\n",
    "elif data_time < Today_date:\n",
    "    report_old = True\n",
    "    print(f\"El reporte es de hace {a.days} dias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c134dd11-3728-4c42-b341-7e45ea1dbaf7",
   "metadata": {},
   "source": [
    "## Cargo EAs_old 4 responsables \n",
    "\n",
    "1. Fecha de cracion del prepa Actual\n",
    "2. Resto la fecha menos 2 meses\n",
    "3. Cargo la ruta de los archivos y obtengo la lista de tiempos de todos los archivos\n",
    "4. Encuentro la fecha mas cercana\n",
    "5. devuelve la ruta del EA mas cercano\n",
    "6. Cargo el archivo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1bd7b3bd-02db-4307-adbb-762b8e3b3bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Busco archivo cercano a:Thu Jun 27 12:51:59 2024\n",
      "C:\\Users\\C26764\\America Movil Peru S.A.C\\EAS - 1\\EAUPDATE25.06.xlsx\n"
     ]
    }
   ],
   "source": [
    "ruta_EA = get_near_time_path(path_archivo,r\"C:\\Users\\C26764\\America Movil Peru S.A.C\\EAS - 1/\",\n",
    "                             \".xlsx\",5260032)\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "279025fd-ca16-46f2-b074-74d2a591b5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EA_data = pd.read_excel(ruta_EA)\n",
    "EA_data_cut = EA_data[[\"CONCATENADO\",\"RESPONSABLE_DE_EA\"]]\n",
    "dicc_respo = EA_data_cut.set_index('CONCATENADO')['RESPONSABLE_DE_EA'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c5a68a3f-e46d-4fd1-aaf3-09a9029f6ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1267 entries, 0 to 1266\n",
      "Data columns (total 2 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   CONCATENADO        1267 non-null   int64 \n",
      " 1   RESPONSABLE_DE_EA  1253 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 19.9+ KB\n"
     ]
    }
   ],
   "source": [
    "EA_data_cut.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cfe123-bafe-41a1-9cf3-e0c7f921d659",
   "metadata": {},
   "source": [
    "## Cargo PAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1487e057-4c24-4291-b916-3335cfdfe0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Scripts1\\Code\\ActPEA\\archvis\\PAP\\PAP 29.08_2.xlsx\n",
      "D:\\Scripts1\\Code\\ActPEA\\archvis\\PAP\\Administrativo\\PAP A 29.08.2024_2.xlsx\n"
     ]
    }
   ],
   "source": [
    "PAP_O,_ = get_recent_df(r'D:\\Scripts1\\Code\\ActPEA\\archvis\\PAP','Hoja2')# PAP de Otros\n",
    "#PAP_O,_ = get_recent_df_B(r'D:\\Scripts1\\Code\\ActPEA\\archvis\\PAP','Hoja2')# PAP de Otros\n",
    "\n",
    "PAP_A,_ = get_recent_df(r'D:\\Scripts1\\Code\\ActPEA\\archvis\\PAP\\Administrativo','Hoja2')# PAP de Admin\n",
    "#PAP_A,_ = get_recent_df_B(r'D:\\Scripts1\\Code\\ActPEA\\archvis\\PAP\\Administrativo','Hoja2')# PAP de Admin\n",
    "\n",
    "PAP = pd.concat([PAP_O,PAP_A])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3737d0b0-1712-4774-baf6-e3e79330f151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Hay una variacion de columnas en la Base,['CENFILE', 'CODIGO_SITE', 'ESTADO_CENFILE', 'INDICADOR2', 'FECHA - PROY', 'STATUS FINAL', 'NOMBRE_SITE'] \n",
      "¿Deseas continuar con la ejecución? (s/n):  S\n"
     ]
    }
   ],
   "source": [
    "\n",
    "columns_E = lista_columns_standar\n",
    "columns_Ac = Prepa_N.columns.tolist() \n",
    "if columns_Ac != lista_columns_standar:\n",
    "    Columns_diff = list(set(columns_Ac) - set(columns_E))\n",
    "    respuesta = input(f'Hay una variacion de columnas en la Base,{Columns_diff} \\n¿Deseas continuar con la ejecución? (s/n): ')\n",
    "    if respuesta.lower() != 's':\n",
    "            sys.exit()\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215bb0a3-b225-41cb-8f03-a2136e66e302",
   "metadata": {},
   "source": [
    "## Cargo los Bloqueos de Factura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a2a67960-3634-4ad9-b470-fec68057e350",
   "metadata": {},
   "outputs": [],
   "source": [
    "BDF = pd.read_csv(r'D:\\Scripts1\\Code\\ActPEA\\CODE\\Temps\\BAF_4FC.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "08fc6b25-4092-4cb1-a88e-8d17a56bf8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'STATUSFINAL' in Prepa_N.columns:\n",
    "    BDF['STATUSFINAL'] == 'PENDIENTE'\n",
    "    deep_columns = BDF[['MES - COMPROMISO','STATUS FINAL','FECHASCOMPROMISO']]  \n",
    "\n",
    "elif 'STATUS FINAL' in Prepa_N.columns:\n",
    "    BDF['STATUS FINAL'] == 'PENDIENTE'\n",
    "    deep_columns = BDF[['MES - COMPROMISO','STATUS FINAL','FECHASCOMPROMISO']]  \n",
    "else:\n",
    "    deep_columns = BDF[['MES - COMPROMISO','FECHASCOMPROMISO']]  \n",
    "\n",
    "BDF.drop_duplicates(subset=['CONCA','DOCUMENTO_REFERENCIA'],inplace=True)\n",
    "BDF.drop([16],axis=0,inplace=True) # duplicado\n",
    "# Selecciona las primeras 10 columnas\n",
    "first_ten = BDF.iloc[:, :8]\n",
    "\n",
    "# Combina las columnas seleccionadas en un nuevo DataFrame\n",
    "BDF_cut = pd.concat([first_ten, deep_columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d9b8d3b9-4bbd-4354-82c9-64459dec73e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 112 entries, 0 to 112\n",
      "Data columns (total 11 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   DOC_COMPRAS           112 non-null    float64\n",
      " 1   POSIC                 112 non-null    float64\n",
      " 2   CONCA                 112 non-null    int64  \n",
      " 3   DOCUMENTO_REFERENCIA  112 non-null    float64\n",
      " 4   TEXTO_BREVE           112 non-null    object \n",
      " 5   PEND_FACT_SOLES       112 non-null    float64\n",
      " 6   NOMPROVEEDOR          112 non-null    object \n",
      " 7   RESPONSABLE2          112 non-null    object \n",
      " 8   MES - COMPROMISO      112 non-null    object \n",
      " 9   STATUS FINAL          112 non-null    object \n",
      " 10  FECHAS COMPROMISO     112 non-null    object \n",
      "dtypes: float64(4), int64(1), object(6)\n",
      "memory usage: 10.5+ KB\n"
     ]
    }
   ],
   "source": [
    "BDF_cut.dropna(subset='CONCA',inplace=True)\n",
    "BDF_cut.CONCA = BDF_cut.CONCA.astype('int64')\n",
    "BDF_cut.rename(columns={'FECHASCOMPROMISO':'FECHAS COMPROMISO'},inplace=True)\n",
    "BDF_cut.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a566b6a9-a2b8-4819-9e6d-0a5bdeb667ac",
   "metadata": {},
   "source": [
    "## Compruebo las filas que voy a insertar\n",
    "- La variable reporte_old(boolean) indica si el reporte de prepa es nuevo o antiguo\n",
    "- La variable row_e indica si las filas del bloque existen en el PREPA_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a5ad3d93-7c83-4d8d-bf64-1c73ad353f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenar los dos DataFrames\n",
    "df_concat_0 = pd.concat([BDF_cut, Prepa_N]) # este es un temporal para comprobar si hay filas duplicadas \n",
    "# Crear una copia del DataFrame concatenado\n",
    "df_copia = df_concat_0.copy()\n",
    "# Eliminar las filas duplicadas basándose en las columnas de identificación\n",
    "df_final_0 = df_concat_0.drop_duplicates(subset=['CONCA', 'DOCUMENTO_REFERENCIA'], keep='last')\n",
    "# Obtener las filas duplicadas\n",
    "df_duplicados = df_copia[df_copia.duplicated(subset=['CONCA', 'DOCUMENTO_REFERENCIA'], keep=False)] ## Estas son las duplicas, \n",
    "#si este no es vacio entonces mi variable True Para duplicados\n",
    "row_e = not(df_duplicados.empty)\n",
    "df_duplicados.PEND_FACT_SOLES.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f8cf6d2d-592b-42cd-8f49-f03abb323d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las filas no existen en el reporte, concateno\n"
     ]
    }
   ],
   "source": [
    "if (row_e == True) and (report_old==True):\n",
    "    print(\"Las filas existen en el Prepa_N, solo cambio datos\")\n",
    "    Prepa_N = change_rows(df_concat_0,df_duplicados,Prepa_N)\n",
    "elif (row_e == True) and (report_old==False):\n",
    "    Prepa_N = drop_duplicatesR(df_duplicados,BDF_cut,Prepa_N)\n",
    "else: \n",
    "    print(\"Las filas no existen en el reporte, concateno\")\n",
    "    Prepa_N = pd.concat([Prepa_N, BDF_cut]) \n",
    "    Prepa_N.reset_index(drop=True,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4165886c-3dc4-41a7-a00e-540e16caf639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MES - COMPROMISO\n",
       "Octubre          330\n",
       "Agosto           312\n",
       "Setiembre        305\n",
       "Noviembre        268\n",
       "Bloqueo de AF    112\n",
       "Diciembre         34\n",
       "Mayo               8\n",
       "Nuevas EA          5\n",
       "Julio              3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Prepa_N['MES - COMPROMISO'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312ce782-ef66-4cef-a150-c6be23f734b2",
   "metadata": {},
   "source": [
    "# Prepocesing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11175001-27e7-4797-a6b4-0d03b8289aa3",
   "metadata": {},
   "source": [
    "## Preprosecing of OTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8e976bf1-e884-4b16-aca8-ab2a2728bb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Proyectos = ['ROLLOUT - 2023','ROLLOUT - 2022','ROLLOUT - 2024','CANON - 2023',]\n",
    "OT_cut_mant = OT_cut[OT_cut['Tipo Req'] == 'MANT. MEJORA DE RED'].copy()\n",
    "OT_cut_M = OT_cut[OT_cut.Proyecto == 'EXPANSIÓN'].copy()\n",
    "OT_cut_RRL = OT_cut[OT_cut.Etiqueta.isin(Proyectos)].copy()\n",
    "\n",
    "OT_concat = pd.concat([OT_cut_M,OT_cut_RRL,OT_cut_mant],axis=0)\n",
    "OT_concat.rename(columns={'Codigo de Site':'ID_SITIO',\n",
    "                           'Contrata':'NOMPROVEEDOR',\n",
    "                           'Nombre de Site':'SITE'},inplace=True)\n",
    "OT_concat = OT_concat[['OT','ID_SITIO','SITE','Proyecto','NOMPROVEEDOR','Etiqueta','Status OT']].copy()\n",
    "\n",
    "OT_agg_ID_PRO = compact_rows(OT_concat,['SITE','NOMPROVEEDOR'],'/')\n",
    "#OT_agg_ID_NM_PRO = compact_rows(OT_cut_RRL_2M,['ID_SITIO','SITE','PROVEEDOR'],'/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ded2a71-4f35-4f85-bf51-c35c160f516d",
   "metadata": {},
   "source": [
    "## Prepro of PAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e2db256c-7158-4908-8409-e9105ef99dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAP_f = PAP[['OC Posición','N° Sol','Estado','Acción','Nombre responsable','Id.SIte',\n",
    "             'SIte','# Días','F.Creación','F.Modifica']]\n",
    "##Filtro solo del norte\n",
    "PAP_f = PAP_f.dropna(subset=['Id.SIte'])\n",
    "#PAP_f = PAP_f[PAP_f['# Días'] < 500]\n",
    "\n",
    "PAP_f_N = PAP_f[PAP_f['Id.SIte'].str.startswith(('L','T','SAD','CL','CAC'))].copy()\n",
    "PAP_f_N = PAP_f_N.rename(columns={'N° Sol': 'PAP', #Rename\n",
    "                               'SIte': 'SITE',\n",
    "                               'Estado': 'ESTADO_PAP',\n",
    "                                '# Días': 'ANTIGUAMIENTO_PAP',\n",
    "                                'Id.SIte': 'ID Site',\n",
    "                             'Nombre responsable': 'RESPONSABLE_PAP' })\n",
    "PAP_S = Ac.split_ocs(PAP_f_N) # Spliteo OCs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f943646a-5d7f-4193-b012-e48c8d088594",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Consuto si se quiere ejecutar la actualizaccion total o solo parcial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4d5bc944-4380-4bcd-8d23-971bcdf74adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PREPA_UPDATE29.08.xlsx', 'PREPA_UPDATE27.08.xlsx']\n",
      "Bi actualizado\n"
     ]
    }
   ],
   "source": [
    "a =  Act_PBI(r'\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\Resultados_prepa',3,column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "def26ef0-ad09-4b3d-a9d0-d50f8855e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def goto_section_2():\n",
    "    print(\"Saltando a la sección PBI\")\n",
    "    Act_PBI(r'\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\Resultados_prepa',3,column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c4ee1fd2-9ad9-4f08-a400-93e20ea40f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "¿Quieres Actualizar de forma Completa, o Parcial?(C/P) C\n"
     ]
    }
   ],
   "source": [
    "# Preguntar al usuario si quiere saltar a una sección específic\n",
    "user_input = input(\"¿Quieres Actualizar de forma Completa, o Parcial?(C/P)\").strip().lower()   \n",
    "if user_input == 'p':\n",
    "        goto_section_2()\n",
    "        sys.exit()\n",
    "else:pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6379922-1120-468d-90db-63ce76f7d423",
   "metadata": {},
   "source": [
    "## Preprosecing of PREP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a7a470ac-2dea-4e2b-9ca0-5571f57b140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################### PASO 01#####################################################################\n",
    "Prepa_EI_NC = pre_proces(Prepa_N,['DOC_COMPRAS', 'POSIC','CONCA'],'RESPONSABLE2','int64')\n",
    "#####################################################################################################\n",
    "###############################################################################################3\n",
    "Prepa_EI = pre_proces(Prepa_EI_NC,['DOC_COMPRAS', 'POSIC','CONCA'],'RESPONSABLE2','str')\n",
    "                    \n",
    "Prepa_EI.loc[:, \"OC Posición\"] = Prepa_EI.loc[:, \"DOC_COMPRAS\"].str.cat(Prepa_EI.loc[:, \"POSIC\"], sep= \":\")\n",
    "Monto_In = Prepa_EI[\"PEND_FACT_SOLES\"].sum() #Mont of USD \n",
    "#Prepa_EI = Prepa_EI.drop(columns=['FECHAS COMPROMISO','MES - COMPROMISO'])\n",
    "#Prepa_EI.DOCUMENTO_REFERENCIA = Prepa_EI.DOCUMENTO_REFERENCIA.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "64d4cdda-b3e2-49f0-ae71-4087cf9ade04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Prepa_EI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "05eb9571-e555-4395-b425-1e2390307972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Preprosecing of PREP_OLD\n",
    "PrePa_O_EI = pre_proces(PrePa_O,['DOC_COMPRAS', 'POSIC','CONCA'],'RESPONSABLE2','str')\n",
    "PrePa_O_EI.replace('0x2a', np.nan, inplace=True)\n",
    "PrePa_O_EI['PAP'] = PrePa_O_EI['PAP'].replace('sin pap', np.nan)\n",
    "PrePa_O_EI = PrePa_O_EI.rename(columns={'FECHA': 'F.Creación'}) #Rename\n",
    "#Filter of PEA_old\n",
    "PrePa_O_EI = PrePa_O_EI[['DOCUMENTO_REFERENCIA','PAP','CONCA', 'SITE', 'COMENTARIO','ESTADO', 'RESPONSABLE3','CONTRA.FEC','CONTRAT.COM','ANTIGUAMIENTO_PAP']] #Columns i need 4 heredar\n",
    "PrePa_O_EI.CONCA = PrePa_O_EI.CONCA.astype('int64')\n",
    "Prepa_EI.CONCA = Prepa_EI.CONCA.astype('int64')\n",
    "Prepa_EI = Prepa_EI.drop_duplicates(subset=['DOCUMENTO_REFERENCIA','CONCA'],keep='first')\n",
    "len(PrePa_O_EI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3051e089-7725-4aa9-ad2d-ddb3c53046d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 249 entries, 6 to 1376\n",
      "Data columns (total 10 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   DOCUMENTO_REFERENCIA  249 non-null    int64  \n",
      " 1   PAP                   249 non-null    float64\n",
      " 2   CONCA                 249 non-null    int64  \n",
      " 3   SITE                  249 non-null    object \n",
      " 4   COMENTARIO            249 non-null    object \n",
      " 5   ESTADO                249 non-null    object \n",
      " 6   RESPONSABLE3          232 non-null    object \n",
      " 7   CONTRA.FEC            18 non-null     object \n",
      " 8   CONTRAT.COM           47 non-null     object \n",
      " 9   ANTIGUAMIENTO_PAP     249 non-null    float64\n",
      "dtypes: float64(2), int64(2), object(6)\n",
      "memory usage: 21.4+ KB\n"
     ]
    }
   ],
   "source": [
    "PrePa_O_EI.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69694f1d-9486-42cf-9f83-97a21970afd8",
   "metadata": {},
   "source": [
    "# Paso 02 (heredo info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "20f4332d-528a-40e0-bc56-d4d4ad8d46fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filas del merge:  249\n",
      "filas index:  249\n"
     ]
    }
   ],
   "source": [
    "## ########################### # Merge new con el old (PASO 02) ####################################################################################\n",
    "#####################################################################################################################3\n",
    "PrePa_act = merge_dataframes(Prepa_EI,PrePa_O_EI,['CONCA','DOCUMENTO_REFERENCIA'], index=Prepa_EI.index)### Info heredada del old REDY \n",
    "\n",
    "#Mantengo el index para luego\n",
    "###### Creo columnas para el que actualice el PAP####################### ///\n",
    "PrePa_act[['F.Creación','Acción','RESPONSABLE_PAP','F.Modifica']] = pd.NA\n",
    "PrePa_act.rename(columns={'ESTADO' : 'ESTADO_PAP'},inplace=True)\n",
    "PrePa_act['RESPONSABLE_PAP'] =PrePa_act['RESPONSABLE_PAP'].astype(str)\n",
    "try:\n",
    "    PrePa_act = combine_and_rename(PrePa_act,[('ANTIGUAMIENTO_PAP_x', 'ANTIGUAMIENTO_PAP_y')],{'ANTIGUAMIENTO_PAP_x':'ANTIGUAMIENTO_PAP'})\n",
    "except: pass\n",
    "Cash_in = PrePa_act.PEND_FACT_SOLES.sum() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4ac02d11-c0af-4d8c-90c1-1172218b586b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESTADO_PAP\n",
       "Finalizado              80\n",
       "Pre registro            66\n",
       "CHOCOS                  37\n",
       "Observado               30\n",
       "Solic FAC               13\n",
       "Registrar Pendientes    10\n",
       "Edwin                    5\n",
       "JULIO A.                 2\n",
       "MIGUEL R.                2\n",
       "Marco                    1\n",
       "Wilbert                  1\n",
       "ORLANDO Z.               1\n",
       "PAC                      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PrePa_act.ESTADO_PAP.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ae7e231f-5147-4061-83d7-c815382f12cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 249 entries, 6 to 1376\n",
      "Data columns (total 54 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   DOC_COMPRAS           249 non-null    object        \n",
      " 1   POSIC                 249 non-null    object        \n",
      " 2   CONCA                 249 non-null    int64         \n",
      " 3   DOCUMENTO_REFERENCIA  249 non-null    float64       \n",
      " 4   FONDO                 137 non-null    object        \n",
      " 5   TIPO_MAT              137 non-null    object        \n",
      " 6   TEXTO_BREVE           249 non-null    object        \n",
      " 7   CODPROVEEDOR          137 non-null    float64       \n",
      " 8   NOMPROVEEDOR          249 non-null    object        \n",
      " 9   FECH_CONTAB           137 non-null    datetime64[ns]\n",
      " 10  MON                   137 non-null    object        \n",
      " 11  INDICADOR             137 non-null    object        \n",
      " 12  EA_EM                 137 non-null    float64       \n",
      " 13  FACTURADO             0 non-null      float64       \n",
      " 14  PEND_FACT             137 non-null    float64       \n",
      " 15  PEND_FACT_SOLES       249 non-null    float64       \n",
      " 16  ESTATUS_ACEPT         137 non-null    object        \n",
      " 17  FECH_LIB_AF           0 non-null      float64       \n",
      " 18  DEMORA                137 non-null    float64       \n",
      " 19  ANTIGUAMIENTO         137 non-null    object        \n",
      " 20  RESPONSABLE           137 non-null    object        \n",
      " 21  RESPONSABLE2          249 non-null    object        \n",
      " 22  CE_GESTOR             137 non-null    object        \n",
      " 23  SUB_DIRECCION         137 non-null    object        \n",
      " 24  FECHA_REPORTE         137 non-null    datetime64[ns]\n",
      " 25  SOLICITUD             0 non-null      float64       \n",
      " 26  ESTATUS_SOLICITUD     0 non-null      float64       \n",
      " 27  AREA_RESPONSABLE      0 non-null      float64       \n",
      " 28  RESPONSABLE_PAP       249 non-null    object        \n",
      " 29  ACCION                0 non-null      float64       \n",
      " 30  ANTIGUAMIENTO_PAP     249 non-null    float64       \n",
      " 31  CENFILE               0 non-null      float64       \n",
      " 32  ESTADO_CENFILE        0 non-null      float64       \n",
      " 33  CODIGO_SITE           0 non-null      object        \n",
      " 34  NOMBRE_SITE           0 non-null      object        \n",
      " 35  ELEMENTO_PEP          137 non-null    object        \n",
      " 36  FECHAS COMPROMISO     249 non-null    object        \n",
      " 37  MES - COMPROMISO      249 non-null    object        \n",
      " 38  FECHA - PROY          137 non-null    datetime64[ns]\n",
      " 39  DÍAS -2               137 non-null    float64       \n",
      " 40  ANT-PROYECTADO        137 non-null    object        \n",
      " 41  STATUS FINAL          249 non-null    object        \n",
      " 42  INDICADOR2            137 non-null    object        \n",
      " 43  OC Posición           249 non-null    object        \n",
      " 44  PAP                   249 non-null    float64       \n",
      " 45  SITE                  249 non-null    object        \n",
      " 46  COMENTARIO            249 non-null    object        \n",
      " 47  ESTADO_PAP            249 non-null    object        \n",
      " 48  RESPONSABLE3          232 non-null    object        \n",
      " 49  CONTRA.FEC            18 non-null     object        \n",
      " 50  CONTRAT.COM           47 non-null     object        \n",
      " 51  F.Creación            0 non-null      object        \n",
      " 52  Acción                0 non-null      object        \n",
      " 53  F.Modifica            0 non-null      object        \n",
      "dtypes: datetime64[ns](3), float64(17), int64(1), object(33)\n",
      "memory usage: 115.1+ KB\n"
     ]
    }
   ],
   "source": [
    "PrePa_act.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e33fb21-5934-4bc0-afd7-62d5f0937b6c",
   "metadata": {},
   "source": [
    "# Paso 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c0fef4d7-9074-435c-a994-ad10027e4aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Filas_limpias = limpio_duplicados(filas_in,filas_out)######\n",
    "PRE_all_act =  Ac.update_values_optimized_V2(PrePa_act, PAP_S, \"OC Posición\",Filas_limpias) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "56f29883-65fe-462c-8f8d-6d750a270fda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 249 entries, 6 to 1376\n",
      "Data columns (total 54 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   DOC_COMPRAS           249 non-null    object        \n",
      " 1   POSIC                 249 non-null    object        \n",
      " 2   CONCA                 249 non-null    int64         \n",
      " 3   DOCUMENTO_REFERENCIA  249 non-null    float64       \n",
      " 4   FONDO                 137 non-null    object        \n",
      " 5   TIPO_MAT              137 non-null    object        \n",
      " 6   TEXTO_BREVE           249 non-null    object        \n",
      " 7   CODPROVEEDOR          137 non-null    float64       \n",
      " 8   NOMPROVEEDOR          249 non-null    object        \n",
      " 9   FECH_CONTAB           137 non-null    datetime64[ns]\n",
      " 10  MON                   137 non-null    object        \n",
      " 11  INDICADOR             137 non-null    object        \n",
      " 12  EA_EM                 137 non-null    float64       \n",
      " 13  FACTURADO             0 non-null      float64       \n",
      " 14  PEND_FACT             137 non-null    float64       \n",
      " 15  PEND_FACT_SOLES       249 non-null    float64       \n",
      " 16  ESTATUS_ACEPT         137 non-null    object        \n",
      " 17  FECH_LIB_AF           0 non-null      float64       \n",
      " 18  DEMORA                137 non-null    float64       \n",
      " 19  ANTIGUAMIENTO         137 non-null    object        \n",
      " 20  RESPONSABLE           137 non-null    object        \n",
      " 21  RESPONSABLE2          249 non-null    object        \n",
      " 22  CE_GESTOR             137 non-null    object        \n",
      " 23  SUB_DIRECCION         137 non-null    object        \n",
      " 24  FECHA_REPORTE         137 non-null    datetime64[ns]\n",
      " 25  SOLICITUD             0 non-null      float64       \n",
      " 26  ESTATUS_SOLICITUD     0 non-null      float64       \n",
      " 27  AREA_RESPONSABLE      0 non-null      float64       \n",
      " 28  RESPONSABLE_PAP       249 non-null    object        \n",
      " 29  ACCION                0 non-null      float64       \n",
      " 30  ANTIGUAMIENTO_PAP     249 non-null    float64       \n",
      " 31  CENFILE               0 non-null      float64       \n",
      " 32  ESTADO_CENFILE        0 non-null      float64       \n",
      " 33  CODIGO_SITE           0 non-null      object        \n",
      " 34  NOMBRE_SITE           0 non-null      object        \n",
      " 35  ELEMENTO_PEP          137 non-null    object        \n",
      " 36  FECHAS COMPROMISO     249 non-null    object        \n",
      " 37  MES - COMPROMISO      249 non-null    object        \n",
      " 38  FECHA - PROY          137 non-null    datetime64[ns]\n",
      " 39  DÍAS -2               137 non-null    float64       \n",
      " 40  ANT-PROYECTADO        137 non-null    object        \n",
      " 41  STATUS FINAL          249 non-null    object        \n",
      " 42  INDICADOR2            137 non-null    object        \n",
      " 43  OC Posición           249 non-null    object        \n",
      " 44  PAP                   249 non-null    float64       \n",
      " 45  SITE                  249 non-null    object        \n",
      " 46  COMENTARIO            249 non-null    object        \n",
      " 47  ESTADO_PAP            249 non-null    object        \n",
      " 48  RESPONSABLE3          232 non-null    object        \n",
      " 49  CONTRA.FEC            18 non-null     object        \n",
      " 50  CONTRAT.COM           47 non-null     object        \n",
      " 51  F.Creación            188 non-null    object        \n",
      " 52  Acción                188 non-null    object        \n",
      " 53  F.Modifica            188 non-null    object        \n",
      "dtypes: datetime64[ns](3), float64(17), int64(1), object(33)\n",
      "memory usage: 107.0+ KB\n"
     ]
    }
   ],
   "source": [
    "PRE_all_act.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "552ccc20-2125-4b98-a318-77ff4fb16158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2955110.74"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ESTADO_PAP\n",
       "Finalizado          80\n",
       "Pre registro        66\n",
       "Aprobaciones FAC    47\n",
       "Observado           31\n",
       "FAC                 13\n",
       "Visita ejecutada    10\n",
       "Aprobaciones PAC     1\n",
       "PAC                  1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#PRE_all_act.drop(columns = 'OC Posición',inplace=True)\n",
    "display(PRE_all_act.PEND_FACT_SOLES.sum())\n",
    "\n",
    "\n",
    "duplicated_index = PRE_all_act.index.duplicated(keep=False)\n",
    "df_duplicated = PRE_all_act[duplicated_index].copy()\n",
    "PRE_all_act.drop(PRE_all_act[duplicated_index].index, inplace=True) # Borro duplicados del principal para tratarlos en el otro datafra \n",
    "\n",
    "# Ordena el DataFrame por la columna de fechas  LA FECHA DE CREACION ME SIRVE PARA ELMINAR DUPLICADOS\n",
    "\n",
    "df_duplicated['F.Modifica'] = pd.to_datetime(df_duplicated['F.Modifica'], format=\"%d/%m/%Y %I:%M:%S %p\") # Paso a fecha para ordenar\n",
    "df_duplicated.sort_values('F.Modifica',ascending=False, inplace=True)\n",
    "df_duplicated_NR = df_duplicated[df_duplicated.ESTADO_PAP != 'Rechazado'].sort_index(ascending=False)\n",
    "\n",
    "df_duplicated_NR = df_duplicated_NR.loc[~df_duplicated_NR.index.duplicated(keep='first')] # niego la condicional que me selecciona cada indice duplicado excepto el primero\n",
    "#df_duplicated['F.Creación'] = df_duplicated['F.Creación'].dt.strftime(\"%m/%d/%Y %I:%M:%S %p\") # Regreso a str \n",
    "PRE_all_act = pd.concat([PRE_all_act, df_duplicated_NR])\n",
    "PRE_all_act = cleanrows(PRE_all_act)\n",
    "\n",
    "#PRE_all_act = PRE_all_act.drop(columns=['F.Modifica'])\n",
    "Cash_out = PRE_all_act.PEND_FACT_SOLES.sum()\n",
    "print(Cash_in - Cash_out)\n",
    "#PRE_all_act['ESTADO_PAP'] = PRE_all_act['ESTADO_PAP'].fillna('Finalizado')\n",
    "PRE_all_act.ESTADO_PAP.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "605d02a5-3acf-4bcd-9a41-250d6eb81319",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_all_act['RESPONSABLE_PAP'] = PRE_all_act['RESPONSABLE_PAP'].replace('<NA>','SIN RESPONSABLE')\n",
    "# Aplicar la transformación\n",
    "PRE_all_act['RESPONSABLE_PAP'] = PRE_all_act['RESPONSABLE_PAP'].apply(transformar_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9f467f0a-9c81-42ea-938d-f5202408c18d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 249 entries, 6 to 1376\n",
      "Data columns (total 54 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   DOC_COMPRAS           249 non-null    object        \n",
      " 1   POSIC                 249 non-null    object        \n",
      " 2   CONCA                 249 non-null    int64         \n",
      " 3   DOCUMENTO_REFERENCIA  249 non-null    float64       \n",
      " 4   FONDO                 137 non-null    object        \n",
      " 5   TIPO_MAT              137 non-null    object        \n",
      " 6   TEXTO_BREVE           249 non-null    object        \n",
      " 7   CODPROVEEDOR          137 non-null    float64       \n",
      " 8   NOMPROVEEDOR          249 non-null    object        \n",
      " 9   FECH_CONTAB           137 non-null    datetime64[ns]\n",
      " 10  MON                   137 non-null    object        \n",
      " 11  INDICADOR             137 non-null    object        \n",
      " 12  EA_EM                 137 non-null    float64       \n",
      " 13  FACTURADO             0 non-null      float64       \n",
      " 14  PEND_FACT             137 non-null    float64       \n",
      " 15  PEND_FACT_SOLES       249 non-null    float64       \n",
      " 16  ESTATUS_ACEPT         137 non-null    object        \n",
      " 17  FECH_LIB_AF           0 non-null      float64       \n",
      " 18  DEMORA                137 non-null    float64       \n",
      " 19  ANTIGUAMIENTO         137 non-null    object        \n",
      " 20  RESPONSABLE           137 non-null    object        \n",
      " 21  RESPONSABLE2          249 non-null    object        \n",
      " 22  CE_GESTOR             137 non-null    object        \n",
      " 23  SUB_DIRECCION         137 non-null    object        \n",
      " 24  FECHA_REPORTE         137 non-null    datetime64[ns]\n",
      " 25  SOLICITUD             0 non-null      float64       \n",
      " 26  ESTATUS_SOLICITUD     0 non-null      float64       \n",
      " 27  AREA_RESPONSABLE      0 non-null      float64       \n",
      " 28  RESPONSABLE_PAP       249 non-null    object        \n",
      " 29  ACCION                0 non-null      float64       \n",
      " 30  ANTIGUAMIENTO_PAP     249 non-null    float64       \n",
      " 31  CENFILE               0 non-null      float64       \n",
      " 32  ESTADO_CENFILE        0 non-null      float64       \n",
      " 33  CODIGO_SITE           0 non-null      object        \n",
      " 34  NOMBRE_SITE           0 non-null      object        \n",
      " 35  ELEMENTO_PEP          137 non-null    object        \n",
      " 36  FECHAS COMPROMISO     249 non-null    object        \n",
      " 37  MES - COMPROMISO      249 non-null    object        \n",
      " 38  FECHA - PROY          137 non-null    datetime64[ns]\n",
      " 39  DÍAS -2               137 non-null    float64       \n",
      " 40  ANT-PROYECTADO        137 non-null    object        \n",
      " 41  STATUS FINAL          249 non-null    object        \n",
      " 42  INDICADOR2            137 non-null    object        \n",
      " 43  OC Posición           249 non-null    object        \n",
      " 44  PAP                   249 non-null    float64       \n",
      " 45  SITE                  249 non-null    object        \n",
      " 46  COMENTARIO            249 non-null    object        \n",
      " 47  ESTADO_PAP            249 non-null    object        \n",
      " 48  RESPONSABLE3          232 non-null    object        \n",
      " 49  CONTRA.FEC            18 non-null     object        \n",
      " 50  CONTRAT.COM           47 non-null     object        \n",
      " 51  F.Creación            188 non-null    object        \n",
      " 52  Acción                188 non-null    object        \n",
      " 53  F.Modifica            188 non-null    object        \n",
      "dtypes: datetime64[ns](3), float64(17), int64(1), object(33)\n",
      "memory usage: 107.0+ KB\n"
     ]
    }
   ],
   "source": [
    "PRE_all_act.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f17970c-eecc-4c1d-95c6-109b8403614a",
   "metadata": {},
   "source": [
    "### Mapeo Compuesto (Tratamiento de Estados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "14c962a7-15cf-4a2a-b723-449c5621be61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ESTADO</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Finalizado</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pre registro</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Observado</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHOCOS</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Solic FAC</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ORLANDO Z.</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Registrar Pendientes</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MIGUEL R.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>JULIO A.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Eduardo I.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PAC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ESTADO  count\n",
       "0             Finalizado     80\n",
       "1           Pre registro     66\n",
       "2              Observado     31\n",
       "3                 CHOCOS     25\n",
       "4              Solic FAC     13\n",
       "5             ORLANDO Z.     13\n",
       "6   Registrar Pendientes     10\n",
       "7              MIGUEL R.      6\n",
       "8               JULIO A.      3\n",
       "9             Eduardo I.      1\n",
       "10                   PAC      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definimos los valores que estamos buscando en una lista\n",
    "valores_buscados = ['Registrado', 'Aprobaciones FAC', 'Aprobaciones PAC','Lista Pendientes']\n",
    "\n",
    "# Creamos la nueva columna basada en la condición\n",
    "PRE_all_act['ESTADO'] = np.where(PRE_all_act['ESTADO_PAP'].isin(valores_buscados), \n",
    "                                 PRE_all_act['RESPONSABLE_PAP'], \n",
    "                                 PRE_all_act['ESTADO_PAP'])\n",
    "map_estado = {'Auto ATP' : 'Registrar Pendientes',\n",
    "       'Visita ejecutada' : 'Registrar Pendientes',\n",
    "        'Programado' : 'WILBERT ATP',\n",
    "        'FAC' : 'Solic FAC',\n",
    "        'Gustavo Chocos':'CHOCOS',\n",
    "        'Edwin Hurtado':'Edwin',\n",
    "        'Marco Mejia':'Marco',\n",
    "        'Wilbert Pintado':'Wilbert',\n",
    "        'Orlando Zapata':'ORLANDO Z.',\n",
    "        'Miguel Rojas':'MIGUEL R.',\n",
    "        'Julio Arciniega':'JULIO A.',\n",
    "        'Juan Trigo':'JUAN JOSE',\n",
    "        'Eduardo Iberico':'Eduardo I.',\n",
    "            }\n",
    "Estados_finales = [ \"Miguel Rojas\",\n",
    "                    \"Finalizado\",\n",
    "                    \"Orlando Z.\",\n",
    "                    \"Chocos\",\n",
    "                    \"Miguel R.\",\n",
    "                    \"Julio A.\",\n",
    "                    \"Marco\",\n",
    "                    \"Edwin\"]\n",
    "\n",
    "PRE_all_act['ESTADO'] = PRE_all_act['ESTADO'].replace(map_estado)\n",
    "\n",
    "PRE_all_act['RESPONSABLE3'] = PRE_all_act['RESPONSABLE3'].fillna(PRE_all_act['CONCA'].map(dicc_respo))\n",
    "\n",
    "#Modifico el valor par adetemrina condicion\n",
    "PRE_all_act.loc[PRE_all_act['ESTADO'] == 'SIN RESPONSABLE', 'ESTADO'] = 'Por Asignar'\n",
    "PRE_all_act.loc[PRE_all_act['ESTADO'].isin(Estados_finales), 'COMENTARIO'] = ' '\n",
    "\n",
    "a = PRE_all_act.ESTADO.value_counts(dropna=False).reset_index()\n",
    "a.to_csv(f'D:/Prepa_R/Recuento_estados-{Today_D_M}.csv',index_label=False)\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "beb94b02-05c6-4916-a932-f0fa1899a664",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PRE_all_act = PRE_all_act.drop(columns=['F.Creación','RESPONSABLE_PAP'\n",
    "                                        ,'ANTIGUAMIENTO','DEMORA','FECH_LIB_AF','INDICADOR','MON'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "677b60ac-c5eb-4dff-a983-639ebe8361de",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_all_act['MES - COMPROMISO'].replace('Setiembre','Septiembre',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c1f681d3-e889-4e61-a2ee-5816329d3ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOC_COMPRAS</th>\n",
       "      <th>POSIC</th>\n",
       "      <th>CONCA</th>\n",
       "      <th>DOCUMENTO_REFERENCIA</th>\n",
       "      <th>FONDO</th>\n",
       "      <th>TIPO_MAT</th>\n",
       "      <th>TEXTO_BREVE</th>\n",
       "      <th>CODPROVEEDOR</th>\n",
       "      <th>NOMPROVEEDOR</th>\n",
       "      <th>FECH_CONTAB</th>\n",
       "      <th>...</th>\n",
       "      <th>PAP</th>\n",
       "      <th>SITE</th>\n",
       "      <th>COMENTARIO</th>\n",
       "      <th>ESTADO_PAP</th>\n",
       "      <th>RESPONSABLE3</th>\n",
       "      <th>CONTRA.FEC</th>\n",
       "      <th>CONTRAT.COM</th>\n",
       "      <th>Acción</th>\n",
       "      <th>F.Modifica</th>\n",
       "      <th>ESTADO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [DOC_COMPRAS, POSIC, CONCA, DOCUMENTO_REFERENCIA, FONDO, TIPO_MAT, TEXTO_BREVE, CODPROVEEDOR, NOMPROVEEDOR, FECH_CONTAB, EA_EM, FACTURADO, PEND_FACT, PEND_FACT_SOLES, ESTATUS_ACEPT, RESPONSABLE, RESPONSABLE2, CE_GESTOR, SUB_DIRECCION, FECHA_REPORTE, SOLICITUD, ESTATUS_SOLICITUD, AREA_RESPONSABLE, ACCION, ANTIGUAMIENTO_PAP, CENFILE, ESTADO_CENFILE, CODIGO_SITE, NOMBRE_SITE, ELEMENTO_PEP, FECHAS COMPROMISO, MES - COMPROMISO, FECHA - PROY, DÍAS -2, ANT-PROYECTADO, STATUS FINAL, INDICADOR2, OC Posición, PAP, SITE, COMENTARIO, ESTADO_PAP, RESPONSABLE3, CONTRA.FEC, CONTRAT.COM, Acción, F.Modifica, ESTADO]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 48 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(PRE_all_act[PRE_all_act.ESTADO.isna()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa88fd2e-5e58-457c-a643-9da096a70d11",
   "metadata": {},
   "source": [
    "# Enbullo la info actualizada del norte en el global\n",
    "    04. Para ello necesito primero darle formato al global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "696e1b59-59bb-4422-8045-dffb73450018",
   "metadata": {},
   "outputs": [],
   "source": [
    "Resultado = PRE_all_act\n",
    "columnas_para_actualizar = ['ANTIGUAMIENTO_PAP', 'PAP','SITE','COMENTARIO','RESPONSABLE3','CONTRA.FEC','CONTRAT.COM','ESTADO_PAP','ESTADO']\n",
    "Resultado_reducido = Resultado[columnas_para_actualizar]\n",
    "#############################################################\n",
    "Prepa_N = Prepa_N.dropna(axis=1, how='all')\n",
    "unhavent_columns = set(Resultado.columns.tolist()) - set(Prepa_N.columns.tolist()) # Columnas que le faltan al grande del chico\n",
    "# Crear las columnas necesarias con dtype 'object'\n",
    "for col in list(unhavent_columns):\n",
    "    Prepa_N[col] = pd.Series(dtype='object')\n",
    "## Df pequeño en el Df grande \n",
    "Resultado.columns = Resultado.columns.str.replace(' ', '')\n",
    "Prepa_N.columns = Prepa_N.columns.str.replace(' ', '')\n",
    "Prepa_N.update(Resultado_reducido, overwrite=True) ### Aqui es dond eembullo la data.\n",
    "Prepa_N.columns = Prepa_N.columns.astype(str)\n",
    "Prepa_N_1 = Ac.convert_columns_to_str(Prepa_N.copy(), ['COMENTARIO'])\n",
    "Prepa_N_1 = Prepa_N_1.drop(columns=['ESTADO_PAP','Acción','SOLICITUD','ESTATUS_SOLICITUD','AREA_RESPONSABLE','ACCION','TIPO_MAT','CE_GESTOR'])\n",
    "Prepa_N_1 = Prepa_N_1.replace({'nan':''})\n",
    "cols = list(Prepa_N_1.columns)\n",
    "Prepa_N_1 = Prepa_N_1[cols[:-9] + ['ANTIGUAMIENTO_PAP','PAP', 'SITE', 'ESTADO','COMENTARIO','RESPONSABLE3','CONTRA.FEC','CONTRAT.COM']]\n",
    "Prepa_N_1.COMENTARIO = Prepa_N_1.COMENTARIO.replace({'1899-12-30 00:00:00' : ''})\n",
    "Prepa_N_1 = Prepa_N_1.dropna(axis=1, how='all') # elimino columnas vacias\n",
    "Prepa_N_1 = Prepa_N_1.loc[:,~Prepa_N_1.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c5e1ef-b52b-43e2-8161-34f991bad91b",
   "metadata": {},
   "source": [
    "# 2 Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b900815a-6358-4d80-9bff-fce083b932a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4227a565-0a4f-4c3e-828f-bea5f06c8b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El código ya se ha ejecutado hoy.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "useless_columns= ['DOCUMENTO_REFERENCIA','FONDO','FECH_CONTAB','MON','INDICADOR',\n",
    "                      'PEND_FACT','EA_EM','ESTATUS_ACEPT','RESPONSABLE','DEMORA',\n",
    "                  'SUB_DIRECCION','FECHA_REPORTE','ELEMENTO_PEP','CENFILE','ESTADO_CENFILE','CODIGO_SITE','INDICADOR2',\n",
    "                      'ÁREA','AGRUPADOR','RESPON','NOMBRE SITE','RESPONSABLE_PAP']\n",
    "## Formato Tabla en el excel \n",
    "#Today_D_M=Today_D_M+'_2'\n",
    "Ac.Excel_format(Prepa_N_1,r'\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\Resultados_prepa\\PREPA_UPDATE{}.xlsx'.format(Today_D_M),useless_columns)\n",
    "#Ac.Excel_format(Prepa_N_1,r'D:\\Prepa_local\\PREPA_UPDATE{}.xlsx'.format(Today_D_M),useless_columns)\n",
    "\n",
    "filename = r'D:\\Scripts1\\Code\\ActPEA\\CODE\\last_run.json'\n",
    "# Carga la última fecha de ejecución\n",
    "last_run_date = Ac.load_last_run_date(filename)\n",
    "\n",
    "# Comprueba si la celda ya se ha ejecutado hoy\n",
    "if last_run_date != datetime.datetime.now().date():\n",
    "    # Tu código aquí\n",
    "    print('Ejecutado: ',Today_str)\n",
    "    process_to_bcsv(PRE_all_act,'D:/Prepa/TIME.S/Prepa_TS1.csv',Today_str)\n",
    "    process_to_bcsv(PRE_all_act,r'\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\PowerBI\\Prepa_TS.csv',Today_str)# añado al acumulado\n",
    "\n",
    "    # Guarda la fecha de hoy como la última fecha de ejecución\n",
    "    Ac.save_last_run_date(filename)\n",
    "else:\n",
    "    print(\"El código ya se ha ejecutado hoy.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defc9c79-6752-4a5c-9068-6ab1739038e7",
   "metadata": {},
   "source": [
    "# To BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7d71190f-af7f-46c5-88e9-576786f4a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Act_PBI(Path:str,range_diff,column_list):\n",
    "        \n",
    "    ## Revisar esto porque se ejecuto 2 veces el mismo dia pero en diferentes idles\n",
    "    dataframes = get_2_df_from_path(Path,'.xlsx',range_diff,'Sheet1')\n",
    "    #dataframes = get_2_df_from_path(r'D:\\Prepa_local','.xlsx',4,'Sheet1')\n",
    "    # Ruta de donde extraigo los archivos a comparar,intervalo,Nombre de la hoja\n",
    "    dataframes_filt = list(map(filtro,dataframes)) # filtro solo del norte\n",
    "    df_diff = calc_diff_V1(dataframes_filt[0],dataframes_filt[1]) # Funcion que me crea la columna de diferencia.\n",
    "    df_diff = cleanrows(df_diff)\n",
    "    #Divide la plata para el agg del BI \n",
    "    df_diff['DIFERENCIA EN SOLES'] = df_diff['DIFERENCIA EN SOLES'].transform(lambda x: x / df_diff['DIFERENCIA EN SOLES'].value_counts()[x] if pd.notnull(x) else x) \n",
    "    \n",
    "    df_diff = normalize_company_names(df_diff,'NOMPROVEEDOR') \n",
    "    \n",
    "    df_diff_m = pd.merge(df_diff,OT_agg_ID_PRO[['SITE','NOMPROVEEDOR','Status OT']],on=['SITE','NOMPROVEEDOR'],how='left')\n",
    "    \n",
    "    df_diff_m.loc[df_diff_m.CONCA.isin(BDF_dntdo),'Status OT'] = 'No Ejecutado'#BDF No ejecutados por lista\n",
    "    \n",
    "    try:\n",
    "        df_diff_m.PAP = df_diff_m.PAP.astype('int')\n",
    "        df_diff_m.drop(columns=['ANT-PROYECTADO'],inplace=True)\n",
    "    except:pass\n",
    "    df_diff_m.rename(columns={'Status OT':'ANTIGUAMIENTO PROYECTADO'},inplace = True)\n",
    "    ### esta pasando un df vacio , probablemente falta la columna o los valores\n",
    "    df_fomarted = formated2bi(df_diff_m,contrata_dic,columns_dic,column_list)\n",
    "    \n",
    "    if 'Estado Cenfile' in df_fomarted.columns:\n",
    "        df_fomarted['Estado Cenfile'] = '0'\n",
    "    else: pass\n",
    "    #return df_fomarted\n",
    "    df_fomarted.to_csv(r'\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\PowerBI\\Reporte Prepasivo.csv',index=False)\n",
    "    subprocess.call([\"python\",r\"D:\\Scripts1\\Code\\ActPEA\\CODE\\2BI_Norma.py\"])\n",
    "    print('Bi actualizado')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "1788a183-a3a2-4036-ab36-311989949bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PREPA_UPDATE29.08.xlsx', 'PREPA_UPDATE27.08.xlsx']\n",
      "Bi actualizado\n"
     ]
    }
   ],
   "source": [
    "Act_PBI(r'\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\Resultados_prepa',3,column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "061c356c-5707-42c5-9d30-e8f67ba00d66",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_fomarted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[180], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf_fomarted\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMes De Compromiso\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_fomarted' is not defined"
     ]
    }
   ],
   "source": [
    "print(df_fomarted['Mes De Compromiso'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a1b319e1-9ece-4553-a4f3-e27123970600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935c9ef2-0d7b-4792-bc29-bbc58abd91b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cbce18-b912-4b46-ac5e-9853faefa9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e56a32-dd64-4376-813b-11afc37eb524",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
