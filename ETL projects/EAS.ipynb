{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dccce539-a221-44c5-b9ab-57a64cd4f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"D:\\Scripts1\\Code\\ActPEA\\CODE\\Actu_Colums.py\")\n",
    "import pandas as pd\n",
    "import matplotlib as mptl\n",
    "import Actu_Colums as Ac\n",
    "from openpyxl import load_workbook\n",
    "from typing import Optional\n",
    "import glob,os,re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from babel.dates import format_date, format_datetime, Locale\n",
    "import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import locale\n",
    "locale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')\n",
    "import pickle\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0ad4135-18ab-4c1e-9124-c21f0aff1bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addSite(EA_act_df: pd.DataFrame, SAP_4_use: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Función que toma las tablas SAP y extrae los sites y los códigos de AHI.\n",
    "       Devuelve un DataFrame con la información añadida.\"\"\"\n",
    "    \n",
    "    # Merge y eliminación de duplicados\n",
    "    df_merged = pd.merge(EA_act_df, SAP_4_use, on='CONCATENADO', how='left')\n",
    "    df_merged = df_merged.drop_duplicates(subset='CONCATENADO')\n",
    "    \n",
    "    # Convertir 'PEP Desc' a string\n",
    "    df_merged['PEP Desc'] = df_merged['PEP Desc'].astype(str)\n",
    "    \n",
    "    # Precompilar la expresión regular\n",
    "    pattern = re.compile(r'(?<=\\d)(?=[a-zA-Z])')\n",
    "    \n",
    "    # Función para dividir 'PEP Desc'\n",
    "    def split_pep_desc(x):\n",
    "        if pattern.search(x):\n",
    "            return re.split(pattern, x)\n",
    "        else:\n",
    "            return [np.nan, x]\n",
    "    \n",
    "    # Vectorizar la función split_pep_desc\n",
    "    vectorized_split = np.vectorize(split_pep_desc, otypes=[object])\n",
    "    \n",
    "    # Aplicar la función vectorizada\n",
    "    split_results = vectorized_split(df_merged['PEP Desc'])\n",
    "    \n",
    "    # Convertir los resultados a un array de numpy y asignar a las columnas\n",
    "    df_merged['ID_SITE_SAP'] = [item[0] for item in split_results]\n",
    "    df_merged['SITE_SAP'] = [item[1] for item in split_results]\n",
    "    \n",
    "    return df_merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fce9f1-d986-4e01-9a0e-c6333be42a56",
   "metadata": {},
   "source": [
    "# La estrucutra es: \n",
    "- https://miro.com/welcomeonboard/MXJUUHNkTk5LWFFtamRMUDBUTnAxNVhDVjJyeTlzYnN5SzhzQXN6MkhwNzBsQXZoSzNBcHNsSlFMT1lIdlRrZXwzNDU4NzY0NTkxMzU2NDUxNTY0fDI=?share_link_id=885670918178\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43031fe5-17b8-41c0-990f-f6feb27276eb",
   "metadata": {},
   "source": [
    "# Funciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed9536cd-26e9-40b2-be0f-acc41f21760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Today_date = datetime.date.today()\n",
    "Today_str = datetime.date.today().strftime('%d-%m-%Y')\n",
    "filas_normal = Today_date\n",
    "Today_D_M = Today_str[0:5]\n",
    "Today_D_M = [Today_D_M[0:2],'.',Today_D_M[3:6]]\n",
    "Today_D_M = ''.join(Today_D_M)\n",
    "with open(r\"\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\PowerBI\\Dimensiones\\temp.pkl\", \"rb\") as f:\n",
    "    filas_count = pickle.load(f)\n",
    "def cortar_hora(fecha_hora_str):\n",
    "    return fecha_hora_str.split(\" \")[0]\n",
    "def addSite(EA_act_df : pd.DataFrame, SAP_4_use : pd.DataFrame):\n",
    "    \"\"\"Funcione que toma las tablas SAP y extrae los sites y los codigos de AHI\n",
    "        Devuelve un DF con la Informacion Añadida ahi\"\"\"\n",
    "    df_merged = pd.merge(EA_act_df,SAP_4_use,on='CONCATENADO',how='left')\n",
    "    df_merged = df_merged.drop_duplicates(subset='CONCATENADO')\n",
    "    df_merged['PEP Desc'] = df_merged['PEP Desc'].astype(str)\n",
    "    df_merged['ID_SITE_SAP'], df_merged['SITE_SAP'] = zip(*df_merged['PEP Desc'].apply(lambda x: re.split('(?<=\\d)(?=[a-zA-Z])', x) \n",
    "                                                                                           if re.search('(?<=\\d)(?=[a-zA-Z])', x) else [np.nan,x]))\n",
    "    \n",
    "    # Verifica si las columnas 'SITE' e 'ID_SITIO' existen, si no, las crea\n",
    "    if 'SITE' not in df_merged.columns:\n",
    "        df_merged['SITE'] = np.nan\n",
    "    if 'ID_SITIO' not in df_merged.columns:\n",
    "        df_merged['ID_SITIO'] = np.nan\n",
    "\n",
    "    # Reemplaza los valores NaN en 'SITE' e 'ID_SITIO' con los valores de 'SITE_SAP' e 'ID_SITE_SAP'\n",
    "    df_merged['SITE'] = df_merged['SITE'].combine_first(df_merged['SITE_SAP'])\n",
    "    df_merged['ID_SITIO'] = df_merged['ID_SITIO'].combine_first(df_merged['ID_SITE_SAP'])\n",
    "\n",
    "    df_merged.SITE = df_merged.SITE.str.strip() # Quito espacios\n",
    "\n",
    "    EA_act_df = df_merged.drop(columns=['ID_SITE_SAP','SITE_SAP','Fecha OC']) # Elimino las columnas que use para el Merge \n",
    "    return EA_act_df\n",
    "\n",
    "\n",
    "def addSite_V2(EA_act_df: pd.DataFrame, SAP_4_use: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Función que toma las tablas SAP y extrae los sites y los códigos de AHI.\n",
    "       Devuelve un DataFrame con la información añadida.\"\"\"\n",
    "    \n",
    "    # Merge y eliminación de duplicados\n",
    "    df_merged = pd.merge(EA_act_df, SAP_4_use, on='CONCATENADO', how='left')\n",
    "    df_merged = df_merged.drop_duplicates(subset='CONCATENADO')\n",
    "    \n",
    "    # Convertir 'PEP Desc' a string\n",
    "    df_merged['PEP Desc'] = df_merged['PEP Desc'].astype(str)\n",
    "    \n",
    "    # Precompilar la expresión regular\n",
    "    pattern = re.compile(r'(?<=\\d)(?=[a-zA-Z])')\n",
    "    \n",
    "    # Función para dividir 'PEP Desc'\n",
    "    def split_pep_desc(x):\n",
    "        if pattern.search(x):\n",
    "            return re.split(pattern, x)\n",
    "        else:\n",
    "            return [np.nan, x]\n",
    "    \n",
    "    # Vectorizar la función split_pep_desc\n",
    "    vectorized_split = np.vectorize(split_pep_desc, otypes=[object])\n",
    "    \n",
    "    # Aplicar la función vectorizada\n",
    "    split_results = vectorized_split(df_merged['PEP Desc'])\n",
    "    \n",
    "    # Convertir los resultados a un array de numpy y asignar a las columnas\n",
    "    df_merged['ID_SITE_SAP'] = [item[0] for item in split_results]\n",
    "    df_merged['SITE_SAP'] = [item[1] for item in split_results]\n",
    "\n",
    "    # Verifica si las columnas 'SITE' e 'ID_SITIO' existen, si no, las crea\n",
    "    if 'SITE' not in df_merged.columns:\n",
    "        df_merged['SITE'] = np.nan\n",
    "    if 'ID_SITIO' not in df_merged.columns:\n",
    "        df_merged['ID_SITIO'] = np.nan\n",
    "\n",
    "    # Reemplaza los valores NaN en 'SITE' e 'ID_SITIO' con los valores de 'SITE_SAP' e 'ID_SITE_SAP'\n",
    "    df_merged['SITE'] = df_merged['SITE'].combine_first(df_merged['SITE_SAP'])\n",
    "    df_merged['ID_SITIO'] = df_merged['ID_SITIO'].combine_first(df_merged['ID_SITE_SAP'])\n",
    "\n",
    "    df_merged.SITE = df_merged.SITE.str.strip() # Quito espacios\n",
    "\n",
    "    EA_act_df = df_merged.drop(columns=['ID_SITE_SAP','SITE_SAP','Fecha OC']) # Elimino las columnas que use para el Merge \n",
    "    return EA_act_df\n",
    "    \n",
    "\n",
    "    \n",
    "def get_recent_df_B(Carpeta_path: str, sheet_name: str):\n",
    "    \"\"\" Devuelve el df de la hoja especifica, del archivo mas reciente sin guion bajo creado de la carpeta especificada\"\"\"\n",
    "    Path_n= Carpeta_path + '/*'\n",
    "    tipo_de_archivo = '*.xlsx'\n",
    "    # Busca el archivo más reciente\n",
    "    archivos = glob.glob(Path_n + tipo_de_archivo)\n",
    "    # Filtra los archivos que no contienen \"_\" en su nombre\n",
    "    archivos_sin_guion_bajo = [archivo for archivo in archivos if \"_\" not in os.path.basename(archivo)]\n",
    "    archivo_mas_reciente = max(archivos_sin_guion_bajo, key=os.path.getctime)\n",
    "    nombre_del_archivo_N = os.path.basename(archivo_mas_reciente)\n",
    "    print(archivo_mas_reciente)\n",
    "    # Lee el archivo sin especificar el tipo de datos\n",
    "    df = pd.read_excel(archivo_mas_reciente , sheet_name=sheet_name)\n",
    "    # Si la columna \"COMENTARIO\" existe, cambia su tipo de datos a str\n",
    "    if 'COMENTARIO' in df.columns:\n",
    "        df['COMENTARIO'] = df['COMENTARIO'].astype(str)\n",
    "    return df,nombre_del_archivo_N\n",
    "def get_recent_df(Carpeta_path: str, sheet_name: str):\n",
    "    \"\"\" Devuelve el df de la hoja especifica, del archivo mas reciente modificado de la carpeta especificada\"\"\"\n",
    "    Path_n= Carpeta_path + '/*'\n",
    "    tipo_de_archivo = '*.xlsx'\n",
    "    # Busca el archivo más reciente\n",
    "    archivos = glob.glob(Path_n + tipo_de_archivo)\n",
    "    archivo_mas_reciente = max(archivos, key=os.path.getmtime)\n",
    "    nombre_del_archivo_N = os.path.basename(archivo_mas_reciente)\n",
    "    print(archivo_mas_reciente)\n",
    "    # Lee el archivo sin especificar el tipo de datos\n",
    "    df = pd.read_excel(archivo_mas_reciente , sheet_name=sheet_name)\n",
    "    # Si la columna \"COMENTARIO\" existe, cambia su tipo de datos a str\n",
    "    if 'COMENTARIO' in df.columns:\n",
    "        df['COMENTARIO'] = df['COMENTARIO'].astype(str)\n",
    "    return df,nombre_del_archivo_N\n",
    "\n",
    "\n",
    "def get_recent_df_by_N(Carpeta_path: str, sheet_name: str, prefijo_nombre_archivo: str):\n",
    "    \"\"\" Devuelve el df de la hoja especifica, del archivo mas reciente de la carpeta especificada, con el nombre especificado\"\"\"\n",
    "    # Busca el archivo más reciente que comienza con el prefijo_nombre_archivo\n",
    "    archivos = glob.glob(os.path.join(Carpeta_path, prefijo_nombre_archivo + '*.xlsx'))\n",
    "    archivo_mas_reciente = max(archivos, key=os.path.getctime)\n",
    "    nombre_del_archivo_N = os.path.basename(archivo_mas_reciente)\n",
    "    print(archivo_mas_reciente)\n",
    "    # Lee el archivo sin especificar el tipo de datos\n",
    "    df = pd.read_excel(archivo_mas_reciente , sheet_name=sheet_name)\n",
    "    # Si la columna \"COMENTARIO\" existe, cambia su tipo de datos a str\n",
    "    if 'COMENTARIO' in df.columns:\n",
    "        df['COMENTARIO'] = df['COMENTARIO'].astype(str)\n",
    "    return df,archivo_mas_reciente\n",
    "\n",
    "\n",
    "def get_recent_csv(Carpeta_path: str):\n",
    "    \"\"\" Devuelve el df del archivo csv más reciente de la carpeta especificada\"\"\"\n",
    "    Path_n = Carpeta_path + '/*'\n",
    "    tipo_de_archivo = '*.csv'\n",
    "    # Busca el archivo más reciente\n",
    "    archivos = glob.glob(Path_n + tipo_de_archivo)\n",
    "    archivo_mas_reciente = max(archivos, key=os.path.getmtime)\n",
    "    nombre_del_archivo_N = os.path.basename(archivo_mas_reciente)\n",
    "    print(archivo_mas_reciente)\n",
    "    # Lee el archivo sin especificar el tipo de datos\n",
    "    df = pd.read_csv(archivo_mas_reciente)\n",
    "    # Si la columna \"COMENTARIO\" existe, cambia su tipo de datos a str\n",
    "    if 'COMENTARIO' in df.columns:\n",
    "        df['COMENTARIO'] = df['COMENTARIO'].astype(str)\n",
    "    return df, nombre_del_archivo_N\n",
    "\n",
    "def Process_PAP(PAP:pd.DataFrame):\n",
    "    PAP_f = PAP[['OC Posición','N° Sol','Estado','Acción','Nombre responsable','Id.SIte',\n",
    "                 'SIte','# Días','F.Creación','F.Modifica']]\n",
    "    ##Filtro solo del norte\n",
    "    PAP_f = PAP_f.dropna(subset=['Id.SIte'])\n",
    "    PAP_f = PAP_f[PAP_f['# Días'] < 500]\n",
    "    \n",
    "    PAP_f_N = PAP_f[PAP_f['Id.SIte'].str.startswith(('L','T','SAD','CL','CAC'))].copy()\n",
    "    PAP_f_N = PAP_f_N.rename(columns={'N° Sol': 'PAP', #Rename\n",
    "                                   'SIte': 'SITE',\n",
    "                                   'Estado': 'ESTADO_PAP',\n",
    "                                    '# Días': 'ANTIGUAMIENTO_PAP',\n",
    "                                    'Id.SIte': 'ID Site',\n",
    "                                 'Nombre responsable': 'RESPONSABLE_PAP' })\n",
    "    PAP_S = Ac.split_ocs(PAP_f_N) # Spliteo OCs\n",
    "    return PAP_S\n",
    "def pre_proces(df: pd.DataFrame ,columns_2str: list[str] ,column_filter: str,C_format) -> pd.DataFrame: \n",
    "    \"\"\"Funcion que filtra y convierte a str ciertas columnas especificadas \"\"\"\n",
    "    ## Preprosecing of PREP_NEW\n",
    "    df = convert_columns(df.copy(),columns_2str,C_format ) #Convert to str a key column\n",
    "    df_EI = df.loc[df[column_filter] == 'Eduardo Iberico']#Filter\n",
    "    df_EI = df_EI.copy()  # Crea una copia del DataFrame original para evitar modificar los datos originales\n",
    "    return df_EI\n",
    "def convert_columns(df, columns,type):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].astype(type)\n",
    "    return df\n",
    "\n",
    "def tratar_codigo(codigo):\n",
    "    # Si el código es vacío o nulo, devolver tal cual\n",
    "    if pd.isna(codigo) or codigo == '':\n",
    "        return codigo\n",
    "    \n",
    "    # Si el código tiene una letra extra en el medio\n",
    "    if not re.match(r'^[A-Za-z]{2}\\d{4}$', codigo):\n",
    "        # Quitar la tercera letra si es una letra\n",
    "        if codigo[2].isalpha():\n",
    "            codigo = codigo[:2] + codigo[3:]\n",
    "        # Quitar los números extras al final si son números\n",
    "        while len(codigo) > 6 and codigo[-1].isdigit():\n",
    "            codigo = codigo[:-1]\n",
    "    \n",
    "    return codigo\n",
    "\n",
    "def compact_rows(df:pd.DataFrame, columns:list, delimiter:str ='/'):\n",
    "    \"\"\" Entra el DF, Las conlumnas key, y el delimitador\"\"\"\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "    # Usar 'join' como función de agregación para concatenar los valores\n",
    "    agg_func = {col: lambda x: '/'.join(x.unique()) for col in df.columns if col not in  columns}\n",
    "    \n",
    "    # Agrupar por 'OC Posición' y aplicar la función de agregación\n",
    "    df1 = df.groupby(columns).agg(agg_func).reset_index()\n",
    "    return df1\n",
    "def contar_prefijos(lista):\n",
    "        contador = defaultdict(int)\n",
    "        for cadena in lista:\n",
    "            for i in range(1, len(cadena) + 1):\n",
    "                prefijo = cadena[:i]\n",
    "                contador[prefijo] += 1\n",
    "        return contador\n",
    "\n",
    "def normalize_company_names(df, column):\n",
    "    \"\"\"Normaliza los nombres de las empresas en la columna especificada del DataFrame.\"\"\"\n",
    "    # Reemplaza \"SAC\" o \"S. A. C.\" al final de los nombres de las empresas con \"S.A.C\"\n",
    "    df.loc[:,column] = df[column].str.replace(r\"(SAC|S\\. ?A\\. ?C\\.)$\", \"S.A.C.\", regex=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def update_and_rename(df1, update_cols, new_names):\n",
    "    df = df1.copy()\n",
    "    for col in update_cols:\n",
    "        df[col[0]].update(df[col[1]])\n",
    "    df.rename(columns=new_names, inplace=True)\n",
    "    df.drop(columns=[col for sublist in update_cols for col in sublist[1:]], inplace=True)\n",
    "    return df\n",
    "def combine_and_rename(df1, combine_cols, new_names):\n",
    "    df = df1.copy()\n",
    "    for cols in combine_cols:\n",
    "        df[cols[0]] = df[cols[0]].combine_first(df[cols[1]])\n",
    "    df.rename(columns=new_names, inplace=True)\n",
    "    df.drop(columns=[col for sublist in combine_cols for col in sublist[1:]], inplace=True)\n",
    "    return df\n",
    "def load_merge(directorio:str):\n",
    "    # Dicc para almacenar los DataFrames\n",
    "    dfs = {}\n",
    "    # Itera sobre todos los archivos en el directorio\n",
    "    for filename in os.listdir(directorio):\n",
    "        file_path = os.path.join(directorio, filename)\n",
    "        # Verifica si el path es un archivo y no un directorio\n",
    "        if os.path.isfile(file_path):\n",
    "            df = pd.read_excel(file_path)  \n",
    "            # Obtiene el nombre del archivo sin la extensión\n",
    "            nombre_sin_extension = os.path.splitext(filename)[0]\n",
    "            # Añade el DataFrame al diccionario\n",
    "            dfs[nombre_sin_extension] = df\n",
    "    ## Etiqueto cada Df por su temporalidad, añado una columna mas con dicha etiqueta         \n",
    "    for tiempo, df in dfs.items():\n",
    "        df['TIME'] = tiempo\n",
    "    # Combina todos los dataframes en uno solo\n",
    "    df_combinado = pd.concat(dfs.values())\n",
    "    return df_combinado \n",
    "def clean_nan(df:pd.DataFrame,column:str):\n",
    "    if filas_count > filas_normal:\n",
    "        df.loc[:,column] = df.loc[:,column].replace('nan',np.nan)\n",
    "        return df\n",
    "    else: return df\n",
    "def process_to_bcsv(df : pd.DataFrame,ruta_del_csv : str,Fecha: str):\n",
    "    df['TIME'] = Fecha\n",
    "    pivot_table = pd.pivot_table(df, values='EN_PROC_USD', index=['TIME','FECHA_DOC','RESPONSABLE_DE_EA','Estado de EA'], aggfunc=pd.Series.sum) #Agrupo \n",
    "    df_reset = pivot_table.reset_index(drop=False)\n",
    "    #convetir la columna del agrupado al formato d efehca \n",
    "    # Convierte la columna 'Fecha' a datetime\n",
    "    df_reset['TIME'] = pd.to_datetime(df_reset['TIME'],format = '%d-%m-%Y')\n",
    "    \n",
    "    # Formatea la columna 'Fecha'\n",
    "    df_reset['TIME Format'] = df_reset['TIME'].apply(lambda x: format_date(x, 'EEE dd-MM-yyyy', locale=Locale('es', 'ES')))\n",
    "    #return df_reset\n",
    "    df_reset.to_csv(ruta_del_csv, mode='a', header=False,index=False)\n",
    "def get_OCS (df:pd.DataFrame): ## Correguir esta funcion(Solo debe hacer una cosa)\n",
    "    EA_PAP_clean_4_SAP = df[(df.SITE.isna())|(df.SITE == 'nan')] #filto2\n",
    "    listocs = EA_PAP_clean_4_SAP['CONCATENADO'].astype(str).tolist() # Creo lsitas de OCS\n",
    "    resultado = [valor[4:] for valor in listocs] # Tomo los valores luego del 4500\n",
    "    \n",
    "    a,b= MostCL_prefix(resultado)\n",
    "    return a,b\n",
    "def MostCL_prefix(lista : list):\n",
    "    b = 0\n",
    "    list_n = []\n",
    "    list_n1 = {}\n",
    "    contador = contar_prefijos(lista)\n",
    "    for key in contador.keys(): \n",
    "       ###Aqui iria la nueva condicional### \n",
    "        if contador[key] < b:\n",
    "               list_n.append(key)\n",
    "        b = contador[key]\n",
    "    claves = list(contador.keys())\n",
    "    for clave_dada in list_n:\n",
    "           indice = claves.index(clave_dada)\n",
    "           clave_anterior = claves[indice - 1]\n",
    "           #list_n.append(clave_anterior)\n",
    "           list_n1[clave_anterior] = contador[clave_anterior]\n",
    "    return list_n1,contador\n",
    "def limpiar_id(df, col_id, col_nombre):\n",
    "    # Convertir la columna de ID a string para poder hacer la comparación\n",
    "    df[col_id] = df[col_id].astype(str)\n",
    "    \n",
    "    # Crear una función para limpiar el nombre\n",
    "    def limpiar_nombre(row):\n",
    "        nombre = str(row[col_nombre])  # Convertir el nombre a string\n",
    "        id_actual = row[col_id]\n",
    "        \n",
    "        # Dividir el nombre por \"_\" ,\"-\" y \" \" \n",
    "        partes = re.split('_|-| ', nombre)\n",
    "        \n",
    "        # Si el ID está en el nombre, eliminarlo\n",
    "        if id_actual in partes:\n",
    "            partes.remove(id_actual)\n",
    "        \n",
    "        # Devolver el nombre limpio\n",
    "        return ' '.join(partes)\n",
    "    \n",
    "    # Aplicar la función de limpieza a cada fila del DataFrame\n",
    "    df[col_nombre] = df.apply(limpiar_nombre, axis=1)\n",
    "    return df\n",
    "def data_date(path:str):\n",
    "    modification_time = os.path.getctime(path)\n",
    "    dt = datetime.fromtimestamp(modification_time)  \n",
    "    dt = dt.date()\n",
    "    # Formatea la fecha en el formato deseado\n",
    "    return dt\n",
    "\n",
    "\n",
    "def standar_columns(df:pd.DataFrame,columns:list[str]):\n",
    "    try:\n",
    "        df1 = df[columns].copy()\n",
    "    except KeyError as e:\n",
    "        # Identificamos las columnas faltantes\n",
    "        missing_columns = list(set(columns) - set(df.columns))\n",
    "        # Rellenamos las columnas faltantes con valores vacíos\n",
    "        for col in missing_columns:\n",
    "            df.loc[:,col] = ''\n",
    "        # Seleccionamos las columnas nuevamente\n",
    "        df1 = df[columns].copy()   \n",
    "    return df1  \n",
    "\n",
    "# Cargar lista de OCs que ya fueron etiquetadas antes\n",
    "with open(r'D:\\Scripts1\\Code\\ActPEA\\CODE\\Temps\\DF_OCS.pkl', \"rb\") as archivo:\n",
    "    Ocs_DF = pickle.load(archivo)\n",
    "# Eliminar duplicados y convertir a lista\n",
    "Ocs_F = Ocs_DF.drop_duplicates()['CONCATENADO'].tolist()\n",
    "\n",
    "def process_2_model(df_real:pd.DataFrame,selector):\n",
    "    \"\"\" Convierte el DF en el formato necesario para hacer una prediccion con el modelo \"\"\"\n",
    "    df_real = df_real[['TEXTO','PROVEEDOR','NOMBRE PROYECTO','TIPO_PROYECTOS'\n",
    "                             ,'CLASIF_RED_1']]\n",
    "    \n",
    "    encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "    encoded_data = encoder.fit_transform(df_real)\n",
    "    # Aplica el codificador a tus datos reales\n",
    "    df_real_encoded = encoder.transform(df_real)\n",
    "    \n",
    "    # Convierte el resultado en un DataFrame\n",
    "    df_real_encoded = pd.DataFrame(df_real_encoded, columns=encoder.get_feature_names_out(df_real.columns))\n",
    "\n",
    "    for col in selector.feature_names_in_.tolist():\n",
    "        if col not in df_real_encoded.columns:\n",
    "            # Si falta alguna columna en tus datos reales, añade una nueva columna llena de ceros\n",
    "            df_real_encoded[col] = 0\n",
    "    # Ordena las columnas de df_real_encoded para que coincidan con el orden de las columnas en df_encoded\n",
    "    df_real_encoded = df_real_encoded.reindex(columns=selector.feature_names_in_.tolist())\n",
    "    df_real_encoded_select = selector.transform(df_real_encoded)\n",
    "\n",
    "    return df_real_encoded_select\n",
    "def load_pkl(path2load:str):\n",
    "    with open(path2load, \"rb\") as s:\n",
    "        var = pickle.load(s)   \n",
    "    return var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31632e6f-33b0-4e35-afba-f95a024da7f1",
   "metadata": {},
   "source": [
    "# Cargo Archivos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a32ea752-1058-473d-a78a-029e85f7d0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\EA\\Resultados\\EAS_11-09-2024.csv\n"
     ]
    }
   ],
   "source": [
    "df_yesterday, _ = get_recent_csv(r'D:\\EA\\Resultados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd2f9b09-0d81-4d6f-9b16-16e6f99ebfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\C26764\\America Movil Peru S.A.C\\EAS - 1\\EAUPDATE10.09.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Obtener el DataFrame reciente y renombrar columna\n",
    "df_analist, _ = get_recent_df(r'C:\\Users\\C26764\\America Movil Peru S.A.C\\EAS - 1', sheet_name='Sheet1')\n",
    "df_analist.rename(columns={'RESPONSABLE DE EA': 'RESPONSABLE_DE_EA','ESTATUS_GENERAL_x':'ESTATUS_GENERAL'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd6203a9-81b6-4c15-866d-692c2dcdc90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_path = r'\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\EAS\\model_selector_v1.pkl'\n",
    "modelo_path = r'\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\EAS\\model_v2.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f55c9db-3b1a-4a93-9521-888949deb62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Scripts1\\Code\\ActPEA\\archvis\\PAP\\PAP 12.09.xlsx\n",
      "D:\\Scripts1\\Code\\ActPEA\\archvis\\PAP\\Administrativo\\PAP A 12.09.2024.xlsx\n"
     ]
    }
   ],
   "source": [
    "##########PAP################ MAIN_2\n",
    "PAP_O,_ = get_recent_df_B(r'D:\\Scripts1\\Code\\ActPEA\\archvis\\PAP','Hoja2')# PAP de Otros\n",
    "PAP_A,_ = get_recent_df(r'D:\\Scripts1\\Code\\ActPEA\\archvis\\PAP\\Administrativo','Hoja2')# PAP de Admin\n",
    "PAP = pd.concat([PAP_O,PAP_A])\n",
    "PAP_DB = PAP[['Id.SIte','SIte']].drop_duplicates(subset='Id.SIte').rename(columns={'Id.SIte':'ID_SITIO','SIte':'SITE'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a8db6a1-5e5b-445a-ab13-7a1a98018e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\C26764\\America Movil Peru S.A.C\\EAS - 2\\Pendiente de entrada al 09.09.24.xlsx\n"
     ]
    }
   ],
   "source": [
    "PEA,Path_data = get_recent_df_by_N(r'C:\\Users\\C26764\\America Movil Peru S.A.C\\EAS - 2',sheet_name= 'DATA',# Ruta carpeta de EA Base\n",
    "                                              prefijo_nombre_archivo='Pendiente de entrada al') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abc78ba3-f73e-4ae6-9e36-c6b7c14734fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El reporte es de hace 2 dias\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime ## Calculo la aniguedad del Reporte \n",
    "global reporte_old\n",
    "data_time = data_date(Path_data)\n",
    "a = Today_date-data_time\n",
    "if data_time == Today_date:\n",
    "    reporte_old = False\n",
    "    print(\"El reporte es actual\")\n",
    "elif data_time < Today_date:\n",
    "    reporte_old = True\n",
    "    print(f\"El reporte es de hace {a.days} dias\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f95be4b-5664-4cf8-90a4-2f8e017924df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\EAS\\EAs_done.pkl', \"rb\") as f:\n",
    "    lista_EA_done = pickle.load(f)\n",
    "\n",
    "if Path_data in lista_EA_done: # compruebo si ya ejecute el script sobre esa base de archivo \n",
    "    reporte_old = True\n",
    "else: \n",
    "    reporte_old = False \n",
    "    lista_EA_done.append(Path_data) ##añado a la lista de ya ejecutados \n",
    "    with open(r'\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\EAS\\EAs_done.pkl', \"wb\") as archivo:\n",
    "        pickle.dump(lista_EA_done,archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65550f02-f425-412c-b24d-42b6d214b299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Scripts1\\Code\\ActPEA\\archvis\\OTs\\OTS 09.09.2024.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Archivo PAP OTs\n",
    "OT, _ = get_recent_df(r'D:\\Scripts1\\Code\\ActPEA\\archvis\\OTs', 'Hoja2')\n",
    "OT = OT.query(\"`Status OT` not in ['PDTE CONTRATA', 'PDTE RESPONSABLE']\")\n",
    "\n",
    "# Aplica la función a toda la columna 'Fecha de Creación' y convierte a datetime\n",
    "OT['Fecha de Creación'] = pd.to_datetime(OT['Fecha de Creación'].apply(cortar_hora), format='%d/%m/%Y', dayfirst=True)\n",
    "\n",
    "# Filtra las OTs solo de este año\n",
    "fecha_limite = pd.Timestamp('2022-01-01')\n",
    "OT_cut = OT[OT['Fecha de Creación'] > fecha_limite].copy()\n",
    "\n",
    "# Normaliza los nombres de las compañías\n",
    "OT_cut = normalize_company_names(OT_cut, 'Contrata')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83d0a385-1c9a-45e2-bb2e-75abea323963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Scripts1\\Code\\ActPEA\\archvis\\EAS\\Eas 12.09.xlsx\n"
     ]
    }
   ],
   "source": [
    "EAs,_= get_recent_df(r'D:\\Scripts1\\Code\\ActPEA\\archvis\\EAS','Hoja2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f184a8-df85-4d76-bb92-e592ceaec207",
   "metadata": {},
   "source": [
    "# Preproces \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e4df3-6c4c-4f4e-b7fd-739a7887ca37",
   "metadata": {},
   "source": [
    "## PAP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d451acf-5922-486f-bd83-fa2b0336e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar columnas y realizar merge\n",
    "locale.setlocale(locale.LC_ALL, '')\n",
    "PAP_N_S = Process_PAP(PAP).query(\"`ESTADO_PAP` != 'Rechazado'\")\n",
    "PAP_N_S.drop_duplicates(subset='OC Posición',inplace=True)\n",
    "PAP2M = PAP_N_S[['PAP', 'ESTADO_PAP', 'SITE', 'ANTIGUAMIENTO_PAP', 'OC Posición', 'F.Creación']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5275fdeb-eb30-466a-824c-3fc528cca27b",
   "metadata": {},
   "source": [
    "## EAs-Registradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c16ab23b-1ff3-4ebb-9808-004e21c984d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'es_ES.UTF-8'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########### EAS DEL PAP ###############################\n",
    "# Drop columna imputacion y filtrar datos\n",
    "EAs_2Merge = (EAs.drop(columns=['IMPUTACION'])\n",
    "              .query(\"GERENCIA != 'Proyectos OyM'\") #Filtro columnas\n",
    "              .rename(columns={'OC':'NUMERO OC', 'Posición': 'POS_PREC'})\n",
    "              .astype({'POS_PREC': 'int64', 'NUMERO OC': 'int64'}) #Cambio el tipo de dato\n",
    "              [['NUMERO OC', 'POS_PREC', 'Estado EA', 'F.Aprob']] # Filtro estas columnas \n",
    "              .replace({'Estado EA': {'AF ejecutada':'Ea ejecutada',\n",
    "                                      'Aprobado': 'Ea ejecutada', \n",
    "                                      'Observado Soporte': 'Observado', \n",
    "                                      'En Registro': 'PENDIENTE'}})) # Reemplazo valores\n",
    "# Configurar localización y convertir fechas\n",
    "locale.setlocale(locale.LC_ALL, '')\n",
    "EAs_2Merge['F.Aprob'] = pd.to_datetime(EAs_2Merge['F.Aprob'], format=\"%d/%m/%Y %I:%M:%S %p\").dt.date\n",
    "locale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7485ffc5-6ff9-4975-ad02-a0e81fce6ba6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 68807 entries, 0 to 68806\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   NUMERO OC  68807 non-null  int64 \n",
      " 1   POS_PREC   68807 non-null  int64 \n",
      " 2   Estado EA  68807 non-null  object\n",
      " 3   F.Aprob    66375 non-null  object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "EAs_2Merge.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603dbcab-27c2-43e2-9181-8b13363dbf6d",
   "metadata": {},
   "source": [
    "## OTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c96d2d3-6696-48b4-8dd3-f730a0b4b8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RollOUT\n",
    "# Definir proyectos y filtrar DataFrame\n",
    "Proyectos = ['ROLLOUT - 2023', 'ROLLOUT - 2022', 'ROLLOUT - 2024']\n",
    "OT_cut_RRL = (OT_cut[OT_cut.Etiqueta.isin(Proyectos)]\n",
    "              .rename(columns={'Codigo de Site': 'ID_SITIO',\n",
    "                               'Contrata': 'PROVEEDOR',\n",
    "                               'Nombre de Site': 'SITE'})\n",
    "              [['OT', 'ID_SITIO', 'SITE', 'Proyecto', 'PROVEEDOR', 'Status OT']])\n",
    "\n",
    "# Compactar filas\n",
    "OT_agg_ID_PRO_RLL = compact_rows(OT_cut_RRL, ['ID_SITIO', 'PROVEEDOR'], '/')\n",
    "OT_agg_ID_NM_PRO_RLL = compact_rows(OT_cut_RRL, ['ID_SITIO', 'SITE', 'PROVEEDOR'], '/')\n",
    "\n",
    "#MOdernizacion\n",
    "# Filtrar DataFrame y renombrar columnas\n",
    "OT_cut_M = (OT_cut[OT_cut.Proyecto == 'EXPANSIÓN']\n",
    "            .rename(columns={'Codigo de Site': 'ID_SITIO',\n",
    "                             'Contrata': 'PROVEEDOR',\n",
    "                             'Nombre de Site': 'SITE',\n",
    "                             'Proyecto': 'RESPONSABLE_DE_EA'})\n",
    "            .assign(RESPONSABLE_DE_EA='ANGGIE')\n",
    "            [['OT', 'ID_SITIO', 'SITE', 'RESPONSABLE_DE_EA', 'PROVEEDOR', 'Status OT']])\n",
    "\n",
    "# Compactar filas\n",
    "OT_agg_ID_PRO_M = compact_rows(OT_cut_M, ['ID_SITIO', 'PROVEEDOR'])\n",
    "OT_agg_ID_N_PRO_M = compact_rows(OT_cut_M, ['ID_SITIO', 'SITE', 'PROVEEDOR'])\n",
    "\n",
    "\n",
    "## Energia\n",
    "# Definir fecha límite y filtrar DataFrame\n",
    "fecha_limite = pd.Timestamp('2023-02-15')\n",
    "OT_cut = (OT[OT['Fecha de Creación'] > fecha_limite]\n",
    "          .pipe(normalize_company_names, 'Contrata'))\n",
    "\n",
    "# Filtrar por tipo de requerimiento y proyectos de energía\n",
    "Proy_energia = ['AC ESTABILIZADA', 'AA', 'INCREMENTO DE POTENCIA (INTERNO - CONSTRUCCIÓN)',\n",
    "                'AC COMERCIAL', 'ENERGÍA DC', 'AMPLIACION DE POTENCIA']\n",
    "OT_cut_E = (OT_cut[OT_cut['Tipo Req'] == 'MANT. MEJORA DE RED']\n",
    "            .rename(columns={'Codigo de Site': 'ID_SITIO',\n",
    "                             'Contrata': 'PROVEEDOR',\n",
    "                             'Nombre de Site': 'SITE',\n",
    "                             'Proyecto': 'RESPONSABLE_DE_EA'})\n",
    "            .query(\"RESPONSABLE_DE_EA in @Proy_energia\")# El @ se usa para referirse a una variable externa\n",
    "            .assign(RESPONSABLE_DE_EA='JORGE')  \n",
    "            [['OT', 'ID_SITIO', 'SITE', 'RESPONSABLE_DE_EA', 'PROVEEDOR', 'Status OT']])\n",
    "\n",
    "# Compactar filas y contar valores de 'Status OT'\n",
    "OT_cut_E_ID_PRO = compact_rows(OT_cut_E, ['ID_SITIO', 'PROVEEDOR'])\n",
    "#status_counts = OT_cut_E_ID_PRO['Status OT'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d266d0-fc96-4e93-88fa-8f03967fadca",
   "metadata": {},
   "source": [
    "## PEA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9df13432-a445-46ec-ae86-265a0f7e1145",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Proceso el archivo de finanzas ##########################\n",
    "# Preprocesar y normalizar nombres de compañías\n",
    "PEA_EI = (pre_proces(PEA, ['DOC_PREC', 'POS_PREC', 'CONCATENADO'], 'SOLICITANTE', 'int64')\n",
    "          .pipe(normalize_company_names, 'PROVEEDOR')\n",
    "          .pipe(pre_proces, ['DOC_PREC', 'POS_PREC', 'CONCATENADO'], 'SOLICITANTE', 'str'))\n",
    "\n",
    "# Crear columna \"OC Posición\" y calcular monto en USD\n",
    "PEA_EI[\"OC Posición\"] = PEA_EI[\"DOC_PREC\"].str.cat(PEA_EI[\"POS_PREC\"], sep=\":\")\n",
    "Cash_In = PEA_EI[\"EN_PROC_USD\"].sum()\n",
    "# Tratar los códigos\n",
    "PEA_EI['ID_SITIO'] = PEA_EI['ID_SITIO'].apply(tratar_codigo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "241b3140-52f8-45a6-8785-d15439a1921d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4853789.903225807"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PEA_EI.EN_PROC_USD.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1091bf28-d418-47cc-baca-f5f81d234fb7",
   "metadata": {},
   "source": [
    "## PEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93b36b7d-c17c-49fb-93b7-f768a13d04a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Scripts1\\Code\\ActPEA\\archvis\\PEXT_S\\SEGUIMIENTO DE PROYECTOS 2024-3.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\C26764\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    PEXT,_ = get_recent_df(r'D:\\Scripts1\\Code\\ActPEA\\archvis\\PEXT_S',sheet_name='General') # Ruta archivo pEXT\n",
    "    PEXT.loc[:,'REQ'] = PEXT.loc[:,'REQ'].fillna(0000)\n",
    "    # Filtrar y seleccionar columnas\n",
    "    PEXT_N = (PEXT[PEXT.REGION == 'NORTE']\n",
    "              .iloc[:, 1:]\n",
    "              [['REQ', 'ID', 'SITE', 'RESPONSABLE DE IMPLEMENTACION', 'CONTRATISTA', 'ESTATUS GENERAL', 'ORDEN DE COMPRA', 'POS']]\n",
    "              .astype({'REQ': 'int32', 'ORDEN DE COMPRA': 'str', 'POS': 'str'}))\n",
    "    \n",
    "    # Crear columna \"OC Posición\" y reemplazar valores 'nan'\n",
    "    PEXT_N[\"OC Posición\"] = PEXT_N[\"ORDEN DE COMPRA\"].str.cat(PEXT_N[\"POS\"], sep=\":\")\n",
    "    PEXT_N.replace({'ORDEN DE COMPRA': {'nan': np.nan}, 'POS': {'nan': np.nan}, 'OC Posición': {'nan:nan': np.nan}}, inplace=True)\n",
    "    \n",
    "    # Eliminar columnas y filtrar filas no nulas\n",
    "    PEXT_N.drop(columns=['ORDEN DE COMPRA', 'POS'], inplace=True)\n",
    "    PEXT_N_OCs = PEXT_N[~PEXT_N['OC Posición'].isna()].copy()\n",
    "    PEXT_N_OCs['REQ'] = PEXT_N_OCs['REQ'].astype(str)\n",
    "    \n",
    "    for col in PEXT_N_OCs.columns:\n",
    "        PEXT_N_OCs[col] = PEXT_N_OCs[col].astype(str)\n",
    "    \n",
    "    agg_func = {col: 'first' for col in PEXT_N_OCs.columns if col != 'OC Posición'}\n",
    "    # Agrupar por 'Asignatura' y aplicar la función de agregación\n",
    "    PEXT_agg = PEXT_N_OCs.groupby('OC Posición').agg(agg_func).reset_index()\n",
    "except: pass \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41be4b5b-f25b-49ff-a374-581275b7f33b",
   "metadata": {},
   "source": [
    "# PROCES MAIN\n",
    " - @TODO, Cambiar el orden del PAP y la tabla SAP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2966b004-081e-4aa4-b700-096837a2a801",
   "metadata": {},
   "source": [
    "### Actualizo alguna informacion de la tablas SAP(SAP_4_use/DB PAP) -- Solo se ejecuta cuando hay nuevo reporte "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e683059c-b359-4d95-98d7-3b30175b420a",
   "metadata": {},
   "source": [
    "### TABLAS SAP - DB SITE PAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32298efc-2171-404a-ba9d-cddd47e750dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El reporte es OLD, uso la base\n"
     ]
    }
   ],
   "source": [
    "\n",
    "continuar = True\n",
    "if not reporte_old:\n",
    "    directorio = r'\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\EAS\\Tablas SAP'\n",
    "    while continuar:\n",
    "        SAP_4_use = (load_merge(directorio)\n",
    "                      .pipe(Ac.convert_columns_to_str, ['OC','Pos']) # Uso pipe para poder encadenar funciones, dentro de pipe puedo pasar cualquier funcion a ejecutar en el df con sus argumentos\n",
    "                      .assign(CONCATENADO=lambda df: df[\"OC\"].str.cat(df[\"Pos\"], sep=\"\"))\n",
    "                      .loc[:, ['CONCATENADO','PEP Desc','Fecha OC']]\n",
    "                      .replace({'PEP Desc': {'TJ5125-SANTIAGO DE CHUCO' : 'TJ5125-SANTIAGO_DE_CHUCO'}})\n",
    "                      .dropna(subset=['PEP Desc']))\n",
    "        #Separo por el numero inicial pues aveces hay solpes\n",
    "        EA_4 = PEA_EI[PEA_EI.CONCATENADO.astype(str).str.startswith('4')]\n",
    "        EA_2 = PEA_EI[~PEA_EI.CONCATENADO.astype(str).str.startswith('4')]\n",
    "        EA_SAP = pd.concat([addSite_V2(EA_4.astype(str), SAP_4_use).drop(columns=['PEP Desc']), EA_2], axis=0) # Añado el sitio y concateno del SAP\n",
    "        EA_SAP = clean_nan(EA_SAP,'SITE') # Limpio vacios\n",
    "        EA_SAP_SITE = pd.merge(EA_SAP,PAP_DB,on='ID_SITIO',how='left') ## Añado site del datab del PAP\n",
    "        EA_SAP_SITE = combine_and_rename(EA_SAP_SITE,[('SITE_x', 'SITE_y')],{'SITE_x':'SITE'}) \n",
    "        \n",
    "        EA_SAP_PAP = (pd.merge(EA_SAP_SITE, PAP2M, how='left', on='OC Posición')\n",
    "                      .pipe(update_and_rename, [('SITE_x', 'SITE_y')], {'SITE_x': 'SITE'})\n",
    "                      .rename(columns={'DOC_PREC': 'NUMERO OC'}))\n",
    "        # Convertir fechas y limpiar filas duplicadas\n",
    "        EA_SAP_PAP['F.Creación'] = pd.to_datetime(EA_SAP_PAP['F.Creación'], format=\"%d/%m/%Y %I:%M:%S %p\")\n",
    "        \n",
    "        a,b = get_OCS(EA_SAP_PAP) # obtiene ocs mas frecuentes con sitios vacios\n",
    "\n",
    "        print(f\"Aun faltan descargar: {a}\")\n",
    "        respuesta = input(\"¿Quieres continuar? (s/n): \")\n",
    "        if respuesta.lower() != 's':\n",
    "            continuar = False\n",
    "    #display(EA_SAP_SITE.info())\n",
    "    \n",
    "    EA_SAP_PAP.to_csv(r'D:\\Scripts1\\Code\\ActPEA\\CODE\\Temps\\Base_EAS',index=False)\n",
    "    print(\"Actualizo info de las Tablas SAP\")\n",
    "else:\n",
    "    print(\"El reporte es OLD, uso la base\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22ab32f-6541-4612-b7df-170a76448e05",
   "metadata": {},
   "source": [
    "## Actualizo alguna informacion del PAP(PAP2M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bee3314d-c35a-471b-a829-961c2cc52731",
   "metadata": {},
   "outputs": [],
   "source": [
    "EA_SAP_load = pd.read_csv(r'D:\\Scripts1\\Code\\ActPEA\\CODE\\Temps\\Base_EAS') # literalmete uso la base \n",
    "#EA_SAP_load.drop(columns=['PEP Desc'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67df921e-f9a8-4083-b6e8-559b3bd07ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "847"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(EA_SAP_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58e6695d-f4a3-4e86-a6ed-4d138d97dce0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 847 entries, 0 to 846\n",
      "Data columns (total 54 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   CONCATENADO               847 non-null    int64  \n",
      " 1   NUMERO OC                 847 non-null    int64  \n",
      " 2   POS_PREC                  847 non-null    int64  \n",
      " 3   FECHA_DOC                 847 non-null    object \n",
      " 4   Estrategia de Liberación  847 non-null    object \n",
      " 5   Cod.Solicitante           847 non-null    object \n",
      " 6   SOLICITANTE               847 non-null    object \n",
      " 7   TIPO_DOC                  847 non-null    object \n",
      " 8   TIPO DE MATERIAL          847 non-null    object \n",
      " 9   MATERIAL                  847 non-null    int64  \n",
      " 10  TEXTO                     847 non-null    object \n",
      " 11  COD_PROVEEDOR             847 non-null    float64\n",
      " 12  PROVEEDOR                 847 non-null    object \n",
      " 13  CENTRO_GESTOR             847 non-null    object \n",
      " 14  POS_PRESUP                847 non-null    object \n",
      " 15  MONEDA_ORIG               847 non-null    object \n",
      " 16  COMPROMETIDO              847 non-null    float64\n",
      " 17  RECIBIDO                  847 non-null    float64\n",
      " 18  EN_PROCESO                847 non-null    float64\n",
      " 19  CARRY                     0 non-null      float64\n",
      " 20  COMPROM_USD               847 non-null    float64\n",
      " 21  RECIBIDO_USD              847 non-null    float64\n",
      " 22  EN_PROC_USD               847 non-null    float64\n",
      " 23  ESTATUS                   847 non-null    object \n",
      " 24  PPTO_INICIAL              847 non-null    int64  \n",
      " 25  RECLA_INTERNA             847 non-null    int64  \n",
      " 26  PPTO_FINAL                847 non-null    int64  \n",
      " 27  TIPO_PRES                 847 non-null    object \n",
      " 28  TIPO_PRY                  847 non-null    object \n",
      " 29  SUB_DIRECCION             847 non-null    object \n",
      " 30  CE_GESTOR                 847 non-null    object \n",
      " 31  RUBRO                     847 non-null    object \n",
      " 32  TIPO_PROYECTOS            847 non-null    object \n",
      " 33  fecha reporte             847 non-null    object \n",
      " 34  AÑO                       847 non-null    int64  \n",
      " 35  Proyecto CAPEX            847 non-null    object \n",
      " 36  NOMBRE PROYECTO           847 non-null    object \n",
      " 37  DESCRIPCIÓN PROYECTO      847 non-null    object \n",
      " 38  PEP                       845 non-null    object \n",
      " 39  T.CAMBIO                  847 non-null    float64\n",
      " 40  CLASIF_FINANZAS           847 non-null    object \n",
      " 41  CLASIF_RED_1              847 non-null    object \n",
      " 42  CLASIF_RED_2              847 non-null    object \n",
      " 43  ID_SITIO                  822 non-null    object \n",
      " 44  NUM_SOT                   822 non-null    object \n",
      " 45  NATURALEZA                822 non-null    object \n",
      " 46  BLOQUEO                   0 non-null      float64\n",
      " 47  FECHA_VENTA               0 non-null      float64\n",
      " 48  OC Posición               847 non-null    object \n",
      " 49  SITE                      829 non-null    object \n",
      " 50  PAP                       458 non-null    float64\n",
      " 51  ESTADO_PAP                458 non-null    object \n",
      " 52  ANTIGUAMIENTO_PAP         458 non-null    float64\n",
      " 53  F.Creación                458 non-null    object \n",
      "dtypes: float64(13), int64(8), object(33)\n",
      "memory usage: 357.5+ KB\n"
     ]
    }
   ],
   "source": [
    "EA_SAP_load.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a23c691-ae17-494e-848f-bd4c10304113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4853789.903225807"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EA_SAP_load['EN_PROC_USD'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dac3fae-78cf-4632-b123-0e49366ed908",
   "metadata": {},
   "source": [
    "## Añado info de PEXT \n",
    "- Como es por OC, no me conviene separa los vacios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e12b390-a0cd-4d0e-920b-a8b3d4f188bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    EA_PAP_SAP_PEXT = pd.merge(EA_SAP_load,PEXT_agg,on='OC Posición',how='left')\n",
    "    EA_PAP_SAP_PEXT = combine_and_rename(EA_PAP_SAP_PEXT,[('SITE_x', 'SITE_y')],{'SITE_x':'SITE'})\n",
    "except: \n",
    "    EA_PAP_SAP_PEXT = EA_SAP_load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e34291c-89ed-4098-ba5f-d7acda97f251",
   "metadata": {},
   "source": [
    "## Añado info de las OTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6accd7e5-d2b0-41b0-bc67-5bded88a76bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 847 entries, 0 to 846\n",
      "Data columns (total 60 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   CONCATENADO               847 non-null    int64  \n",
      " 1   NUMERO OC                 847 non-null    int64  \n",
      " 2   POS_PREC                  847 non-null    int64  \n",
      " 3   FECHA_DOC                 847 non-null    object \n",
      " 4   Estrategia de Liberación  847 non-null    object \n",
      " 5   Cod.Solicitante           847 non-null    object \n",
      " 6   SOLICITANTE               847 non-null    object \n",
      " 7   TIPO_DOC                  847 non-null    object \n",
      " 8   TIPO DE MATERIAL          847 non-null    object \n",
      " 9   MATERIAL                  847 non-null    int64  \n",
      " 10  TEXTO                     847 non-null    object \n",
      " 11  COD_PROVEEDOR             847 non-null    float64\n",
      " 12  PROVEEDOR                 847 non-null    object \n",
      " 13  CENTRO_GESTOR             847 non-null    object \n",
      " 14  POS_PRESUP                847 non-null    object \n",
      " 15  MONEDA_ORIG               847 non-null    object \n",
      " 16  COMPROMETIDO              847 non-null    float64\n",
      " 17  RECIBIDO                  847 non-null    float64\n",
      " 18  EN_PROCESO                847 non-null    float64\n",
      " 19  CARRY                     0 non-null      float64\n",
      " 20  COMPROM_USD               847 non-null    float64\n",
      " 21  RECIBIDO_USD              847 non-null    float64\n",
      " 22  EN_PROC_USD               847 non-null    float64\n",
      " 23  ESTATUS                   847 non-null    object \n",
      " 24  PPTO_INICIAL              847 non-null    int64  \n",
      " 25  RECLA_INTERNA             847 non-null    int64  \n",
      " 26  PPTO_FINAL                847 non-null    int64  \n",
      " 27  TIPO_PRES                 847 non-null    object \n",
      " 28  TIPO_PRY                  847 non-null    object \n",
      " 29  SUB_DIRECCION             847 non-null    object \n",
      " 30  CE_GESTOR                 847 non-null    object \n",
      " 31  RUBRO                     847 non-null    object \n",
      " 32  TIPO_PROYECTOS            847 non-null    object \n",
      " 33  fecha reporte             847 non-null    object \n",
      " 34  AÑO                       847 non-null    int64  \n",
      " 35  Proyecto CAPEX            847 non-null    object \n",
      " 36  NOMBRE PROYECTO           847 non-null    object \n",
      " 37  DESCRIPCIÓN PROYECTO      847 non-null    object \n",
      " 38  PEP                       845 non-null    object \n",
      " 39  T.CAMBIO                  847 non-null    float64\n",
      " 40  CLASIF_FINANZAS           847 non-null    object \n",
      " 41  CLASIF_RED_1              847 non-null    object \n",
      " 42  CLASIF_RED_2              847 non-null    object \n",
      " 43  ID_SITIO                  822 non-null    object \n",
      " 44  NUM_SOT                   822 non-null    object \n",
      " 45  NATURALEZA                822 non-null    object \n",
      " 46  BLOQUEO                   0 non-null      float64\n",
      " 47  FECHA_VENTA               0 non-null      float64\n",
      " 48  OC Posición               847 non-null    object \n",
      " 49  SITE                      830 non-null    object \n",
      " 50  PAP                       458 non-null    float64\n",
      " 51  ESTADO_PAP                458 non-null    object \n",
      " 52  ANTIGUAMIENTO_PAP         458 non-null    float64\n",
      " 53  F.Creación                458 non-null    object \n",
      " 54  REQ                       20 non-null     object \n",
      " 55  ID                        20 non-null     object \n",
      " 56  RESPONSABLE_DE_EA         297 non-null    object \n",
      " 57  CONTRATISTA               20 non-null     object \n",
      " 58  ESTATUS_GENERAL           297 non-null    object \n",
      " 59  OT                        277 non-null    object \n",
      "dtypes: float64(13), int64(8), object(39)\n",
      "memory usage: 397.2+ KB\n"
     ]
    }
   ],
   "source": [
    "########### ROLL OUT ###################################### \n",
    "# Merge a 3 columnas\n",
    "EA_PAP_PEXT_OTS_3 = (pd.merge(EA_PAP_SAP_PEXT, OT_agg_ID_NM_PRO_RLL, on=['ID_SITIO', 'SITE', 'PROVEEDOR'], how='left')\n",
    "                     .assign(ESTATUS_GENERAL=lambda x: x['ESTATUS GENERAL'].combine_first(x['Status OT']))\n",
    "                     .drop(columns=['Status OT']))\n",
    "EA_PAP_PEXT_OTS_3.drop(columns=['ESTATUS GENERAL'],inplace=True)\n",
    "# Separar por los que tienen OT(Hcieron merge) y el resto(no hicieron merge)\n",
    "EA_PAP_PEXT_OTS_3_R = EA_PAP_PEXT_OTS_3[~EA_PAP_PEXT_OTS_3.OT.isna()]\n",
    "EA_PAP_PEXT_OTS_3_NR = (EA_PAP_PEXT_OTS_3[EA_PAP_PEXT_OTS_3.OT.isna()]\n",
    "                        .drop(columns=['OT', 'Proyecto']))\n",
    "\n",
    "\n",
    "# Merge a 2 columnas\n",
    "EA_PAP_PEXT_OTS_2 = (pd.merge(EA_PAP_PEXT_OTS_3_NR, OT_agg_ID_PRO_RLL, on=['ID_SITIO', 'PROVEEDOR'], how='left')\n",
    "                     .pipe(combine_and_rename, [('SITE_x', 'SITE_y'), ('ESTATUS_GENERAL', 'Status OT')], {'SITE_x': 'SITE'}))\n",
    "# Unir los dos merges\n",
    "EA_PAP_SAP_PEXT_RLL = pd.concat([EA_PAP_PEXT_OTS_2, EA_PAP_PEXT_OTS_3_R], axis=0)\n",
    "\n",
    "# Resetear índice\n",
    "EA_PAP_SAP_PEXT_RLL = EA_PAP_SAP_PEXT_RLL.reset_index(drop=True)\n",
    "\n",
    "# Reemplazar valores en la columna 'Proyecto'\n",
    "EA_PAP_SAP_PEXT_RLL['Proyecto'].replace({\n",
    "    'STREET CELL/NUEVO RADIOBASE': 'STREET CELL',\n",
    "    'NUEVO RADIOBASE': 'JHORDAN',\n",
    "    'STREET CELL': 'DEMETRIO'\n",
    "}, inplace=True)\n",
    "\n",
    "# Renombrar columna y combinar valores\n",
    "EA_PAP_SAP_PEXT_RLL = (EA_PAP_SAP_PEXT_RLL\n",
    "                       .rename(columns={'RESPONSABLE DE IMPLEMENTACION': 'RESPONSABLE_DE_EA'})\n",
    "                       .assign(RESPONSABLE_DE_EA=lambda x: x['RESPONSABLE_DE_EA'].combine_first(x['Proyecto']))\n",
    "                       .drop(columns=['Proyecto']))\n",
    "\n",
    "# Mostrar información final del DataFrame\n",
    "EA_PAP_SAP_PEXT_RLL.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc277e7e-c42a-4172-b9be-1f6a5cff385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### MODERNIZACION ################################################\n",
    "# Realizar merge inicial en 3 columnas\n",
    "EA_PAP_SAP_PEXT_RLL_M3 = (pd.merge(EA_PAP_SAP_PEXT_RLL, OT_agg_ID_N_PRO_M, on=['ID_SITIO', 'SITE', 'PROVEEDOR'], how='left')\n",
    "                          .pipe(combine_and_rename, [('RESPONSABLE_DE_EA_x', 'RESPONSABLE_DE_EA_y'),\n",
    "                                                    ('OT_x', 'OT_y'), ('ESTATUS_GENERAL', 'Status OT')],\n",
    "                                {'RESPONSABLE_DE_EA_x': 'RESPONSABLE_DE_EA', 'OT_x': 'OT'}))\n",
    "\n",
    "# Separar filas con y sin OT\n",
    "EA_PAP_SAP_PEXT_RLL_M_R = EA_PAP_SAP_PEXT_RLL_M3[~EA_PAP_SAP_PEXT_RLL_M3.OT.isna()]\n",
    "EA_PAP_SAP_PEXT_RLL_M_NA = (EA_PAP_SAP_PEXT_RLL_M3[EA_PAP_SAP_PEXT_RLL_M3.OT.isna()]\n",
    "                            .drop(columns=['OT']))\n",
    "\n",
    "# Realizar merge en 2 columnas\n",
    "EA_PAP_SAP_PEXT_RLL_M2 = (pd.merge(EA_PAP_SAP_PEXT_RLL_M_NA, OT_agg_ID_PRO_M, on=['ID_SITIO', 'PROVEEDOR'], how='left')\n",
    "                          .pipe(combine_and_rename, [('RESPONSABLE_DE_EA_x', 'RESPONSABLE_DE_EA_y'),\n",
    "                                                    ('SITE_x', 'SITE_y'), ('ESTATUS_GENERAL', 'Status OT')],\n",
    "                                {'RESPONSABLE_DE_EA_x': 'RESPONSABLE_DE_EA', 'OT_x': 'OT', 'SITE_x': 'SITE'}))\n",
    "\n",
    "# Unir los resultados de los merges\n",
    "EA_PAP_SAP_PEXT_RLL_M = pd.concat([EA_PAP_SAP_PEXT_RLL_M_R, EA_PAP_SAP_PEXT_RLL_M2], axis=0)\n",
    "\n",
    "########################## ENERGIA #############################333\n",
    "EA_PAP_SAP_PEXT_OTS = (pd.merge(EA_PAP_SAP_PEXT_RLL_M,OT_cut_E_ID_PRO,on=['ID_SITIO','PROVEEDOR'],how='left')\n",
    "                       .pipe(combine_and_rename, [('RESPONSABLE_DE_EA_x','RESPONSABLE_DE_EA_y'),\n",
    "                                                 ('SITE_x','SITE_y'),('ESTATUS_GENERAL','Status OT'),('OT_x','OT_y')],\n",
    "                                                {'RESPONSABLE_DE_EA_x':'RESPONSABLE_DE_EA','OT_x':'OT','SITE_x':'SITE'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48910ec7-ca1b-41d4-aab1-e223d7bb8728",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 847 entries, 0 to 846\n",
      "Data columns (total 60 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   CONCATENADO               847 non-null    int64  \n",
      " 1   NUMERO OC                 847 non-null    int64  \n",
      " 2   POS_PREC                  847 non-null    int64  \n",
      " 3   FECHA_DOC                 847 non-null    object \n",
      " 4   Estrategia de Liberación  847 non-null    object \n",
      " 5   Cod.Solicitante           847 non-null    object \n",
      " 6   SOLICITANTE               847 non-null    object \n",
      " 7   TIPO_DOC                  847 non-null    object \n",
      " 8   TIPO DE MATERIAL          847 non-null    object \n",
      " 9   MATERIAL                  847 non-null    int64  \n",
      " 10  TEXTO                     847 non-null    object \n",
      " 11  COD_PROVEEDOR             847 non-null    float64\n",
      " 12  PROVEEDOR                 847 non-null    object \n",
      " 13  CENTRO_GESTOR             847 non-null    object \n",
      " 14  POS_PRESUP                847 non-null    object \n",
      " 15  MONEDA_ORIG               847 non-null    object \n",
      " 16  COMPROMETIDO              847 non-null    float64\n",
      " 17  RECIBIDO                  847 non-null    float64\n",
      " 18  EN_PROCESO                847 non-null    float64\n",
      " 19  CARRY                     0 non-null      float64\n",
      " 20  COMPROM_USD               847 non-null    float64\n",
      " 21  RECIBIDO_USD              847 non-null    float64\n",
      " 22  EN_PROC_USD               847 non-null    float64\n",
      " 23  ESTATUS                   847 non-null    object \n",
      " 24  PPTO_INICIAL              847 non-null    int64  \n",
      " 25  RECLA_INTERNA             847 non-null    int64  \n",
      " 26  PPTO_FINAL                847 non-null    int64  \n",
      " 27  TIPO_PRES                 847 non-null    object \n",
      " 28  TIPO_PRY                  847 non-null    object \n",
      " 29  SUB_DIRECCION             847 non-null    object \n",
      " 30  CE_GESTOR                 847 non-null    object \n",
      " 31  RUBRO                     847 non-null    object \n",
      " 32  TIPO_PROYECTOS            847 non-null    object \n",
      " 33  fecha reporte             847 non-null    object \n",
      " 34  AÑO                       847 non-null    int64  \n",
      " 35  Proyecto CAPEX            847 non-null    object \n",
      " 36  NOMBRE PROYECTO           847 non-null    object \n",
      " 37  DESCRIPCIÓN PROYECTO      847 non-null    object \n",
      " 38  PEP                       845 non-null    object \n",
      " 39  T.CAMBIO                  847 non-null    float64\n",
      " 40  CLASIF_FINANZAS           847 non-null    object \n",
      " 41  CLASIF_RED_1              847 non-null    object \n",
      " 42  CLASIF_RED_2              847 non-null    object \n",
      " 43  ID_SITIO                  822 non-null    object \n",
      " 44  NUM_SOT                   822 non-null    object \n",
      " 45  NATURALEZA                822 non-null    object \n",
      " 46  BLOQUEO                   0 non-null      float64\n",
      " 47  FECHA_VENTA               0 non-null      float64\n",
      " 48  OC Posición               847 non-null    object \n",
      " 49  SITE                      830 non-null    object \n",
      " 50  PAP                       458 non-null    float64\n",
      " 51  ESTADO_PAP                458 non-null    object \n",
      " 52  ANTIGUAMIENTO_PAP         458 non-null    float64\n",
      " 53  F.Creación                458 non-null    object \n",
      " 54  REQ                       20 non-null     object \n",
      " 55  ID                        20 non-null     object \n",
      " 56  RESPONSABLE_DE_EA         461 non-null    object \n",
      " 57  CONTRATISTA               20 non-null     object \n",
      " 58  ESTATUS_GENERAL           461 non-null    object \n",
      " 59  OT                        442 non-null    object \n",
      "dtypes: float64(13), int64(8), object(39)\n",
      "memory usage: 397.2+ KB\n"
     ]
    }
   ],
   "source": [
    "EA_PAP_SAP_PEXT_OTS.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fffc07-315f-46e2-8421-5611a0cf465f",
   "metadata": {},
   "source": [
    "### Clean Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c05e81c1-8655-4726-a6c0-0241b4cdef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar filas con 'ESTATUS' igual a 'EN PROCESO'\n",
    "EA_PAP_SAP_PEXT_OTS = EA_PAP_SAP_PEXT_OTS[EA_PAP_SAP_PEXT_OTS.ESTATUS == 'EN PROCESO']\n",
    "\n",
    "# Eliminar columnas completamente vacías\n",
    "EA_PAP_SAP_PEXT_OTS = EA_PAP_SAP_PEXT_OTS.dropna(axis=1, how='all')\n",
    "\n",
    "# Eliminar columnas innecesarias\n",
    "cols_to_drop = ['F.Creación', 'REQ', 'ANTIGUAMIENTO_PAP', 'NUM_SOT', 'NATURALEZA', 'OC Posición',\n",
    "                'T.CAMBIO', 'AÑO', 'ESTATUS', 'PPTO_INICIAL', 'PPTO_FINAL', 'RECLA_INTERNA', 'ID',\n",
    "                'RECIBIDO_USD', 'MONEDA_ORIG', 'CONTRATISTA', 'COMPROMETIDO', 'RECIBIDO', 'EN_PROCESO',\n",
    "                'TIPO_PRES', 'TIPO_PRY']\n",
    "EA_PAP_SAP_PEXT_OTS = EA_PAP_SAP_PEXT_OTS.drop(columns=cols_to_drop)\n",
    "\n",
    "# Rellenar valores nulos en 'ESTADO_PAP' y 'PROVEEDOR'\n",
    "EA_PAP_SAP_PEXT_OTS['ESTADO_PAP'] = EA_PAP_SAP_PEXT_OTS['ESTADO_PAP'].fillna('Sin Registrar')\n",
    "EA_PAP_SAP_PEXT_OTS['PROVEEDOR'] = EA_PAP_SAP_PEXT_OTS['PROVEEDOR'].fillna('Sin Proveedor')\n",
    "# Limpiar IDs y convertir 'CONCATENADO' a int64\n",
    "EA_PAP_SAP_PEXT_OTS = limpiar_id(EA_PAP_SAP_PEXT_OTS, 'ID_SITIO', 'SITE')\n",
    "EA_PAP_SAP_PEXT_OTS['CONCATENADO'] = EA_PAP_SAP_PEXT_OTS['CONCATENADO'].astype('int64')\n",
    "# Copiar DataFrame y mapear nombres de analistas\n",
    "EA_act = EA_PAP_SAP_PEXT_OTS.copy()\n",
    "analistas_map = {'JENNY PIZAN': 'JENNY', 'DANNER YARLEQUE': 'DANNER', 'LAURA RAFAEL': 'LAURA'}\n",
    "# Convertir nombres de analistas a mayúsculas y reemplazar según el mapa\n",
    "EA_act['RESPONSABLE_DE_EA'] = EA_act['RESPONSABLE_DE_EA'].str.upper().replace(analistas_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3fc86503-ba42-4043-9606-1790f2ab2082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 847 entries, 0 to 846\n",
      "Data columns (total 36 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   CONCATENADO               847 non-null    int64  \n",
      " 1   NUMERO OC                 847 non-null    int64  \n",
      " 2   POS_PREC                  847 non-null    int64  \n",
      " 3   FECHA_DOC                 847 non-null    object \n",
      " 4   Estrategia de Liberación  847 non-null    object \n",
      " 5   Cod.Solicitante           847 non-null    object \n",
      " 6   SOLICITANTE               847 non-null    object \n",
      " 7   TIPO_DOC                  847 non-null    object \n",
      " 8   TIPO DE MATERIAL          847 non-null    object \n",
      " 9   MATERIAL                  847 non-null    int64  \n",
      " 10  TEXTO                     847 non-null    object \n",
      " 11  COD_PROVEEDOR             847 non-null    float64\n",
      " 12  PROVEEDOR                 847 non-null    object \n",
      " 13  CENTRO_GESTOR             847 non-null    object \n",
      " 14  POS_PRESUP                847 non-null    object \n",
      " 15  COMPROM_USD               847 non-null    float64\n",
      " 16  EN_PROC_USD               847 non-null    float64\n",
      " 17  SUB_DIRECCION             847 non-null    object \n",
      " 18  CE_GESTOR                 847 non-null    object \n",
      " 19  RUBRO                     847 non-null    object \n",
      " 20  TIPO_PROYECTOS            847 non-null    object \n",
      " 21  fecha reporte             847 non-null    object \n",
      " 22  Proyecto CAPEX            847 non-null    object \n",
      " 23  NOMBRE PROYECTO           847 non-null    object \n",
      " 24  DESCRIPCIÓN PROYECTO      847 non-null    object \n",
      " 25  PEP                       845 non-null    object \n",
      " 26  CLASIF_FINANZAS           847 non-null    object \n",
      " 27  CLASIF_RED_1              847 non-null    object \n",
      " 28  CLASIF_RED_2              847 non-null    object \n",
      " 29  ID_SITIO                  847 non-null    object \n",
      " 30  SITE                      847 non-null    object \n",
      " 31  PAP                       458 non-null    float64\n",
      " 32  ESTADO_PAP                847 non-null    object \n",
      " 33  RESPONSABLE_DE_EA         461 non-null    object \n",
      " 34  ESTATUS_GENERAL           461 non-null    object \n",
      " 35  OT                        442 non-null    object \n",
      "dtypes: float64(4), int64(4), object(28)\n",
      "memory usage: 238.3+ KB\n"
     ]
    }
   ],
   "source": [
    "EA_PAP_SAP_PEXT_OTS.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b078103-fb2d-4a49-a554-96166eae9fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La diferencia de montos es:  0.0\n"
     ]
    }
   ],
   "source": [
    "EA_PAP_SAP_PEXT_OTS['EN_PROC_USD'] = EA_PAP_SAP_PEXT_OTS['EN_PROC_USD'].astype(float)\n",
    "EA_PAP_SAP_PEXT_OTS = EA_PAP_SAP_PEXT_OTS.drop_duplicates(subset='CONCATENADO')\n",
    "cash_out = EA_PAP_SAP_PEXT_OTS['EN_PROC_USD'].sum()\n",
    "print(\"La diferencia de montos es: \",Cash_In - cash_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd9fa99-4370-408f-85af-cb7db278599f",
   "metadata": {},
   "source": [
    "# Relleno responsables faltantes con ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7690194-6e51-409c-b494-0e1ad3f04d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cargo el modelo\n",
    "## Creo una copia, proceso el DF\n",
    "## hago prediccion de todo el df, comparo con la columna real solo \n",
    "#### Creo una nueva columasn temporal y en base a ella separo un df donde los valores de esas 2 columnas son diferrentes\n",
    "##### Len de ese dataframe y divido entre el ttal de filas para sacar un porcentaje.\n",
    "## Metrica de cuales coincide, si la medida es alta. Entonces asigno la prediccion alos faltantes\n",
    "## Sino, alerta de reentrenar el modelo, y no se asignan los valores.A futuro gatillar el reentrenamieto del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78c6f2-f3c1-4527-9304-20bfaa947d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_responsable(modelo, df4Model):\n",
    "    predict_array = modelo.predict(df4Model)\n",
    "    return pd.DataFrame(predict_array, columns=['RESPONSABLE_PRED'])\n",
    "def calculate_error(EA_act_NN, df_pred2Comp):\n",
    "    diferencias = EA_act_NN['RESPONSABLE_DE_EA'].compare(df_pred2Comp['RESPONSABLE_PRED'])\n",
    "    const_error = round(len(diferencias) / len(EA_act_NN) * 100, 2)\n",
    "    return const_error, diferencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2cca4ffb-9bc7-4fd1-832c-ae0fcc0ad2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_responsablebyML(EA_act:pd.DataFrame,modelo_path:str,selector_path:str,max_error:int)-> pd.DataFrame:\n",
    "    retrain =True \n",
    "    while retrain\n",
    "        ## Cargo Modelo\n",
    "        selector = load_pkl(selector_path)\n",
    "        modelo = load_pkl(modelo_path)\n",
    "\n",
    "        EA_act_2_ML = EA_act.copy()\n",
    "        EA_act_NN = EA_act[~EA_act['RESPONSABLE_DE_EA'].isna()] # Extraigo las que no son vacias \n",
    "        blank_rows = len(EA_act[EA_act['RESPONSABLE_DE_EA'].isna()])\n",
    "        #blank_rows = len(df_pred2fill)\n",
    "        \n",
    "        if blank_rows > 0:    ## Compruebo si hay filas sin asignar responsable   \n",
    "            df4Model =  process_2_model(EA_act_2_ML,selector) ## Normalizo las columnas como matriz densa\n",
    "            df_pred = predict_responsable(modelo, df4Model) # creo df de predichos para comparar\n",
    "            # Filtrar predicciones para filas no nulas y nulas\n",
    "            df_pred2Comp = df_pred.loc[EA_act_NN.index].copy() ##  seleeciona las filas que no son vacias del predicho \n",
    "            df_pred2fill = df_pred.drop(EA_act_NN.index).copy() ## Dropeo todas las filas que ya tiene responsables, if all have its empty\n",
    "\n",
    "            print(f\"Se llenaran {blank_rows} filas.))\")\n",
    "\n",
    "            const_error, diferencias = calculate_error(EA_act_NN, df_pred2Comp)## Calculo el error y devuelvo el df de diferencias \n",
    "\n",
    "            print(f\"Hay{len(diferencias)Responsables diferentes,{const_error} de erro\") # imprimero el numero de filas diferentes \n",
    "            \n",
    "            if const_error < max_error: # Si el error esta debajo del umbral\n",
    "                df_pred2fill.rename(columns={'RESPONSABLE_PRED':'RESPONSABLE_DE_EA'},inplace=True)\n",
    "                EA_act_2_ML['RESPONSABLE_DE_EA'] = EA_act_2_ML['RESPONSABLE_DE_EA'].combine_first(df_pred2fill['RESPONSABLE_DE_EA'])#Relleno las finlas que no esten vacias\n",
    "                print(f\"El error es de {const_error}. \\nSe etiquetaron {len(df_pred2fill)} filas de responsable por ML\")\n",
    "                retrain = False \n",
    "                return EA_act_2_ML\n",
    "            else: \n",
    "                \n",
    "                respuesta = input(\"El modelo tiene un error por encima del maximo, desea reentrenarlo? (s/n): \")\n",
    "                if respuesta.lower() != 's':\n",
    "                    retrain = False\n",
    "                    return EA_act_2_ML\n",
    "                else: \n",
    "                    subprocess.call([\"Ruta dle script de retrain\"])\n",
    "                    print(\"Se reentreno el modelo\")\n",
    "        else: print(\"Data completa , no se usa el modelo\")\n",
    "        return EA_act_2_ML\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da6468b4-eee3-4c68-b8fb-bf2868a6bf7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ruta al entorno virtual\n",
    "venv_path = r\"C:\\path\\to\\venv\"\n",
    "\n",
    "# Comando para activar el entorno virtual, ejecutar el script y luego desactivar el entorno\n",
    "command = f'cmd.exe /c \"{venv_path}\\\\Scripts\\\\activate.bat && python C:\\\\path\\\\to\\\\script.py && deactivate\"'\n",
    "\n",
    "# Ejecutar el comando\n",
    "subprocess.call(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ba7c950-876f-4d69-ad90-5608016eb563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "847"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(EA_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cf6d91-7d45-43a7-8ed1-8bd785d78a9e",
   "metadata": {},
   "source": [
    "# 2 Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0afcf62-cba6-4eba-b5da-e654d82a2ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se crea archivo editable\n"
     ]
    }
   ],
   "source": [
    "if not reporte_old:\n",
    "\n",
    "    # Seleccionar columnas relevantes\n",
    "    df_analist_cut = df_analist[['CONCATENADO', 'SITE', 'Estado de EA', 'RESPONSABLE_DE_EA', 'ESTATUS_GENERAL']].copy()\n",
    "    \n",
    "    # Heredar información del DataFrame anterior\n",
    "    EA_ACT_A_D = (pd.merge(EA_act, df_analist_cut, on='CONCATENADO', how='left')\n",
    "                 .pipe(combine_and_rename, [('SITE_x', 'SITE_y')], {'SITE_x': 'SITE'})\n",
    "                  .pipe(update_and_rename, [('RESPONSABLE_DE_EA_x', 'RESPONSABLE_DE_EA_y'), \n",
    "                                            ('ESTATUS_GENERAL_x', 'ESTATUS_GENERAL_y')],\n",
    "                        {'RESPONSABLE_DE_EA_x': 'RESPONSABLE_DE_EA', 'ESTATUS_GENERAL_x': 'ESTATUS_GENERAL'}))\n",
    "    \n",
    "    # Etiquetar con Machine Learning\n",
    "    print(len(EA_ACT_A_D[EA_ACT_A_D.RESPONSABLE_DE_EA.isna()]))\n",
    "    EA_ACT_A_D = add_responsablebyML(EA_ACT_A_D, modelo_v2,selector, 25) ## Añado responsable de EA con ML\n",
    "    # Definir columnas inútiles para eliminar\n",
    "    useless_columns = ['PEP', 'TIPO DE MATERIAL', 'SUB_DIRECCION', 'Estrategia de Liberación', 'MATERIAL',\n",
    "                       'Cod.Solicitante', 'TIPO DE MATERIAL', 'TIPO_DOC', 'TIPO_PROYECTOS', 'COD_PROVEEDOR',\n",
    "                       'POS_PRESUP', 'COMPROM_USD', 'CE_GESTOR', 'RUBRO', 'CENTRO_GESTOR', 'Proyecto CAPEX', \n",
    "                       'Cod.Solicitante', 'Proyecto CAPEX', 'AÑO', 'PPTO_INICIAL', 'RECLA_INTERNA', 'PPTO_FINAL', \n",
    "                       'Estrategia de Liberación', 'MONEDA_ORIG', 'POS_PRESUP']\n",
    "    \n",
    "    # Rellenar valores nulos en 'Estado de EA'\n",
    "    EA_ACT_A_D['Estado de EA'].fillna('PENDIENTE', inplace=True)\n",
    "    \n",
    "    # Crear nuevo archivo editable\n",
    "    print(\"Se crea nuevo archivo editable\")\n",
    "    Ac.Excel_format(EA_ACT_A_D, fr'C:\\Users\\C26764\\America Movil Peru S.A.C\\EAS - 1\\EAUPDATE{Today_D_M}.xlsx', useless_columns) ## Ruta en la que se crea el Excel\n",
    "else:\n",
    "    print(\"No se crea archivo editable\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89cb2a5-96bc-4596-8408-27a7b825fa07",
   "metadata": {},
   "source": [
    "### Cargo lista de OCS que ya se asignaron fecha de Ejecucion (Diferencia de Rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5371903-82a0-4396-abf9-1f8ac0106439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\EA\\Resultados\\EAS_11-09-2024.csv\n",
      "C:\\Users\\C26764\\America Movil Peru S.A.C\\EAS - 1\\EAUPDATE10.09.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONCATENADO</th>\n",
       "      <th>SITE</th>\n",
       "      <th>Estado de EA</th>\n",
       "      <th>RESPONSABLE_DE_EA</th>\n",
       "      <th>ESTATUS_GENERAL</th>\n",
       "      <th>Fecha de Ejecucion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [CONCATENADO, SITE, Estado de EA, RESPONSABLE_DE_EA, ESTATUS_GENERAL, Fecha de Ejecucion]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# Obtener DataFrame más reciente del archivo CSV\n",
    "df_yesterday, _ = get_recent_csv(r'D:\\EA\\Resultados')\n",
    "df_analist, _ = get_recent_df(r'C:\\Users\\C26764\\America Movil Peru S.A.C\\EAS - 1', sheet_name='Sheet1')\n",
    "\n",
    "# Merge para comparar estados\n",
    "df_analist_Y = pd.merge(df_analist, df_yesterday[['CONCATENADO', 'Fecha de Ejecucion']], on='CONCATENADO', how='left') ## Merge para poder calcular de la diff \n",
    "\n",
    "# Seleccionar columnas relevantes y reemplazar valores\n",
    "df_analist_cut = df_analist_Y[['CONCATENADO', 'SITE', 'Estado de EA', 'RESPONSABLE_DE_EA', 'ESTATUS_GENERAL', 'Fecha de Ejecucion']].copy()\n",
    "df_analist_cut['ESTATUS_GENERAL'].replace('TERMINADO', 'EJECUTADO', inplace=True)\n",
    "\n",
    "# Filtrar filas donde el estado general ha cambiado a 'EJECUTADO' y no están en la lista de OCs etiquetadas\n",
    "df_row_diff = df_analist_cut[df_analist_cut['ESTATUS_GENERAL'].ne(df_yesterday['ESTATUS_GENERAL']) & \n",
    "                             (df_analist_cut['ESTATUS_GENERAL'] == 'EJECUTADO') & \n",
    "                             (~df_analist_cut['CONCATENADO'].isin(Ocs_F))]\n",
    "\n",
    "# Asignar fecha de ejecución a las filas que cambiaron a 'EJECUTADO'\n",
    "df_row_diff['Fecha de Ejecucion'] = pd.to_datetime(datetime.date.today().strftime('%d-%m-%Y'), format='%d-%m-%Y')\n",
    "\n",
    "display(df_row_diff)\n",
    "Ocs_DF = pd.concat([Ocs_DF,df_row_diff[['CONCATENADO','Fecha de Ejecucion']]],axis=0)\n",
    "\n",
    "with open(r'D:\\Scripts1\\Code\\ActPEA\\CODE\\Temps\\DF_OCS.pkl', \"wb\") as archivo:\n",
    "    pickle.dump(Ocs_DF, archivo)\n",
    "df_analist_cut.update(df_row_diff)# Las filas diferentes etiquetadas actualizan el df de analista\n",
    "df_analist_cut.rename(columns={'ESTATUS GENERAL':'ESTATUS_GENERAL'},inplace=True)\n",
    "df_analist_cut['Fecha de Ejecucion'] = pd.to_datetime(df_analist_cut['Fecha de Ejecucion'], unit='ns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b8c1953-68e2-47b4-9e68-e04283d50430",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 847 entries, 0 to 846\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   CONCATENADO         847 non-null    int64         \n",
      " 1   SITE                830 non-null    object        \n",
      " 2   Estado de EA        847 non-null    object        \n",
      " 3   RESPONSABLE_DE_EA   847 non-null    object        \n",
      " 4   ESTATUS_GENERAL     480 non-null    object        \n",
      " 5   Fecha de Ejecucion  92 non-null     datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(1), object(4)\n",
      "memory usage: 39.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_analist_cut.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c16ce7-56f1-406b-ba4d-405e79374cbf",
   "metadata": {},
   "source": [
    "## Merge data of the analist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "952968d1-53d6-4f04-a6c4-2ce6ec655997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapa para abreviar nombres de analistas\n",
    "analistas_map = {'JENNY PIZAN': 'JENNY', 'DANNER YARLEQUE': 'DANNER', 'LAURA RAFAEL': 'LAURA'}\n",
    "\n",
    "# Merge y procesamiento de datos\n",
    "EA_ACT_A_D = (\n",
    "    pd.merge(EA_act, df_analist_cut, on='CONCATENADO', how='left')\n",
    "    .pipe(combine_and_rename, [('SITE_x', 'SITE_y')], {'SITE_x': 'SITE'})\n",
    "    .pipe(update_and_rename, [('RESPONSABLE_DE_EA_x', 'RESPONSABLE_DE_EA_y'), ('ESTATUS_GENERAL_x', 'ESTATUS_GENERAL_y')],\n",
    "          {'RESPONSABLE_DE_EA_x': 'RESPONSABLE_DE_EA', 'ESTATUS_GENERAL_x': 'ESTATUS_GENERAL'})\n",
    ")\n",
    "\n",
    "# Convertir nombres de analistas a mayúsculas y reemplazar según el mapa\n",
    "EA_ACT_A_D['RESPONSABLE_DE_EA'] = EA_ACT_A_D['RESPONSABLE_DE_EA'].str.upper().replace(analistas_map)\n",
    "\n",
    "# Normalizar valores y rellenar vacíos\n",
    "EA_ACT_A_D['RESPONSABLE_DE_EA'] = EA_ACT_A_D['RESPONSABLE_DE_EA'].fillna('Por asignar')\n",
    "EA_ACT_A_D['Estado de EA'] = EA_ACT_A_D['Estado de EA'].fillna('PENDIENTE').str.upper()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Definir y aplicar valores específicos en 'Estado de EA'\n",
    "valores_anuladas = ['OC ANULADA']\n",
    "valores_anular = ['PENDIENTE ANULAR']\n",
    "EA_ACT_A_D['Estado de EA'] = EA_ACT_A_D['Estado de EA'].replace(valores_anuladas, 'ANULADA').replace(valores_anular, 'ANULAR')\n",
    "\n",
    "# Asignar valor por defecto si no está en los valores permitidos\n",
    "valores_permitidos = ['PENDIENTE', 'EJECUTADO', 'ANULAR', 'ANULADA', 'LIQUIDADO']\n",
    "EA_ACT_A_D['Estado de EA'] = EA_ACT_A_D['Estado de EA'].where(EA_ACT_A_D['Estado de EA'].isin(valores_permitidos), 'PENDIENTE')\n",
    "EA_ACT_A_D['Estado de EA'].replace('EJECUTADA', 'Ea ejecutada', inplace=True)\n",
    "\n",
    "# Crear columna de mes para agrupar y convertir a fecha\n",
    "EA_ACT_A_D['FECHA_DOC'] = pd.to_datetime(EA_ACT_A_D['FECHA_DOC'].str.replace(' 00:00:00', ''), format='%Y-%m-%d')\n",
    "\n",
    "# Convertir columnas a tipo int64\n",
    "EA_ACT_A_D[['NUMERO OC', 'POS_PREC']] = EA_ACT_A_D[['NUMERO OC', 'POS_PREC']].astype('int64')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "647d448c-578d-4230-9762-8c4c1f83411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(EA_ACT_A_D, EAs_2Merge, on=['NUMERO OC', 'POS_PREC'], how='left')\n",
    "df_merged['Estado de EA'].update(df_merged['Estado EA'])\n",
    "df_merged['Fecha de Ejecucion'].update(df_merged['F.Aprob'])\n",
    "# Eliminar columnas innecesarias y asignar el DataFrame final\n",
    "EA_ACT_A_D = df_merged.drop(columns=['Estado EA', 'F.Aprob'])\n",
    "EA_ACT_A_D['ESTATUS_GENERAL'].replace(pd.NA,'Sin detalle',inplace=True)\n",
    "EA_ACT_A_D['Estado de EA'] = EA_ACT_A_D['Estado de EA'].str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c05d8a5-fc5b-4b93-8424-6151a70679ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Estado de EA\n",
       "Pendiente       581\n",
       "Solicitado      154\n",
       "Ea Ejecutada    105\n",
       "Anular            4\n",
       "Observado         3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EA_ACT_A_D['Estado de EA'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2845acae-a31e-4768-8537-d32946af4b5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 847 entries, 0 to 846\n",
      "Data columns (total 38 columns):\n",
      " #   Column                    Non-Null Count  Dtype         \n",
      "---  ------                    --------------  -----         \n",
      " 0   CONCATENADO               847 non-null    int64         \n",
      " 1   NUMERO OC                 847 non-null    int64         \n",
      " 2   POS_PREC                  847 non-null    int64         \n",
      " 3   FECHA_DOC                 847 non-null    datetime64[ns]\n",
      " 4   Estrategia de Liberación  847 non-null    object        \n",
      " 5   Cod.Solicitante           847 non-null    object        \n",
      " 6   SOLICITANTE               847 non-null    object        \n",
      " 7   TIPO_DOC                  847 non-null    object        \n",
      " 8   TIPO DE MATERIAL          847 non-null    object        \n",
      " 9   MATERIAL                  847 non-null    int64         \n",
      " 10  TEXTO                     847 non-null    object        \n",
      " 11  COD_PROVEEDOR             847 non-null    float64       \n",
      " 12  PROVEEDOR                 847 non-null    object        \n",
      " 13  CENTRO_GESTOR             847 non-null    object        \n",
      " 14  POS_PRESUP                847 non-null    object        \n",
      " 15  COMPROM_USD               847 non-null    float64       \n",
      " 16  EN_PROC_USD               847 non-null    float64       \n",
      " 17  SUB_DIRECCION             847 non-null    object        \n",
      " 18  CE_GESTOR                 847 non-null    object        \n",
      " 19  RUBRO                     847 non-null    object        \n",
      " 20  TIPO_PROYECTOS            847 non-null    object        \n",
      " 21  fecha reporte             847 non-null    object        \n",
      " 22  Proyecto CAPEX            847 non-null    object        \n",
      " 23  NOMBRE PROYECTO           847 non-null    object        \n",
      " 24  DESCRIPCIÓN PROYECTO      847 non-null    object        \n",
      " 25  PEP                       845 non-null    object        \n",
      " 26  CLASIF_FINANZAS           847 non-null    object        \n",
      " 27  CLASIF_RED_1              847 non-null    object        \n",
      " 28  CLASIF_RED_2              847 non-null    object        \n",
      " 29  ID_SITIO                  847 non-null    object        \n",
      " 30  SITE                      847 non-null    object        \n",
      " 31  PAP                       458 non-null    float64       \n",
      " 32  ESTADO_PAP                847 non-null    object        \n",
      " 33  RESPONSABLE_DE_EA         847 non-null    object        \n",
      " 34  ESTATUS_GENERAL           847 non-null    object        \n",
      " 35  OT                        442 non-null    object        \n",
      " 36  Estado de EA              847 non-null    object        \n",
      " 37  Fecha de Ejecucion        112 non-null    datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(4), int64(4), object(28)\n",
      "memory usage: 251.6+ KB\n"
     ]
    }
   ],
   "source": [
    "EA_ACT_A_D.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3913884-e661-4db3-aad4-3d7831462abe",
   "metadata": {},
   "source": [
    "### Guardo dato en la serie Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b309247-8ecb-4e05-aeec-3a10309ac140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutado:  12-09-2024\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "## guardo el TSLM\n",
    "filename = r'\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\EAS\\last_run.json'\n",
    "# Carga la última fecha de ejecución\n",
    "last_run_date = Ac.load_last_run_date(filename)\n",
    "\n",
    "# Comprueba si la celda ya se ha ejecutado hoy\n",
    "if last_run_date != datetime.datetime.now().date():\n",
    "    # Tu código aquí\n",
    "    print('Ejecutado: ',Today_str)\n",
    "    #process_to_bcsv(PRE_all_act,'D:/Prepa/TIME.S/Prepa_TS1.csv',Today_str)\n",
    "    EA_ACT_A_D.to_csv(fr'D:\\EA\\Resultados\\EAS_{Today_str}.csv',index=False) ## guardo el TODAY (Opcional) \n",
    "    process_to_bcsv(EA_ACT_A_D.copy(),r'\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\EAS\\EA_TS1.csv',Today_str)# añado al acumulado\n",
    "\n",
    "    # Guarda la fecha de hoy como la última fecha de ejecución\n",
    "    Ac.save_last_run_date(filename)\n",
    "else:\n",
    "    print(\"El código ya se ha ejecutado hoy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e74f6d-bdd9-4f02-a0f5-86e0fa41d3a2",
   "metadata": {},
   "source": [
    "# TO BI \n",
    "    -Proceso el DF consolidado para el BI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f41363a5-460e-45db-a4ff-5fcde9e30012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las columnas\n",
    "Columnas_2_use = ['NUMERO OC', 'POS_PREC', 'FECHA_DOC', 'SOLICITANTE', 'TEXTO', 'PROVEEDOR',\n",
    "                  'EN_PROC_USD', 'CE_GESTOR', 'TIPO_PROYECTOS']\n",
    "last_columns = ['CLASIF_FINANZAS', 'CLASIF_RED_1', 'CLASIF_RED_2', 'ID_SITIO', 'SITE',\n",
    "                'PAP', 'ESTADO_PAP', 'RESPONSABLE_DE_EA', 'ESTATUS_GENERAL', 'OT',\n",
    "                'Estado de EA', 'Fecha de Ejecucion']\n",
    "total_columns = Columnas_2_use + last_columns\n",
    "\n",
    "# Intentamos seleccionar las columnas deseadas\n",
    "\n",
    "EA_ACT_A_B = standar_columns(EA_ACT_A_D,total_columns)\n",
    "\n",
    "    \n",
    "list_columns = ['TEXTO' ,'SITE','ESTATUS_GENERAL','TIPO_PROYECTOS']\n",
    "for column in list_columns:\n",
    "    EA_ACT_A_B.loc[:,column] = EA_ACT_A_B[column].str.capitalize()\n",
    "EA_ACT_A_B.rename(columns={'ESTATUS_GENERAL':'ESTATUS GENERAL'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a301b61e-e46f-4d01-a0d5-38ac1a96dbb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total de deuda es:,'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4853789.903225807"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\"Total de deuda es:,\",EA_ACT_A_B.EN_PROC_USD.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0ea4d95-47a8-4007-ada1-8da4da4a65f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Estado de EA\n",
       "Pendiente       581\n",
       "Solicitado      154\n",
       "Ea Ejecutada    105\n",
       "Anular            4\n",
       "Observado         3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EA_ACT_A_B['Estado de EA'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4702ca97-cb5d-45a9-93ac-34e09d1a79f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 847 entries, 0 to 846\n",
      "Data columns (total 21 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   NUMERO OC           847 non-null    int64         \n",
      " 1   POS_PREC            847 non-null    int64         \n",
      " 2   FECHA_DOC           847 non-null    datetime64[ns]\n",
      " 3   SOLICITANTE         847 non-null    object        \n",
      " 4   TEXTO               847 non-null    object        \n",
      " 5   PROVEEDOR           847 non-null    object        \n",
      " 6   EN_PROC_USD         847 non-null    float64       \n",
      " 7   CE_GESTOR           847 non-null    object        \n",
      " 8   TIPO_PROYECTOS      847 non-null    object        \n",
      " 9   CLASIF_FINANZAS     847 non-null    object        \n",
      " 10  CLASIF_RED_1        847 non-null    object        \n",
      " 11  CLASIF_RED_2        847 non-null    object        \n",
      " 12  ID_SITIO            847 non-null    object        \n",
      " 13  SITE                847 non-null    object        \n",
      " 14  PAP                 458 non-null    float64       \n",
      " 15  ESTADO_PAP          847 non-null    object        \n",
      " 16  RESPONSABLE_DE_EA   847 non-null    object        \n",
      " 17  ESTATUS GENERAL     847 non-null    object        \n",
      " 18  OT                  442 non-null    object        \n",
      " 19  Estado de EA        847 non-null    object        \n",
      " 20  Fecha de Ejecucion  112 non-null    datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(2), int64(2), object(15)\n",
      "memory usage: 139.1+ KB\n"
     ]
    }
   ],
   "source": [
    "EA_ACT_A_B.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8bb1fa52-11df-404c-bb0f-f7bb93b0f4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4853789.903225807"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EA_ACT_A_B.EN_PROC_USD.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b56adf77-fa0c-4847-a3c9-097dac90eaba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EA_ACT_A_B.to_csv(r'\\\\LIMBIPBICOV01.claro.pe\\Red Región Norte\\EAS\\EAs.csv',index=False) \n",
    "subprocess.call([\"python\",r\"D:\\Scripts1\\Code\\ActPEA\\CODE\\2BI_Norma.py\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f103c2a-5712-4fe1-8ed3-2607908dba89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9814e4e6-b537-4031-9b19-28c898eed441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
